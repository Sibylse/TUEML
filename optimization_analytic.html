
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Analytic Solutions &#8212; Data Mining and Machine Learning Jupyter Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Numerical Optimization" href="optimization_numerical.html" />
    <link rel="prev" title="Convex Optimization" href="optimization_convex.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Mining and Machine Learning Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="notation.html">
   Notation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="linalg.html">
   Linear Algebra
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_normed_vs.html">
     Normed Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="optimization.html">
   Optimization
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_problems.html">
     Optimization Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_convex.html">
     Convex Optimization
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Analytic Solutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_numerical.html">
     Numerical Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_gradients.html">
     Matrix Derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_objective.html">
     Regression Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_functions.html">
     Regression Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_optimization.html">
     Minimizing the RSS
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_bias_var.html">
     The Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_sparse.html">
     The Sparse Regression Task
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_ridge.html">
     Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_lasso.html">
     Lasso
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_problem.html">
     Classification Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_knn.html">
     K-Nearest Neighbor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_naive_bayes.html">
     Naive Bayes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_decision_trees.html">
     Decision Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_random_forests.html">
     Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_svms.html">
     Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_kernel_svm.html">
     Kernel SVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="neuralnets.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_intro.html">
     From Linear Models to Neural Networks
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_mlps.html">
     MLPs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_backprop.html">
     Training Neural Networks: Backpropagation and SGD
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_sgd.html">
     SGD
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_conv.html">
     Convolution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_architecture.html">
     Architecture
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="dim_reduction.html">
   Dimensionality Reduction Techniques
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_mf.html">
     Low Rank Matrix Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_matrix_completion.html">
     Matrix Completion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_pca.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means.html">
     k-Means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means_mf.html">
     k-Means is MF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_kernel_kmeans.html">
     Kernel k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_spectral.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://sibylse.github.io/TUEML/intro.html"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://sibylse.github.io/TUEML/intro.html/issues/new?title=Issue%20on%20page%20%2Foptimization_analytic.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/TUEML/intro.html/edit/master/optimization_analytic.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/optimization_analytic.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-stationary-points-in-higher-dimensions">
   Finding Stationary Points in higher dimensions
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Analytic Solutions</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finding-stationary-points-in-higher-dimensions">
   Finding Stationary Points in higher dimensions
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="analytic-solutions">
<h1>Analytic Solutions<a class="headerlink" href="#analytic-solutions" title="Permalink to this headline">#</a></h1>
<p>You probably know from your highschool math classes that every local minimizer <span class="math notranslate nohighlight">\(x_0\)</span> of a function <span class="math notranslate nohighlight">\(f:\mathbb{R}\rightarrow\mathbb{R}\)</span> is a stationary point: <span class="math notranslate nohighlight">\(\frac{d}{dx}f(x_0)=0\)</span>. This is known as the first order neccessary condition. This property is easily understood, considering that the derivative indicates the slope of a function at a specified point. If we have a real-valued minimizer, then the slope is zero at that minimizer. If the slope would be positive, then we can go to the left to decrease the function value further, and if the slope is negative, we can go to the right. By means of this property, we can identify all the candidates that could be minimizers. Maximizers and saddle points are stationary points too. With the second order neccessary condition, we can filter further the minimizers from the pool of candidates. The second order neccessary condition states that at a minimizer the second derivative is nonnegative <span class="math notranslate nohighlight">\(\frac{d^2}{dx^2}f(x_0)\geq 0\)</span>. The second derivative is the slope of the slope. If we have a minimizer, then the slope increases: first we go down, and then we go up. Hence, we need that the slope of the slope does at least not decrease.</p>
<div class="proof example admonition" id="example-0">
<p class="admonition-title"><span class="caption-number">Example 7 </span></p>
<section class="example-content" id="proof-content">
<p>Let’s have a look at a seemingly simple example. The function <span class="math notranslate nohighlight">\(f(x) = \frac14x^4 + \frac13x^3 -x^2\)</span> is plotted below and we see that there are two minimizers <span class="math notranslate nohighlight">\(x_1=-2\)</span> and <span class="math notranslate nohighlight">\(x_2=1\)</span>. The question is just if those are all minimizers, or if there is another one beyond the scope of what is plotted here.</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-b2427c92aa6d74085e15433b634dd01ad375d2b1.svg" alt="Figure made with TikZ" /></p>
</div><p>To find all the minimizers of the function, we apply the first and second order neccessary condition. We compute the first and second derivative.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{d}{dx} f(x) &amp;= x^3 + x^2 -2x \\
    \frac{d^2}{dx^2}f(x) &amp; = 3x^2 + 2x -2
\end{align*}\]</div>
<p>Now we solve the equation setting the first derivative to zero and get three stationary points:
<div class="math notranslate nohighlight">
\[\frac{d}{dx} f(x) =0 \quad \Leftrightarrow \quad x_1=-2, x_2 = 0, x_3=1\]</div>

Given the plot, we already know which of these are minimizers, but to conclude our example, we apply the second order sufficient condition to identify the local minimizers <span class="math notranslate nohighlight">\(x_1=-2\)</span> and <span class="math notranslate nohighlight">\(x_2=3\)</span>.</p>
<div class="math notranslate nohighlight">
\[\frac{d^2}{dx^2}f(-2)=6\geq 0,\quad \frac{d^2}{dx^2}f(0)=-2&lt; 0,\quad \frac{d^2}{dx^2}f(1)=3\geq 0 \]</div>
</section>
</div><section id="finding-stationary-points-in-higher-dimensions">
<h2>Finding Stationary Points in higher dimensions<a class="headerlink" href="#finding-stationary-points-in-higher-dimensions" title="Permalink to this headline">#</a></h2>
<p>The principles of the first and second order conditions can be generalized to functions <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\rightarrow \mathbb{R}\)</span> mapping from a <span class="math notranslate nohighlight">\(d\)</span>-dimensional vector space to real values. The main difficulty is that we have now more to consider than just left and right when looking for a direction into which we could minimize the function further. In fact, any vector <span class="math notranslate nohighlight">\(\vvec{v}\in\mathbb{R}^d\)</span> could indicate a possible direction in which the function might decrease. Luckily, we can show that we just have to check one direction, given by the negative <em>gradient</em>, which points into the direction of steepest descent. The gradient indicates the slope of a function in the directions of the coordinates, which are called the <em>partial derivatives</em>. A partial derivative <span class="math notranslate nohighlight">\(\frac{\partial f(\vvec{x})}{\partial x_i}\)</span> is computed like a one-dimensional derivative by treating all variables except for <span class="math notranslate nohighlight">\(x_i\)</span> as  a constant. The gradient gathers those partial derivatives in a vector. The transposed of the gradient is called the <em>Jacobian</em>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\partial f(\vvec{x})}{\partial \vvec{x}} &amp;=
    \begin{pmatrix}
    \frac{\partial f(\vvec{x})}{\partial x_1} &amp; \ldots &amp; \frac{\partial f(\vvec{x})}{\partial x_d}
    \end{pmatrix}\in\mathbb{R}^{1\times d} &amp;\text{(Jacobian)}\\
      \nabla_\vvec{x} f(\vvec{x}) &amp;=
    \begin{pmatrix}
    \frac{\partial f(\vvec{x})}{\partial x_1} \\ \vdots \\ \frac{\partial f(\vvec{x})}{\partial x_d}
    \end{pmatrix}\in\mathbb{R}^{d} &amp;\text{(Gradient)}
\end{align*}\]</div>
<p>With the gradient, we get a first order neccessary condition (FONC) for functions mapping from a vector space <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span>.</p>
<div class="proof theorem admonition" id="theorem-1">
<p class="admonition-title"><span class="caption-number">Theorem 6 </span> (FONC)</p>
<section class="theorem-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(\vvec{x}\)</span> is a local  minimizer of <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\rightarrow\mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(f\)</span> is continuously
differentiable in an open neighborhood of <span class="math notranslate nohighlight">\(\vvec{x}\)</span>, then
<div class="math notranslate nohighlight">
\[\nabla f(\vvec{x})=0\]</div>
</p>
</section>
</div><p>Likewise, a vector <span class="math notranslate nohighlight">\(\vvec{x}\)</span> is called <em>stationary point</em> if <span class="math notranslate nohighlight">\(\nabla f(\vvec{x})=0\)</span>. The second order neccessary condition (SONC) uses the generation of the second order derivative to vector spaces, called the <em>Hessian</em>. We state this condition here for reasons of completeness, but we will not need this property for the machine learning models that we discuss in this course.</p>
<div class="proof theorem admonition" id="theorem-2">
<p class="admonition-title"><span class="caption-number">Theorem 7 </span> (SONC)</p>
<section class="theorem-content" id="proof-content">
<p>If <span class="math notranslate nohighlight">\(\vvec{x}\)</span> is a local  minimizer of <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\rightarrow\mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(\nabla^2f\)</span> is continuous in an open
neighborhood of <span class="math notranslate nohighlight">\(\vvec{x}\)</span>, then
<div class="math notranslate nohighlight">
\[\nabla f(\vvec{x})=0 \text{ and } \nabla^2f(\vvec{x}) \text{ is positive semidefinite}\]</div>
</p>
</section>
</div><p>A matrix <span class="math notranslate nohighlight">\(A\in\mathbb{R}^{d\times d}\)</span> is <strong>positive semidefinite</strong> if
<div class="math notranslate nohighlight">
\[\vvec{x}^\top A \vvec{x}\geq 0 \text{ for all } \vvec{x}\in\mathbb{R}^d\]</div>
</p>
<div class="proof example admonition" id="expl_fonc">
<p class="admonition-title"><span class="caption-number">Example 8 </span></p>
<section class="example-content" id="proof-content">
<figure class="align-left" id="rosenbrock">
<a class="reference internal image-reference" href="_images/rosenbrock.png"><img alt="_images/rosenbrock.png" src="_images/rosenbrock.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">The Rosenbrock function</span><a class="headerlink" href="#rosenbrock" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In this example we apply FONC and SONC to find the minimizers of the Rosenbrock function, which is given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    f(\vvec{x})&amp;= 100(x_2-x_1^2)^2 +(1-x_1)^2.
\end{align*}\]</div>
<p>In order to apply FONC, we need to compute the gradient. We do so by computing the partial derivatives. The partial derivatives are computed by the same rules as you know it from computing the derivative of a one-dimensional function.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \frac{\partial}{\partial x_1}f(\vvec{x})&amp;= 400x_1(x_1^2-x_2) +2(x_1-1)\\
    \frac{\partial}{\partial x_2}f(\vvec{x})&amp;= 200(x_2-x_1^2)
\end{align*}\]</div>
<p>FONC says that every minimizer has to be a stationary point. Stationary points are the vectors at which the gradient of <span class="math notranslate nohighlight">\(f\)</span> is zero. We compute the set of stationary points by setting the gradient to zero and solving for <span class="math notranslate nohighlight">\(\vvec{x}\)</span>.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
      \frac{\partial}{\partial x_2}f(\vvec{x})&amp;=200(x_2-x_1^2)=0
      &amp;\Leftrightarrow x_2 =x_1^2\\
      \frac{\partial}{\partial x_1}f\begin{pmatrix}x_1\\x_1^2\end{pmatrix}&amp;= 2(x_1-1) =0 
      &amp;\Leftrightarrow x_1=1
\end{align*}\]</div>
<p>According to FONC we have a stationary point at <span class="math notranslate nohighlight">\(\vvec{x}=(1,1)\)</span>. Now we check with SONC if the stationary point could be a minimizer (it could also be a maximizer or a saddle point). SONC says that every minimizer has a positive definite Hessian. Hence, we require the Hessian, the second derivative of the Rosenbrock function. To that end, we compute the partial derivatives of the partial derivatives:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\frac{\partial^2}{\partial^2 x_1}f(\vvec{x})&amp;= \frac{\partial}{\partial x_1}\left(\frac{\partial}{\partial x_1}f(\vvec{x})\right)= 1200x_1^2-400x_2 +2\\
\frac{\partial^2}{\partial^2 x_2}f(\vvec{x})&amp;=  \frac{\partial}{\partial x_2} \left(\frac{\partial}{\partial x_2}f(\vvec{x})\right)= 200\\
\frac{\partial^2}{\partial x_1\partial x_2}f(\vvec{x})&amp;=\frac{\partial^2}{\partial x_2\partial x_1}f(\vvec{x})= -400x_1
\end{align*}\]</div>
<p>The Hessian is given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \nabla^2 f(\vvec{x})&amp;=  \begin{pmatrix}\frac{\partial^2}{\partial^2 x_1} f(\vvec{x}) &amp; \frac{\partial^2}{\partial x_1x_2} f(\vvec{x})\\ \frac{\partial^2}{\partial x_2x_1}f(\vvec{x}) &amp; \frac{\partial^2}{\partial^2 x_2} f(\vvec{x})\end{pmatrix}\\
    &amp;=200\begin{pmatrix} 6x_1^2-2x_2 + 0.01&amp; -2x_1\\ -2x_1 &amp;1 \end{pmatrix}
\end{align*}\]</div>
<p>We insert our stationary point <span class="math notranslate nohighlight">\(\vvec{x}_0=(1,1)\)</span> into the Hessian and get
<div class="math notranslate nohighlight">
\[\begin{split}\nabla^2f(\vvec{x}_0)= 200\begin{pmatrix} 4.01&amp; -2\\ -2 &amp; 1\end{pmatrix}\end{split}\]</div>

Now we check if the Hessian at the stationary point is positive definite. Let <span class="math notranslate nohighlight">\(\vvec{x}\in\mathbb{R}^2\)</span>, then</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \vvec{x}^\top \nabla^2f(\vvec{x}_0) \vvec{x} &amp;= 200 \begin{pmatrix}x_1 &amp; x_2\end{pmatrix} \begin{pmatrix}
     4.01&amp; -2\\ -2 &amp; 1
    \end{pmatrix}\begin{pmatrix}x_1\\x_2\end{pmatrix}\\
    &amp;= 200\begin{pmatrix}x_1 &amp; x_2\end{pmatrix} \begin{pmatrix}
    4.01x_1-2x_2\\ -2x_1+ x_2
    \end{pmatrix}\\
    &amp;=200(4.01x_1^2 -2x_1x_2 -2x_1x_2 +x_2^2)\\
    &amp;= 200(4.01x_1^2 -4x_1x_2 + x_2^2)\\
    &amp;= 200((2x_1-x_2)^2 +0.01x_1^2) \geq 0
\end{align*}\]</div>
<p>The last inequality follows because the sum of quadratic terms can not be negative.
We conclude that the Hessian at our stationary point is positive semi-definite. As a result, FONC and SONC yield that <span class="math notranslate nohighlight">\(\vvec{x}=(1,1)\)</span> is the only possible local minimizer of <span class="math notranslate nohighlight">\(f\)</span>.</p>
</section>
</div><p>Nice, we have now a strategy yo find local minimizers if we have an unconstrained objective with an objective function which is continuously differentiable. Let’s consider a more complex setting, introducing constraints.</p>
<div class="proof example admonition" id="example-4">
<p class="admonition-title"><span class="caption-number">Example 9 </span> (Solving of a dual)</p>
<section class="example-content" id="proof-content">
<p>We solve the following constrained optimization problem:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\min_{\vvec{w}}\ &amp; w_1^2+w_2^2\\
\text{ s.t } &amp; w_2\geq 1\\
&amp; w_2\geq -w_1+2
\end{align*}\]</div>
<p>Geometrically, the problem looks as follows:</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-cf5cfbed76215bcf3226e9812b672d701d6d3e91.svg" alt="Figure made with TikZ" /></p>
</div><p>We have an objective function that is visualized over the level sets (the rings). Each ring indicates the vectors <span class="math notranslate nohighlight">\(\vvec{w}\)</span> that return the same function value. We see that the minimum is at <span class="math notranslate nohighlight">\((0,0)\)</span>, but that minimum does not lie in the feasible set <span class="math notranslate nohighlight">\(\mathcal{C}\)</span>.</p>
<p>To solve this constrained objective, we formulate the dual, which requires the Lagrangian first. To formulate the Lagrangian, we put the constraints into the form <span class="math notranslate nohighlight">\(g(\vvec{w})\geq 0\)</span>:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
w_2-1&amp;\geq 0\\
w_2+w_1-2 &amp;\geq 0
\end{align*}\]</div>
<p>The Lagrangian is then given by
<div class="math notranslate nohighlight">
\[\mathcal{L}(\vvec{w},\bm{\lambda})=w_1^2 +w_2^2 -\lambda_1(w_2-1)-\lambda_2(w_2+w_1-2)\]</div>

The dual objective function returns the minimum of the Lagrangian:
<div class="math notranslate nohighlight">
\[\mathcal{L}_{dual}=\min_{\vvec{x}}\mathcal{L}(\vvec{w},\bm{\lambda}).\]</div>

We can compute the dual objective function analytically over the stationary point, since the Lagrangian is convex in <span class="math notranslate nohighlight">\(\vvec{w}\)</span> (it is the sum of convex functions: the squared norm and affine functions). Hence, we compute the gradient and set it to zero.</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\nabla_\vvec{w}\mathcal{L}(\vvec{w},\bm{\lambda}) &amp;= \begin{pmatrix}
2w_1-\lambda_2\\
2w_2-\lambda_1-\lambda_2
\end{pmatrix}
=\begin{pmatrix}
0\\ 0
\end{pmatrix}
\Leftrightarrow &amp; \begin{cases}
w_1 = \frac12 \lambda_2\\
w_2 = \frac12 \lambda_1 + \frac12 \lambda_2.
\end{cases}
\end{align*}\]</div>
<p>We plug in the minimizer <span class="math notranslate nohighlight">\(\vvec{w}\)</span> defined above in the Lagrangian and obtain the dual objective function:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
\mathcal{L}_{dual}(\bm{\lambda}) &amp;= -\frac14 \lambda_1^2-\frac12 \lambda_2^2 -\frac12\lambda_1\lambda_2 + \lambda_1 +2\lambda_2.
\end{align*}\]</div>
<p>Hence, we need to maximize the function above, which is equivalent to minimizing the negative dual function. The negative dual function is convex, since it can be written as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-\mathcal{L}_{dual}(\bm{\lambda}) &amp;= \frac14\left\lVert \begin{pmatrix}1 &amp; 1\\0 &amp; 1\end{pmatrix}\bm{\lambda}\right\rVert^2 - \begin{pmatrix}1 &amp;2 \end{pmatrix}\bm{\lambda}
\end{align*}\]</div>
<p>which is the sum of a convex and an affine function. Hence, to solve the dual objective, we set again the gradient to zero:</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
-\nabla\mathcal{L}_{dual}(\bm{\lambda}) = \begin{pmatrix}\frac12\lambda_1 +\frac12\lambda_2-1\\ 
\frac12\lambda_1+\lambda_2-2\end{pmatrix} = \vvec{0} 
\end{align*}\]</div>
<p>which is the case for <span class="math notranslate nohighlight">\(\lambda_1=0\)</span> and <span class="math notranslate nohighlight">\(\lambda_2=2\)</span>. To get the solution nof our primal objective, we plug in the optimal <span class="math notranslate nohighlight">\(\bm{\lambda}\)</span> in the optimal <span class="math notranslate nohighlight">\(\vvec{w}\)</span> definition and get</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
w_1^* &amp;= \frac12\lambda_2^* = 1\\
w_2^* &amp;= \frac12\lambda_1^* +\frac12\lambda_2^* = 1
\end{align*}\]</div>
</section>
</div></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "None/None",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="optimization_convex.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Convex Optimization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="optimization_numerical.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Numerical Optimization</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Sibylle Hess<br/>
  
      &copy; Copyright 2022. Eindhoven University of Technology.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>