
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>The Sparse Regression Task &#8212; Data Mining and Machine Learning Jupyter Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Ridge Regression" href="regression_ridge.html" />
    <link rel="prev" title="The Bias-Variance Tradeoff" href="regression_bias_var.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Mining and Machine Learning Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="notation.html">
   Notation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="linalg.html">
   Linear Algebra
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_normed_vs.html">
     Normed Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="optimization.html">
   Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_problems.html">
     Optimization Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_numerical.html">
     Numerical Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_convex.html">
     Convex Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_gradients.html">
     Matrix Derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="regression.html">
   Regression
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="regression_objective.html">
     Regression Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_functions.html">
     Regression Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_optimization.html">
     Minimizing the RSS
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_bias_var.html">
     The Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     The Sparse Regression Task
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_ridge.html">
     Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_lasso.html">
     Lasso
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_L1vsL2.html">
     L1 vs L2 Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_problem.html">
     Classification Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_knn.html">
     K-Nearest Neighbor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_naive_bayes.html">
     Naive Bayes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_decision_trees.html">
     Decision Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_random_forests.html">
     Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_svms.html">
     Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_kernel_svm.html">
     Kernel SVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="neuralnets.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_intro.html">
     Neural Networks Intro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_comp_graphs.html">
     Computational graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_func_approx.html">
     Function approximator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_mlps.html">
     Multi-Layer Perceptrons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_backprop.html">
     Backpropagation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="dim_reduction.html">
   Dimensionality Reduction Techniques
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_mf.html">
     Low Rank Matrix Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_matrix_completion.html">
     Matrix Completion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_pca.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means.html">
     k-Means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means_mf.html">
     k-Means is MF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_kernel_kmeans.html">
     Kernel k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_spectral.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://gitlab.tue.nl/20214358/dmml"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://gitlab.tue.nl/20214358/dmml/issues/new?title=Issue%20on%20page%20%2Fregression_sparse.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/20214358/dmml/edit/master/regression_sparse.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/regression_sparse.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relaxing-the-sparsity-constraint">
   Relaxing the sparsity constraint
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>The Sparse Regression Task</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relaxing-the-sparsity-constraint">
   Relaxing the sparsity constraint
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="the-sparse-regression-task">
<h1>The Sparse Regression Task<a class="headerlink" href="#the-sparse-regression-task" title="Permalink to this headline">#</a></h1>
<p>We already discussed that there are infinitely many regression solvers <span class="math notranslate nohighlight">\(\beta\)</span> if <span class="math notranslate nohighlight">\(p&gt;n\)</span>. Usually, we can avoid this situation by choosing a simple basis function class, for example affine functions. However, for various types of datasets it’s quite common that there are more features than observations. For example in gene expression analysis, collecting patient data (observations) is costly, while the number of features (the genes) is big. Such datasets usually indicate for each observation (patient) the expression levels of their genes. The patients might have a specified disease, and the task if to find out why some patients live longer with this disease than others. The target would then be something like the survival time after a diagnosis has been made.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>D</p></th>
<th class="head"><p>Gene 1</p></th>
<th class="head"><p>Gene 2</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\ldots\)</span></p></th>
<th class="head"><p>Gene 60,000</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y\)</span>: survival time</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>0.00</p></td>
<td><p>2.75</p></td>
<td><p></p></td>
<td><p>12.93</p></td>
<td><p>0.9</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>0.00</p></td>
<td><p>0.00</p></td>
<td><p></p></td>
<td><p>16.26</p></td>
<td><p>0.7</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\vdots\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>489</p></td>
<td><p>0.00</p></td>
<td><p>5.38</p></td>
<td><p></p></td>
<td><p>0.00</p></td>
<td><p>0.8</p></td>
</tr>
</tbody>
</table>
<p>When it comes to the analysis of a gene expression dataset, we might want to explore more than just the prediction of the target survival time. The identification of the relevant features is also of interest to guide medical research by pointing out the relevant genes.</p>
<p>The regression vector <span class="math notranslate nohighlight">\(\bm{\beta}\)</span> encodes which features are relevant for prediction by nonnegative entries:</p>
<div class="amsmath math notranslate nohighlight" id="equation-7ef77ff8-82cb-4e73-a8cd-0c5b1281e7cb">
<span class="eqno">(16)<a class="headerlink" href="#equation-7ef77ff8-82cb-4e73-a8cd-0c5b1281e7cb" title="Permalink to this equation">#</a></span>\[\begin{align}
    f(\vvec{x}) =  \vvec{x}^\top{\bm\beta} = \sum_{i=1}^p \beta_ix_i 
    =\sum_{i:\beta_k\neq 0} \beta_ix_i
\end{align}\]</div>
<p>The number of nonnegative entries is given by the <span class="math notranslate nohighlight">\(L_0\)</span>-‘norm’:</p>
<div class="amsmath math notranslate nohighlight" id="equation-f4c4a961-8e3f-4eea-9b61-05de1369d0ec">
<span class="eqno">(17)<a class="headerlink" href="#equation-f4c4a961-8e3f-4eea-9b61-05de1369d0ec" title="Permalink to this equation">#</a></span>\[\begin{align}
    \lVert\bm{\beta}\rVert_0 = \lvert\{i\mid \beta_i\neq 0\}\rvert. 
\end{align}\]</div>
<p>Note that the <span class="math notranslate nohighlight">\(L_0\)</span>-‘norm’ is not a real norm. We define the sparse regression task to find the regression model that minimizes the prediction error while using only <span class="math notranslate nohighlight">\(s\)</span> features.</p>
<div class="tip admonition">
<p class="admonition-title">Task (Sparse Regression)</p>
<p><strong>Given</strong> a data matrix <span class="math notranslate nohighlight">\(D\in\mathbb{R}^{n\times d}\)</span>, a target vector <span class="math notranslate nohighlight">\(\vvec{y}\in\mathbb{R}^n\)</span>, the design matrix <span class="math notranslate nohighlight">\(X\in\mathbb{R}^{n\times p}\)</span>, where <span class="math notranslate nohighlight">\(X_{i\cdot}=\bm\phi(D_{i\cdot}^\top)^\top\)</span> and the integer <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p><strong>Find</strong> the regression vector <span class="math notranslate nohighlight">\(\bm\beta\)</span>, solving the following objective</p>
<div class="amsmath math notranslate nohighlight" id="equation-7e23ac4e-80b3-48e5-8538-bdead61c0eec">
<span class="eqno">(18)<a class="headerlink" href="#equation-7e23ac4e-80b3-48e5-8538-bdead61c0eec" title="Permalink to this equation">#</a></span>\[\begin{align}
    \min_{\bm\beta\in\mathbb{R}^p} &amp; \lVert \vvec{y}-X\bm\beta\rVert^2 &amp;
    \text{s.t. }&amp; \lVert\bm{\beta}\rVert_0\leq s.
\end{align}\]</div>
<p><strong>Return</strong> the predictor function <span class="math notranslate nohighlight">\(f:\mathbb{R}^d\rightarrow\mathbb{R}\)</span>, <span class="math notranslate nohighlight">\(f(\vvec{x})=\bm\phi(\vvec{x})^\top\bm\beta\)</span>.</p>
</div>
<p>Adding the seemingly simple sparsity constraint <span class="math notranslate nohighlight">\(\lVert\beta\rVert_0\leq s\)</span> makes the resulting regression task unfortunately computationally much more difficult. An issue is that the sparse regression objective is not convex, because the feasible set <span class="math notranslate nohighlight">\(\{\beta\mid \lVert\bm{\beta}\rVert_0\leq s\}\)</span> is not convex. But it’s even worse than that, because the <span class="math notranslate nohighlight">\(L_0\)</span>-‘norm’ is not a continuous function. The property of the constraining function is relevant, since the optimizers subject to constraints rely on integrating the constraints into the objective function, as seen in the dual formulation. Noncontinuous objective functions are bad for all the optimizers that we know, since they all rely on gradient information, which in turn relies on the assumption that a sufficiently small change in a data point has a small effect on the function value. Noncontinuous functions can differ vastly from one point to an infinitesimal small change from that point. That makes the local information about the directions in which the objective increases and decreases useless, and this means in turn that we are closer to a combinatorial optimization problem, where we just have to try various parameter constellations (e.g., various selections of features).</p>
<section id="relaxing-the-sparsity-constraint">
<h2>Relaxing the sparsity constraint<a class="headerlink" href="#relaxing-the-sparsity-constraint" title="Permalink to this headline">#</a></h2>
<p>The <span class="math notranslate nohighlight">\(L_0\)</span>-‘norm’ is a natural extension from the <span class="math notranslate nohighlight">\(L_p\)</span>-norms, which are actual norms for <span class="math notranslate nohighlight">\(p\geq 1\)</span>. Those are defined as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-65b2693d-4731-4533-ab13-16e39506807d">
<span class="eqno">(19)<a class="headerlink" href="#equation-65b2693d-4731-4533-ab13-16e39506807d" title="Permalink to this equation">#</a></span>\[\begin{align}
    \lVert \vvec{x}\rVert_p = \left(\sum_{k=1}^d \lvert x_k\rvert^p\right)^{1/p}
\end{align}\]</div>
<p>If <span class="math notranslate nohighlight">\(p\in (0,1)\)</span>, then we can compute the value defined above, but it’s not a real norm. For <span class="math notranslate nohighlight">\(p=0\)</span> we can’t compute the value above directly, because we would have to divide by zero. The <span class="math notranslate nohighlight">\(L_0\)</span>-‘norm’ is given by computing the formula above for <span class="math notranslate nohighlight">\(p=0\)</span> without the root term. That is, we have <span class="math notranslate nohighlight">\(\lim_{p\rightarrow 0}\lVert\mathbf{x}\rVert_p^p=\lVert \mathbf{x}\rVert_0\)</span>. For optimization purposes this is good enough, since it means that we can relax the constraint <span class="math notranslate nohighlight">\(\lVert \mathbf{x}\rVert_0\leq s\)</span> with the constraint <span class="math notranslate nohighlight">\(\lVert \mathbf{x}\rVert_p^p \leq s\)</span> for a small <span class="math notranslate nohighlight">\(p\approx 0\)</span>, or equivalently <span class="math notranslate nohighlight">\(\lVert \mathbf{x}\rVert_p \leq s^{1/p}\)</span>.</p>
<p>The plots below visualize the (relaxed) feasible sets of the sparse regression task for <span class="math notranslate nohighlight">\(s=1\)</span>. The red lines indicate the boundary of the <span class="math notranslate nohighlight">\(p\)</span>-norm balls <span class="math notranslate nohighlight">\(\{\mathbf{x}\mid \lVert \mathbf{x}\rVert_p\leq 1\}\)</span>.</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-4459ce5702862578d1a453c545f842dea2d961ec.svg" alt="Figure made with TikZ" /></p>
</div><p>On the left we observe that the <span class="math notranslate nohighlight">\(L_0\)</span>-ball, containing the axes except for the point zero, progresses to a star shape for <span class="math notranslate nohighlight">\(p=0.5\)</span> and further to a diamond for <span class="math notranslate nohighlight">\(p=1\)</span>, a circle for the Euclidean norm <span class="math notranslate nohighlight">\((p=2)\)</span>, and a rectangle for the infinity norm. The <span class="math notranslate nohighlight">\(L_0\)</span>-ball is more suitably approximated for small values of <span class="math notranslate nohighlight">\(p\)</span>, but those <span class="math notranslate nohighlight">\(L_p\)</span>-balls are not convex, resulting again in a nonconvex objective. We see this for the <span class="math notranslate nohighlight">\(p=0.5\)</span>-ball: the line connecting the points <span class="math notranslate nohighlight">\((0,1)\)</span> and <span class="math notranslate nohighlight">\((1,0)\)</span> is outside of the ball. So, a popular choice is to relax the <span class="math notranslate nohighlight">\(L_0\)</span>-norm with the <span class="math notranslate nohighlight">\(L_1\)</span>-norm, that is closest to the <span class="math notranslate nohighlight">\(L_0\)</span>-norm with a convex unit ball.</p>
<p>But how do we optimize subject to the constraints?</p>
<figure class="align-center" id="penalization">
<a class="reference internal image-reference" href="_images/meme_penalization.jpg"><img alt="_images/meme_penalization.jpg" src="_images/meme_penalization.jpg" style="height: 300px;" /></a>
</figure>
<p>Going over the dual is in practice often difficult. In particular if the resulting Lagrangian is not continuously differentiable, solving the dual becomes often impossible. A popular approach is to simply penalize the objective function with the constraints.<br />
That is, given the <span class="math notranslate nohighlight">\(L_p\)</span>-constrained regression problem for <span class="math notranslate nohighlight">\(s&gt;0\)</span></p>
<div class="amsmath math notranslate nohighlight" id="equation-ab2e2544-6f55-4ea0-b0bd-94e375fe04a7">
<span class="eqno">(20)<a class="headerlink" href="#equation-ab2e2544-6f55-4ea0-b0bd-94e375fe04a7" title="Permalink to this equation">#</a></span>\[\begin{align}
    \min_{\bm\beta}&amp; \lVert \vvec{y} -X \bm{\beta}\rVert^2 &amp; \text{s.t. } \lVert\bm{\beta}\rVert_p\leq s,
\end{align}\]</div>
<p>we can reformulate this objective into an unconstrained one by means of the Lagrangian.
If the duality gap is zero, there exists a parameter <span class="math notranslate nohighlight">\(\lambda&gt;0\)</span> such that the objective above is solved by</p>
<div class="amsmath math notranslate nohighlight" id="equation-aba6a174-c13e-4399-bb8e-e4001439cbc7">
<span class="eqno">(21)<a class="headerlink" href="#equation-aba6a174-c13e-4399-bb8e-e4001439cbc7" title="Permalink to this equation">#</a></span>\[\begin{align}
    \min_{\bm\beta}&amp; \lVert \vvec{y} -X \bm{\beta}\rVert^2 +\lambda\lVert \bm{\beta}\rVert_p.
\end{align}\]</div>
<p>The question is now, which <span class="math notranslate nohighlight">\(p\)</span>-norm we are going to choose. We have here a trade-off between <span class="math notranslate nohighlight">\(p\)</span>-norms that are well-optimizable, since they are differentiable, or even smooth, and <span class="math notranslate nohighlight">\(p\)</span>-norms that are not so easily optimizable, but that are close to the behavior of the <span class="math notranslate nohighlight">\(L_0\)</span>-norm in their penalization behavior. We compare the <span class="math notranslate nohighlight">\(p\)</span>-norms with regard to their <em>optimizability</em> below:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>norm</p></th>
<th class="head"><p>continuous</p></th>
<th class="head"><p>differentiable</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(g(\vvec{x})=\lVert \vvec{x}\rVert^2\)</span></p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p><span class="math notranslate nohighlight">\(g(\vvec{x})=\lvert \vvec{x}\rvert\)</span></p></td>
<td><p>✅</p></td>
<td><p>❌</p></td>
</tr>
<tr class="row-even"><td><p><span class="math notranslate nohighlight">\(g(\vvec{x})=\lVert \vvec{x} \rVert_0\)</span></p></td>
<td><p>❌</p></td>
<td><p>❌</p></td>
</tr>
</tbody>
</table>
<p>We see that the <span class="math notranslate nohighlight">\(L_0\)</span>-norm is rather nasty to optimize, because it has a point of discontinuity at zero (see also the plot below), where it is also not differentiable (continuity follows from differentiability). The <span class="math notranslate nohighlight">\(L_1\)</span>-norm is at least contiuous everywhere, but it has a point of nondifferentiability at zero. Finally, the squared <span class="math notranslate nohighlight">\(L_2\)</span>-norm is continuous and differentiable everywhere. Note, that we use the squared <span class="math notranslate nohighlight">\(L_2\)</span> norm because the <span class="math notranslate nohighlight">\(L_2\)</span>-norm itself is also not differentiable at zero (for <span class="math notranslate nohighlight">\(x\in\mathbb{R}\)</span>, we have <span class="math notranslate nohighlight">\(\lVert x\rVert = \lvert x\rvert\)</span>). Hence, the <span class="math notranslate nohighlight">\(L_2\)</span>-norm is squared (as it is also done for the RSS), since squaring a function in an objective doesn’t change the set of minimizers.<br />
The plot below allows for a comparison of the penalization behavior of the various norms.</p>
<div class="figure" style="text-align: center"><p><img  src="_images/tikz-0463ad2275f0da36a67c283b5bae7086173a62be.svg" alt="Figure made with TikZ" /></p>
</div><p>We observe the behavior of the <span class="math notranslate nohighlight">\(L_0\)</span>-‘norm’, adding a penalization term of one for every nonzero entry of <span class="math notranslate nohighlight">\(x\)</span>. Regarding optimization, we can imagine that an <span class="math notranslate nohighlight">\(L_0\)</span> penalization term does not help when using numerical methods like gradient descent, since no information is given locally in what direction the penalization term is minimized. That is different for the <span class="math notranslate nohighlight">\(L_1\)</span> and <span class="math notranslate nohighlight">\(L_2\)</span> norms, whose negative gradients all point towards zero.<br />
The question arises now how the regression solutions change when adding <span class="math notranslate nohighlight">\(L_1\)</span> or <span class="math notranslate nohighlight">\(L_2\)</span> penalization terms, and how we can obtain those solutions. This, we are going to discuss in the next posts.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "None/None",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="regression_bias_var.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">The Bias-Variance Tradeoff</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="regression_ridge.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Ridge Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Sibylle Hess and Stiven Dias<br/>
  
      &copy; Copyright 2022. Eindhoven University of Technology.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>