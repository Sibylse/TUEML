
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Backpropagation &#8212; Data Mining and Machine Learning Jupyter Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "sign": "\\mathrm{sign}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Dimensionality Reduction Techniques" href="dim_reduction.html" />
    <link rel="prev" title="Multi-Layer Perceptrons" href="neuralnets_mlps.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Mining and Machine Learning Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="notation.html">
   Notation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="linalg.html">
   Linear Algebra
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_normed_vs.html">
     Normed Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="optimization.html">
   Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_problems.html">
     Optimization Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_convex.html">
     Convex Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_analytic.html">
     Analytic Solutions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_numerical.html">
     Numerical Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_gradients.html">
     Matrix Derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_objective.html">
     Regression Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_functions.html">
     Regression Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_optimization.html">
     Minimizing the RSS
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_bias_var.html">
     The Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_sparse.html">
     The Sparse Regression Task
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_ridge.html">
     Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_lasso.html">
     Lasso
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_L1vsL2.html">
     L1 vs L2 Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="classification.html">
   Classification
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_problem.html">
     Classification Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_knn.html">
     K-Nearest Neighbor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_naive_bayes.html">
     Naive Bayes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_decision_trees.html">
     Decision Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_random_forests.html">
     Random Forests
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_svms.html">
     Support Vector Machines
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_kernel_svm.html">
     Kernel SVM
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="neuralnets.html">
   Neural Networks
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_intro.html">
     Neural Networks Intro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_comp_graphs.html">
     Computational graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_func_approx.html">
     Function approximator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_mlps.html">
     Multi-Layer Perceptrons
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Backpropagation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="dim_reduction.html">
   Dimensionality Reduction Techniques
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_mf.html">
     Low Rank Matrix Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_matrix_completion.html">
     Matrix Completion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_pca.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means.html">
     k-Means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means_mf.html">
     k-Means is MF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_kernel_kmeans.html">
     Kernel k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_spectral.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://gitlab.tue.nl/20214358/dmml"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://gitlab.tue.nl/20214358/dmml/issues/new?title=Issue%20on%20page%20%2Fneuralnets_backprop.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/20214358/dmml/edit/master/neuralnets_backprop.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/neuralnets_backprop.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-functions">
   Loss functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   Gradient descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vectorized-backprop">
   Vectorized Backprop
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vectorized-backprop-in-mlps">
   Vectorized Backprop in MLPs
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Backpropagation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#loss-functions">
   Loss functions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gradient-descent">
   Gradient descent
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vectorized-backprop">
   Vectorized Backprop
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#vectorized-backprop-in-mlps">
   Vectorized Backprop in MLPs
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="backpropagation">
<h1>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this headline">#</a></h1>
<p>Neural networks are powerful function approximators, with thousands, millions or even billions of parameters that must be tuned (learned). But how do they learn these parameters from data?</p>
<p>In short, we need first to define a loss function indicating what the network should learn. More precisely, the loss function must express how good (or bad) the network is performing the task it is supposed to do during training. Thus, the selection of the loss function often depends on the kind of task the network must perform. For example, we typically use:</p>
<ul class="simple">
<li><p>the mean squared error (MSE) for regression tasks; and</p></li>
<li><p>the cross entropy for classification tasks.</p></li>
</ul>
<p>In the sequel, we must adjust the network parameters to reduce the training loss. This machine learning problem is commonly stated as an <em>optimization problem</em> and it is frequently solved by means of <em>gradient descent</em> methods. The backpropagation algorithm, in turn, provides a systematic procedure to compute gradients over the computational graph representing computations across an artificial neural network. Specifically, it employs the chain rule of calculus with a lot of <em>bookkeeping</em> to allow one to compute the gradient of the training loss w.r.t. each network parameter. Thus, backpropagation plays a major role to allow one to use gradient descent methods for learning the network parameters.</p>
<p><a class="reference internal" href="#learning_alg">Algorithm 10</a> summarizes this high-level learning procedure which employs the back propagation of the <em>prediction quality</em> – encoded in the loss function – to adjust the weights of the artificial synapses.</p>
<div class="proof algorithm admonition" id="learning_alg">
<p class="admonition-title"><span class="caption-number">Algorithm 10 </span> (Learning procedure)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p>Network structure and hyper-parameters.</p></li>
</ul>
<p><strong>Output</strong></p>
<ul class="simple">
<li><p>Trained / tunned network parameters.</p></li>
</ul>
<p><strong>Function</strong> Learn network parameters</p>
<ol class="simple">
<li><p>Initialize network parameters</p></li>
<li><p><strong>repeat</strong></p>
<ol class="simple">
<li><p>Compute the training loss, i.e. assess the network prediction quality.</p></li>
<li><p>Back propagate training loss gradients w.r.t. network parameters.</p></li>
<li><p>Change parameters a bit to reduce the loss. <em>(Gradient descent step)</em></p></li>
</ol>
</li>
<li><p><strong>until</strong> Reach some stop criteria</p></li>
<li><p><strong>return</strong> Trained network parameters</p></li>
</ol>
</section>
</div><section id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Permalink to this headline">#</a></h2>
<p>There are several loss functions we can use. For convenience, let <span class="math notranslate nohighlight">\( {\bf w} \)</span> be a long vector collecting all neural network parameters, i.e. the vector <span class="math notranslate nohighlight">\( {\bf w} \)</span> collect the parameters <span class="math notranslate nohighlight">\( \mathbf{W}^{1}, \mathbf{W}^{2}, \ldots, \mathbf{W}^{L} \)</span> from all network layers. Now, let <span class="math notranslate nohighlight">\( \hat{y} = f({\bf x}; {\bf w}) \)</span> be the network’s output for input vector <span class="math notranslate nohighlight">\( {\bf x} \)</span> and network parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span>. The loss function <span class="math notranslate nohighlight">\(L(y, \hat{y})\)</span> – a.k.a. risk or cost function – measures how well the prediction <span class="math notranslate nohighlight">\(\hat{y}\)</span> approximates the target <span class="math notranslate nohighlight">\(y\)</span>. Moreover, the training loss, i.e. the empirical risk, is computed using the training samples <div class="math notranslate nohighlight">
\[ {\cal D}_{train} = \bigcup_{i=1}^{N} \lbrace \left( {\bf x}_{i}, y_{i} \right) \rbrace \]</div>
 as</p>
<div class="math notranslate nohighlight" id="equation-empirical-risk">
<span class="eqno">(80)<a class="headerlink" href="#equation-empirical-risk" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
L_{train} &amp;=&amp; \frac{1}{N} \sum_{i=1}^{N} L \left( y_{i}, \hat{y}_{i} \right) \\
&amp;=&amp; \frac{1}{N} \sum_{i=1}^{N} L \left( y_{i}, f({\bf x}_{i}; {\bf w}) \right).
\end{eqnarray}\end{split}\]</div>
<p>As mentioned above, the most common loss for regression is the Mean Square Error (MSE). The squared error is defined as</p>
<div class="math notranslate nohighlight" id="equation-squared-error">
<span class="eqno">(81)<a class="headerlink" href="#equation-squared-error" title="Permalink to this equation">#</a></span>\[\begin{equation}
L(y, \hat{y}) = (\hat{y} - y)^2
\end{equation}\]</div>
<p>and its partial derivative w.r.t. estimate <span class="math notranslate nohighlight">\( \hat{y} \)</span> is given simple by</p>
<div class="amsmath math notranslate nohighlight" id="equation-ec7c3c4c-dbbc-4dcd-9e21-8327e673f5cc">
<span class="eqno">(82)<a class="headerlink" href="#equation-ec7c3c4c-dbbc-4dcd-9e21-8327e673f5cc" title="Permalink to this equation">#</a></span>\[\begin{equation}
{\partial L(y, \hat{y}) \over \partial \hat{y}} = 2 (\hat{y} - y).
\end{equation}\]</div>
<p>The Mean Squared Error (MSE) in turn is obtained by plugging <a class="reference internal" href="#equation-squared-error">(81)</a> into the empirical risk <a class="reference internal" href="#equation-empirical-risk">(80)</a></p>
<div class="amsmath math notranslate nohighlight" id="equation-e778dfca-47c1-490e-8cd0-627dfa62b95a">
<span class="eqno">(83)<a class="headerlink" href="#equation-e778dfca-47c1-490e-8cd0-627dfa62b95a" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
L_{train} &amp;=&amp; MSE(y, \hat{y}; {\bf w}) \\
&amp;=&amp; \frac{1}{N} \sum_{i=1}^{N} (\hat{y}_{i} - y_{i})^2  \\
 &amp;=&amp; \frac{1}{N} \sum_{i=1}^{N} (f({\bf x}_{i}; {\bf w}) - y_{i})^2.
\end{eqnarray}\]</div>
<p>On the other hand, a Cross Entropy (CE) loss function is frequently employed for classification tasks. But how do we setup a neural network as a classifier? Let <span class="math notranslate nohighlight">\(C\)</span> denote the number of classes among the network must decide. First, we employ <span class="math notranslate nohighlight">\(C\)</span> linear units at the last hidden layer of the network, i.e. the vector <span class="math notranslate nohighlight">\( {\bf h}^{L} \)</span> collecting the outputs of the last hidden layer has <span class="math notranslate nohighlight">\( C \)</span> elements. Then, we use a <em>softmax</em> activation to convert this vector into a probability distribution.</p>
<p>In the example of <a class="reference internal" href="#classification-mlp-fig"><span class="std std-numref">Fig. 28</span></a>, the network must decide among <span class="math notranslate nohighlight">\( C=3 \)</span> classes. Thus, <span class="math notranslate nohighlight">\(\mathbf{W}^{4}\)</span> must be a <span class="math notranslate nohighlight">\(H_L \times C\)</span> matrix (<span class="math notranslate nohighlight">\(C=3\)</span>) such that <div class="math notranslate nohighlight">
\[ {\bf h}^{4} = {\mathbf{W}^{4}}^{T} \phi_3 \Big( {\mathbf{W}^{3}}^{T} \phi_2 \Big( {\mathbf{W}^{2}}^{T} \phi_1 \Big( {\mathbf{W}^{1}}^{T} {\bf h}^{0} \Big) \Big) \Big) \]</div>
 is a <span class="math notranslate nohighlight">\( 3 \)</span>-element vector with <span class="math notranslate nohighlight">\( {\bf h}^{0} = \begin{bmatrix} 1 &amp; x_1 &amp; x_2 &amp; \ldots &amp; x_D \end{bmatrix}^{T} \)</span>.</p>
<figure class="align-left" id="classification-mlp-fig">
<a class="reference internal image-reference" href="_images/classification_MLP.svg"><img alt="_images/classification_MLP.svg" height="320px" src="_images/classification_MLP.svg" /></a>
<figcaption>
<p><span class="caption-number">Fig. 28 </span><span class="caption-text">A neural network with <span class="math notranslate nohighlight">\( C = 3 \)</span> linear units at its output layer. The <span class="math notranslate nohighlight">\( \sum \)</span> symbol indicates that the corresponding units is linear.</span><a class="headerlink" href="#classification-mlp-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The softmax activation converts an arbitrary input vector into a valid probability distribution. In precise mathematical terms, let <span class="math notranslate nohighlight">\({\bf h}=\begin{bmatrix} h_1 &amp; h_2 &amp; \ldots &amp; h_C \end{bmatrix}^{T}\)</span> be a <span class="math notranslate nohighlight">\(C\)</span>-dimensional vector. Then, the softmax of <span class="math notranslate nohighlight">\({\bf h}\)</span> is defined as</p>
<div class="math notranslate nohighlight" id="equation-softmax">
<span class="eqno">(84)<a class="headerlink" href="#equation-softmax" title="Permalink to this equation">#</a></span>\[\begin{equation}
\softmax({\bf h}) = \begin{bmatrix} \hat{y}_1 &amp; \hat{y}_2 &amp; \ldots &amp; \hat{y}_C \end{bmatrix}^{T},
\end{equation}\]</div>
<p>in which</p>
<div class="amsmath math notranslate nohighlight" id="equation-be2ff5c2-1ec7-44d1-acf3-0c9ad14d81de">
<span class="eqno">(85)<a class="headerlink" href="#equation-be2ff5c2-1ec7-44d1-acf3-0c9ad14d81de" title="Permalink to this equation">#</a></span>\[\begin{equation}
\hat{y}_{i} = {e^{h_{i}} \over \sum_{c=1}^{C} e^{h_{c}}}. 
\end{equation}\]</div>
<p>Note that the softmax <a class="reference internal" href="#equation-softmax">(84)</a> is defined such that it returns a valid categorical distribution, i.e. <div class="math notranslate nohighlight">
\[ \hat{y}_{i} \geq 0, \,\, \forall i \in \lbrace 1, 2, \ldots, C \rbrace, \]</div>
 and <div class="math notranslate nohighlight">
\[ \sum_{i=1}^{C} \hat{y}_{i} = 1. \]</div>
 Lastly, it is worth noting that the vector <span class="math notranslate nohighlight">\({\bf h}\)</span> is often called the <em>logits</em> of the softmax.</p>
<figure class="align-left" id="softmax-fig">
<a class="reference internal image-reference" href="_images/softmax.png"><img alt="_images/softmax.png" src="_images/softmax.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 29 </span><span class="caption-text">Softmax example using <em>NumPy</em> for array data handling and <em>Matplotlib</em> for visualization.</span><a class="headerlink" href="#softmax-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Let the vector <span class="math notranslate nohighlight">\( \hat{\bf y} = \begin{bmatrix} \hat{y}_1 &amp; \hat{y}_2 &amp; \ldots &amp; \hat{y}_C \end{bmatrix}^{T} \)</span> store the predicted categorical distribution <span class="math notranslate nohighlight">\( q(c) \)</span> over the <span class="math notranslate nohighlight">\( C \)</span> classes such that <span class="math notranslate nohighlight">\( q(c) = \hat{y}_c \)</span>, <span class="math notranslate nohighlight">\( \forall c \in \lbrace 1, \ldots, C \rbrace \)</span>. Note that <span class="math notranslate nohighlight">\( \hat{\bf y} \)</span> is the softmax of the last layer, i.e. <span class="math notranslate nohighlight">\( \hat{\bf y} = \softmax({\bf h}^{L}) \)</span>, where <span class="math notranslate nohighlight">\({\bf h}^{L}=\begin{bmatrix} h_1^{L} &amp; h_2^{L} &amp; \ldots &amp; h_C^{L} \end{bmatrix}^{T}\)</span> collect the values at the last hidden layer <span class="math notranslate nohighlight">\( L \)</span>. The Cross Entropy (CE) of the predicted distribution <span class="math notranslate nohighlight">\( q(c) \)</span> relative to the true distribution <span class="math notranslate nohighlight">\( p(c) = \left[ y = c \right] \)</span> (a distribution with no uncertainty at all) is defined as</p>
<div class="math notranslate nohighlight" id="equation-ce">
<span class="eqno">(86)<a class="headerlink" href="#equation-ce" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
L(y, \hat{\bf y}) &amp;=&amp; CE(y, \hat{\bf y}; {\bf w}) \\
%&amp;=&amp; CE(y, f({\bf x}; {\bf w}))  \\
&amp;\triangleq&amp; - \sum_{c=1}^{C} p(c) \log q(c)  \\
&amp;=&amp; - \sum_{c=1}^{C} \left[ y=c \right] \log \left( \hat{y}_{c} \right)  \\
&amp;=&amp; - \left\lbrace \underbrace{\left[ y=1 \right]}_{0} \log \left( \hat{y}_{1} \right) + \ldots + \underbrace{\left[ y=y \right]}_{1} \log \left( \hat{y}_{y} \right) + \ldots + \underbrace{\left[ y=C \right]}_{0} \log \left( \hat{y}_{C} \right) \right\rbrace  \\
&amp;=&amp; - \log \left( \hat{y}_{y} \right),
\end{eqnarray}\end{split}\]</div>
<p>in which <span class="math notranslate nohighlight">\( y \)</span> corresponds to the index of the true class. In this case, the CE <a class="reference internal" href="#equation-ce">(86)</a> is the negative log-probability of the true class. For a perfect prediction (<span class="math notranslate nohighlight">\( \hat{y}_{y} = 1 \)</span> and <span class="math notranslate nohighlight">\( \hat{y}_{c} = 0 \)</span> for <span class="math notranslate nohighlight">\( c \neq y \)</span>), the CE loss is zero (minimum). On the other hand, for an uncertain prediction (<span class="math notranslate nohighlight">\( 0 &lt; \hat{y}_{y} &lt; 1 \)</span>), CE will quickly approach <span class="math notranslate nohighlight">\( \infty \)</span> as <span class="math notranslate nohighlight">\( \hat{y}_{y} \rightarrow 0^{+} \)</span>, i.e. the CE loss strongly penalizes the predicted distribution if the predicted probability of the true-class <span class="math notranslate nohighlight">\( \hat{y}_{y} \)</span> is close to zero.</p>
<p>Therefore, the mean cross entropy, i.e the training loss, is obtained as</p>
<div class="amsmath math notranslate nohighlight" id="equation-9305918d-5456-4423-bcfd-5a12debfc06e">
<span class="eqno">(87)<a class="headerlink" href="#equation-9305918d-5456-4423-bcfd-5a12debfc06e" title="Permalink to this equation">#</a></span>\[\begin{equation}
L_{train} =  {1 \over N} \sum_{i=1}^{N} CE(y_{i}, \hat{\bf y}_{i}; {\bf w}),
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\( y_{i} \)</span> and <span class="math notranslate nohighlight">\( \hat{\bf y}_{i} = f({\bf x}_{i}; {\bf w})\)</span> are respectively the true class and the network output (probability) associated with the <span class="math notranslate nohighlight">\(i\)</span>-th training example <span class="math notranslate nohighlight">\( \left({\bf x}_{i}, y_{i} \right) \in {\cal D}_{train} \)</span>.</p>
<figure class="align-left" id="ce-fig">
<a class="reference internal image-reference" href="_images/MLP_CE_04.png"><img alt="_images/MLP_CE_04.png" src="_images/MLP_CE_04.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 30 </span><span class="caption-text">Computational graph representing a MLP with <span class="math notranslate nohighlight">\(4\)</span> hidden layers designed for classification (<span class="math notranslate nohighlight">\(C=3\)</span>). The last hidden layer comprises <span class="math notranslate nohighlight">\(3\)</span> linear units whose outputs are collected by the vector <span class="math notranslate nohighlight">\( {\bf h}^{4} \)</span>. The <span class="math notranslate nohighlight">\(\softmax\)</span> is applied to <span class="math notranslate nohighlight">\( {\bf h}^{4} \)</span> to build a valid categorical distribution stored in <span class="math notranslate nohighlight">\( \hat{\bf y} = \begin{bmatrix} \hat{y}_{1} &amp; \hat{y}_{2} &amp; \hat{y}_{3} \end{bmatrix}^{T} \)</span> which can be evaluated in turn by computing the cross entropy <span class="math notranslate nohighlight">\( CE(y, \hat{\bf y}; {\bf w}) \)</span> using the true class <span class="math notranslate nohighlight">\( y \)</span> as in <a class="reference internal" href="#equation-ce">(86)</a>.</span><a class="headerlink" href="#ce-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Let <span class="math notranslate nohighlight">\( {\bf h} = \begin{bmatrix} h_1 &amp; h_2 &amp; \ldots &amp; h_C \end{bmatrix}^{T} \)</span> collect the values at the last hidden layer, i.e. the logits of the softmax. To be able to learn, we need to compute the derivative of the cross-entropy. In the following, we use <span class="math notranslate nohighlight">\( CE = CE(y, \hat{\bf y}; {\bf w}) \)</span> for the sake of clarity. The derivative of the cross-entropy w.r.t. the value <span class="math notranslate nohighlight">\( h_j \)</span> of the <span class="math notranslate nohighlight">\(j\)</span>-th unit is given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
{\partial CE \over \partial h_j} =
\begin{cases}
 \hat{y}_j - 1   &amp; \text{if } j=y \\
 \hat{y}_j  &amp; \text{if } j\not=y.
\end{cases}
\end{align*}\]</div>
</section>
<section id="gradient-descent">
<h2>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">#</a></h2>
<p><a class="reference internal" href="#gradient_descent_alg">Algorithm 11</a> summarizes the gradiend descent procedure which optimizes the training loss w.r.t. the network parameters collected by the vector <span class="math notranslate nohighlight">\({\bf w}\)</span>.</p>
<div class="proof algorithm admonition" id="gradient_descent_alg">
<p class="admonition-title"><span class="caption-number">Algorithm 11 </span> (Training Loss Optimization procedure)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Inputs</strong></p>
<ul class="simple">
<li><p>Neural network structure with parameters <span class="math notranslate nohighlight">\({\bf w}\)</span>.</p></li>
<li><p>Training data <span class="math notranslate nohighlight">\( {\cal D}_{train} = \bigcup_{i=1}^{N} \lbrace \left( {\bf x}_i, y_i \right) \rbrace \)</span>.</p></li>
<li><p>Maximum number of training epochs <span class="math notranslate nohighlight">\(max\_epochs\)</span>.</p></li>
<li><p>Step-sizes <span class="math notranslate nohighlight">\(\lbrace \eta_k \rbrace \)</span>.</p></li>
</ul>
<p><strong>Output</strong></p>
<ul class="simple">
<li><p>Trained neural network parameters <span class="math notranslate nohighlight">\({\bf w}\)</span>.</p></li>
</ul>
<p><strong>Function</strong> GradientDescent (<span class="math notranslate nohighlight">\({\bf w}\)</span>, <span class="math notranslate nohighlight">\({\cal D}_{train}\)</span>, <span class="math notranslate nohighlight">\(max\_epochs\)</span>, <span class="math notranslate nohighlight">\(\eta_k\)</span>})</p>
<ol class="simple">
<li><p>Randomly initialize the weigths <span class="math notranslate nohighlight">\({\bf w}\)</span>.</p></li>
<li><p><strong>forall</strong> <span class="math notranslate nohighlight">\(k \in \lbrace 1, \ldots, max\_epochs \rbrace \)</span></p>
<ol class="simple">
<li><p>Compute network predictions for all <span class="math notranslate nohighlight">\(i \in \lbrace 1, \ldots, N \rbrace \)</span>:
<div class="math notranslate nohighlight">
\[ \triangleright \,\,\,\,\,\, \hat{\bf y}_{i} \gets f({\bf x}_{i}; {\bf w})\]</div>
</p></li>
<li><p>Compute the training loss:
<div class="math notranslate nohighlight">
\[ \triangleright \,\,\,\,\,\, L_{train} = {1 \over N} \sum_{i=1}^N L(y_i, \hat{\bf y}_{i})\]</div>
</p></li>
<li><p>Compute the gradient of the loss w.r.t. <span class="math notranslate nohighlight">\({\bf w}\)</span>:
<div class="math notranslate nohighlight">
\[ \triangleright \,\,\,\,\,\, \nabla_{\bf w} L_{train}\]</div>
 <em>(Requires backpropagation)</em></p></li>
<li><p>Update the weights:
<div class="math notranslate nohighlight">
\[ \triangleright \,\,\,\,\,\, {\bf w} \gets {\bf w} - \eta_{k} \, \nabla_{\bf w} L_{train}\]</div>
</p></li>
</ol>
</li>
<li><p><strong>return</strong> Trained neural network parameters <span class="math notranslate nohighlight">\({\bf w}\)</span>.</p></li>
</ol>
</section>
</div><p><a class="reference internal" href="#loss-ladscape-fig"><span class="std std-numref">Fig. 31</span></a> illustrates in turn how challenging could be optimizing the loss function for two parameters.</p>
<figure class="align-left" id="loss-ladscape-fig">
<a class="reference internal image-reference" href="_images/loss_landscape.png"><img alt="_images/loss_landscape.png" src="_images/loss_landscape.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 31 </span><span class="caption-text">Loss landscape of neural networks for a bi-dimensional parameter space (borrowed from <span id="id1">[<a class="reference internal" href="bibliography.html#id20" title="Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. arXiv preprint arXiv:1712.09913, 2017.">10</a>]</span>). There are several local minima and one global minima. Ordinary gradient descent implementations could be easily captured by a local minima.</span><a class="headerlink" href="#loss-ladscape-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Note that computing the gradient <span class="math notranslate nohighlight">\( \nabla_{\bf w} L_{train} \)</span> is key for <a class="reference internal" href="#gradient_descent_alg">Algorithm 11</a>. But how do we compute this gradient? First, let us defined it precisely. The gradient of the training loss <span class="math notranslate nohighlight">\(L_{train}\)</span> w.r.t. to the parameter vector <span class="math notranslate nohighlight">\( {\bf w} \)</span> is defined as</p>
<div class="amsmath math notranslate nohighlight" id="equation-e7d26445-7df6-4168-a89a-9e0acc11a457">
<span class="eqno">(88)<a class="headerlink" href="#equation-e7d26445-7df6-4168-a89a-9e0acc11a457" title="Permalink to this equation">#</a></span>\[\begin{equation}
 \nabla_{\bf w} L_{train} = \begin{bmatrix} \frac{\partial L_{train}}{\partial w_1} &amp; \frac{\partial L_{train}}{\partial w_2} &amp; \cdots \end{bmatrix}^{T}. 
\end{equation}\]</div>
<p>Note though that we can rewrite the overall gradient as a sum of sample-wise gradients</p>
<div class="amsmath math notranslate nohighlight" id="equation-85d74021-a8bc-4579-8793-0ed4921a81e4">
<span class="eqno">(89)<a class="headerlink" href="#equation-85d74021-a8bc-4579-8793-0ed4921a81e4" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
\nabla_{\bf w} L_{train} &amp;=&amp; \nabla_{\bf w}  \left( {1 \over N}  \sum_{i=1}^N L(y_{i}, \hat{\bf y}_{i}) \right)  \\
&amp;=&amp; {1 \over N}  \sum_{i=1}^N \nabla_{\bf w} L(y_{i}, \hat{\bf y}_{i}),
\end{eqnarray}\]</div>
<p>in which the gradient <span class="math notranslate nohighlight">\( \nabla_{\bf w} L(y_{i}, \hat{\bf y}_{i}) \)</span> of the <span class="math notranslate nohighlight">\(i\)</span>-th sample in <span class="math notranslate nohighlight">\( {\cal D}_{train} \)</span> is computed independently.</p>
<p>The gradient of a single sample <div class="math notranslate nohighlight">
\[ \nabla_{\bf w} L(y_{i}, \hat{\bf y}_{i}) \]</div>
 in turn can be computed using the backpropagation of error (backprop). <a class="reference internal" href="#backprop-fig"><span class="std std-numref">Fig. 32</span></a> illustrates the backpropagation procedure. First the input features in <span class="math notranslate nohighlight">\( {\bf x} \)</span> are feedforwarded to compute the network prediction <span class="math notranslate nohighlight">\(\hat{\bf y} \)</span> using the current network parameters stored in <span class="math notranslate nohighlight">\({\bf w}\)</span>. Then, the loss function <span class="math notranslate nohighlight">\( L(y, \hat{\bf y}) \)</span> is computed to evaluate the prediction quality against the true value <span class="math notranslate nohighlight">\( y \)</span>. In this sense, the loss function encodes the prediction error. Lastly, the prediction error is backpropagated. Specifically, the derivatives of the loss function are backpropagated to allow one to compute the loss function derivative <span class="math notranslate nohighlight">\(\frac{\partial L_{train}}{\partial w_j}\)</span> w.r.t any network parameter <span class="math notranslate nohighlight">\( w_j \)</span> in <span class="math notranslate nohighlight">\( {\bf w} \)</span>.</p>
<figure class="align-left" id="backprop-fig">
<a class="reference internal image-reference" href="_images/backprop_scheme_04.png"><img alt="_images/backprop_scheme_04.png" src="_images/backprop_scheme_04.png" style="height: 480px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 32 </span><span class="caption-text">Backpropagation procedure overview.</span><a class="headerlink" href="#backprop-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>But how do we compute the derivatives <span class="math notranslate nohighlight">\( \frac{\partial L_{train}}{\partial w_j} \)</span> of the loss function w.r.t. to each network parameters <span class="math notranslate nohighlight">\( w_j \)</span>? We employ the chain rule of calculus plus some <em>bookkeeping</em> to store partial derivatives along the paths linking this parameter to the network outputs.</p>
<div class="proof definition admonition" id="definition-2">
<p class="admonition-title"><span class="caption-number">Definition 32 </span> (Chain rule of calculus)</p>
<section class="definition-content" id="proof-content">
<p>Let the function <span class="math notranslate nohighlight">\( f(x) = f(g(x), h(x)) \)</span> be the composition of two functions <span class="math notranslate nohighlight">\( g(x) \)</span> and <span class="math notranslate nohighlight">\( h(x) \)</span>. Thus, the derivative of <span class="math notranslate nohighlight">\( f(\cdot) \)</span> w.r.t. the argument <span class="math notranslate nohighlight">\( x \)</span> can be computed as <div class="math notranslate nohighlight">
\[ {\partial f \over \partial x } = {\partial f \over \partial g } {\partial g \over \partial x } + {\partial f \over \partial h } {\partial h \over \partial x }. \]</div>
</p>
<p><a class="reference internal" href="#backprop-detail-fig"><span class="std std-numref">Fig. 33</span></a> shows how the derivatives are combined over the computational graph corresponding to function <span class="math notranslate nohighlight">\( f(x) = f(g(x), h(x)) \)</span>. Partial derivatives along the same backpropagation path between two node are multiplied to obtain <span class="math notranslate nohighlight">\( {\partial f \over \partial g } {\partial g \over \partial x } \)</span> and <span class="math notranslate nohighlight">\( {\partial f \over \partial h } {\partial h \over \partial x } \)</span>. Backpropagated derivatives arriving at the node <span class="math notranslate nohighlight">\( {\bf x} \)</span> are added up to obtain the final derivative <span class="math notranslate nohighlight">\( {\partial f \over \partial x } \)</span>.</p>
<figure class="align-left" id="backprop-detail-fig">
<a class="reference internal image-reference" href="_images/backprop_detail_simple.svg"><img alt="_images/backprop_detail_simple.svg" height="320px" src="_images/backprop_detail_simple.svg" /></a>
<figcaption>
<p><span class="caption-number">Fig. 33 </span><span class="caption-text">Backpropagation of partial derivatives.</span><a class="headerlink" href="#backprop-detail-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</div><p><a class="reference internal" href="#backprop-detail-small-fig1"><span class="std std-numref">Fig. 34</span></a> through <a class="reference internal" href="#backprop-detail-small-fig4"><span class="std std-numref">Fig. 37</span></a> detail the backpropagation procedure for a small network. It shows the backpropagated messages over the network. Note that the computational graph was expanded in the figures to turn the computation of each unit activation explicit.</p>
<figure class="align-left" id="backprop-detail-small-fig1">
<a class="reference internal image-reference" href="_images/backprop_detail_small_01.png"><img alt="_images/backprop_detail_small_01.png" src="_images/backprop_detail_small_01.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 34 </span><span class="caption-text">A single hidden layer network.</span><a class="headerlink" href="#backprop-detail-small-fig1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-left" id="backprop-detail-small-fig2">
<a class="reference internal image-reference" href="_images/backprop_detail_small_02.png"><img alt="_images/backprop_detail_small_02.png" src="_images/backprop_detail_small_02.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 35 </span><span class="caption-text">Backprop message <span class="math notranslate nohighlight">\( {\color{blue} B_{1} } \)</span>.</span><a class="headerlink" href="#backprop-detail-small-fig2" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-left" id="backprop-detail-small-fig3">
<a class="reference internal image-reference" href="_images/backprop_detail_small_03.png"><img alt="_images/backprop_detail_small_03.png" src="_images/backprop_detail_small_03.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 36 </span><span class="caption-text">Backprop messages <span class="math notranslate nohighlight">\( {\color{green} B_{2,1} } \)</span> and <span class="math notranslate nohighlight">\({\color{green} B_{2,2} }\)</span>.</span><a class="headerlink" href="#backprop-detail-small-fig3" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-left" id="backprop-detail-small-fig4">
<a class="reference internal image-reference" href="_images/backprop_detail_small_04.png"><img alt="_images/backprop_detail_small_04.png" src="_images/backprop_detail_small_04.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 37 </span><span class="caption-text">Backprop messagess <span class="math notranslate nohighlight">\({\color{darkred} B_{3,1} } \)</span> and <span class="math notranslate nohighlight">\({\color{darkred} B_{3,2} }\)</span>.</span><a class="headerlink" href="#backprop-detail-small-fig4" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>We detail the bookkeeping of backpropagated messages below.</p>
<p>I. Compute the message <span class="math notranslate nohighlight">\( {\color{blue} {B_1} } \)</span> as the derivative of the loss function (MSE or CE):</p>
<div class="amsmath math notranslate nohighlight" id="equation-539ac6cb-b0e4-4896-81d4-80975bee4b0f">
<span class="eqno">(90)<a class="headerlink" href="#equation-539ac6cb-b0e4-4896-81d4-80975bee4b0f" title="Permalink to this equation">#</a></span>\[\begin{equation}
 {\color{blue} \partial L \over \partial \hat{y}} \triangleq {\color{blue} {B_1} }.
\end{equation}\]</div>
<p>II. Compute the messages <span class="math notranslate nohighlight">\( {\color{green} B_{2,1} } \)</span> and <span class="math notranslate nohighlight">\( {\color{green} B_{2,2} } \)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-ac001a3e-7b52-424a-8058-561adfd25351">
<span class="eqno">(91)<a class="headerlink" href="#equation-ac001a3e-7b52-424a-8058-561adfd25351" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
 {\color{green} \partial L \over \partial h_i} &amp;=&amp; {\color{blue} \partial L \over \partial \hat{y}}{\partial \hat{y} \over \partial h_i}  \\
 &amp;=&amp; {\color{blue} B_1 } w^{2}_{i,1}  \\
 &amp;\triangleq&amp; {\color{green} B_{2,i}}.
\end{eqnarray}\]</div>
<p>III. Compute the messages <span class="math notranslate nohighlight">\({\color{darkred} B_{3,1} } \)</span> and <span class="math notranslate nohighlight">\({\color{darkred} B_{3,2} }\)</span> using the activation function derivative:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b7f10211-ba08-46fe-b5b5-c4f6a30c4acf">
<span class="eqno">(92)<a class="headerlink" href="#equation-b7f10211-ba08-46fe-b5b5-c4f6a30c4acf" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
 {\color{darkred} \partial L \over \partial a_i} &amp;=&amp; {\color{green} \partial L \over \partial h_i}{\partial h_i \over \partial a_i}  \\
 &amp;=&amp; {\color{green} B_{2,i} } \, \phi_1'(a_i)  \\
 &amp;\triangleq&amp; {\color{darkred} B_{3,i} }.
\end{eqnarray}\]</div>
<p>IV. Then, compute the partial derivatives of the loss w.r.t. the network parameters at the output layer:</p>
<div class="amsmath math notranslate nohighlight" id="equation-3c4905ee-035f-4219-82ea-31808061b828">
<span class="eqno">(93)<a class="headerlink" href="#equation-3c4905ee-035f-4219-82ea-31808061b828" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
 {\partial L \over \partial w^2_{i,1}} &amp;=&amp; {\color{blue} \partial L \over \partial \hat{y}}{\partial \hat{y} \over \partial w^2_{i,1}}   \\
&amp;\equiv&amp; {\color{blue} {B_1} } {h_i}.
\end{eqnarray}\]</div>
<p>VI. Finally, compute the partial derivatives of the loss w.r.t. the network parameters at the hidden layer:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6a10c288-4cb2-4cc2-9eaf-25b5cc6de78d">
<span class="eqno">(94)<a class="headerlink" href="#equation-6a10c288-4cb2-4cc2-9eaf-25b5cc6de78d" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\partial L \over \partial w^1_{i,j}} &amp;=&amp; {\color{darkred} \partial L \over \partial a_j}{\partial a_j \over \partial w^1_{i,j}}  \\
&amp;\equiv&amp; {\color{darkred} {B_{3,j}} } {x_i}.
\end{eqnarray}\]</div>
<div class="proof definition admonition" id="definition-3">
<p class="admonition-title"><span class="caption-number">Definition 33 </span> (Jacobian matrix)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(y = f(x)\)</span> be a single input, single output (SISO) function <span class="math notranslate nohighlight">\(f \colon \mathbb{R} \rightarrow \mathbb{R}\)</span>. The derivative of <span class="math notranslate nohighlight">\(f\)</span> is written as <div class="math notranslate nohighlight">
\[ f' \triangleq {\partial f \over \partial x}. \]</div>
 Now, let <span class="math notranslate nohighlight">\(f \colon \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span> denote a multiple input, multiple output (MIMO) function, i.e.</p>
<div class="amsmath math notranslate nohighlight" id="equation-c62502ff-7ffc-4b20-b73c-7ea0745a2843">
<span class="eqno">(95)<a class="headerlink" href="#equation-c62502ff-7ffc-4b20-b73c-7ea0745a2843" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\bf y} &amp;=&amp; f({\bf x})  \\
&amp;=&amp; \begin{bmatrix} f_1({\bf x}) &amp; \ldots &amp; f_m({\bf x})\end{bmatrix}^{T}.  
\end{eqnarray}\]</div>
<p>The <em>derivative</em> of <span class="math notranslate nohighlight">\(f\)</span> w.r.t. to its argument <span class="math notranslate nohighlight">\( {\bf x} = \begin{bmatrix} x_{1} &amp; \ldots &amp; x_{n} \end{bmatrix}^{T} \)</span> is defined by the <strong>Jacobian matrix</strong>. More precisely, the Jacobian matrix <span class="math notranslate nohighlight">\({\bf J}\)</span> is a <span class="math notranslate nohighlight">\(m \times n\)</span> matrix containing all <em>partial</em> derivatives</p>
<div class="amsmath math notranslate nohighlight" id="equation-e2e730f6-8f3b-4f37-a5bf-e5f972396151">
<span class="eqno">(96)<a class="headerlink" href="#equation-e2e730f6-8f3b-4f37-a5bf-e5f972396151" title="Permalink to this equation">#</a></span>\[\begin{equation}
{\bf J} = 
\begin{bmatrix}
{\partial f_1 \over \partial x_1} &amp; \ldots &amp; {\partial f_1 \over \partial x_n} \\
\vdots &amp;  \ddots &amp; \vdots \\
{\partial f_m \over \partial x_1} &amp; \ldots &amp; {\partial f_m \over \partial x_n}
\end{bmatrix}.
\end{equation}\]</div>
</section>
</div></section>
<section id="vectorized-backprop">
<h2>Vectorized Backprop<a class="headerlink" href="#vectorized-backprop" title="Permalink to this headline">#</a></h2>
<p>Now, let <span class="math notranslate nohighlight">\({\color{green}\mathbf{v}}\)</span> and <span class="math notranslate nohighlight">\({\color{darkred} \mathbf{v'}}\)</span> be two sets containing respectively <span class="math notranslate nohighlight">\(n\)</span> and <span class="math notranslate nohighlight">\(m\)</span> units of a neural network (possibly from different layers). Moreover, let  <span class="math notranslate nohighlight">\(f({\color{green}\mathbf{v}}) = {\color{darkred} \mathbf{v'}}\)</span> be a function <span class="math notranslate nohighlight">\(f \colon \mathbb{R}^n \mapsto \mathbb{R}^m\)</span> mapping values from <span class="math notranslate nohighlight">\({\color{green}\mathbf{v}}\)</span> into values at  <span class="math notranslate nohighlight">\({\color{darkred} \mathbf{v'}}\)</span>. If all computational paths from the units in <span class="math notranslate nohighlight">\({\color{green}\mathbf{v}}\)</span> to the loss function <span class="math notranslate nohighlight">\(L\)</span> (at the output) go over the units in <span class="math notranslate nohighlight">\({\color{darkred}\mathbf{v}'}\)</span>, then we can write</p>
<div class="math notranslate nohighlight" id="equation-jacob-magic">
<span class="eqno">(97)<a class="headerlink" href="#equation-jacob-magic" title="Permalink to this equation">#</a></span>\[\begin{equation}
{\color{green} \nabla_{\mathbf{v}} L} =  {\color{blue} \mathbf{J}^{T}} {\color{darkred}\nabla_{\mathbf{v}'} L}.
\end{equation}\]</div>
<p>That is, the Gradient of the loss <span class="math notranslate nohighlight">\(L\)</span> w.r.t. <span class="math notranslate nohighlight">\({\color{green}\mathbf{v}}\)</span> is the Gradient of the loss <span class="math notranslate nohighlight">\(L\)</span> w.r.t. <span class="math notranslate nohighlight">\({\color{darkred}\mathbf{v}'}\)</span> pre-multiplied by the Jacobian of <span class="math notranslate nohighlight">\(f\)</span> transposed. Thus, it suffices to know the Jacobian of <span class="math notranslate nohighlight">\(f\)</span> to backpropagate the Gradients from units ahead (<span class="math notranslate nohighlight">\({\color{darkred} \mathbf{v}'}\)</span>) to units backwards (<span class="math notranslate nohighlight">\({\color{green}\mathbf{v}}\)</span>) in the network. Note that this is similar to the <span class="math notranslate nohighlight">\(1D\)</span> case in which <span class="math notranslate nohighlight">\(L = L(f({\color{green}v})) = L({\color{darkred}v'})\)</span> and</p>
<div class="amsmath math notranslate nohighlight" id="equation-509f6dcb-6f41-42a4-a9c0-d6c8cc49876f">
<span class="eqno">(98)<a class="headerlink" href="#equation-509f6dcb-6f41-42a4-a9c0-d6c8cc49876f" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
 {\color{green} \partial L \over \partial v} &amp;=&amp; {\color{darkred} \partial L \over \partial v'}{\color{blue} \partial v' \over \partial v}  \\
 &amp;=&amp; {\color{blue} f'} {\color{darkred} \partial L \over \partial v'}.
\end{eqnarray}\]</div>
<p><a class="reference internal" href="#vectorized-backprop-fig"><span class="std std-numref">Fig. 38</span></a> compares the <span class="math notranslate nohighlight">\(1D\)</span> backpropagation procedure with the vectorized backpropagation over a simplified computational graph in which grouped units (circles) in <span class="math notranslate nohighlight">\({\color{darkred} \mathbf{v}'}\)</span> and <span class="math notranslate nohighlight">\({\color{green}\mathbf{v}}\)</span> are enclosed by boxes. In the former case, we compute the Gradient at a single unit <span class="math notranslate nohighlight">\( {\color{green}v} \)</span> using the gradient <span class="math notranslate nohighlight">\( {\color{blue} f'} \)</span> of the SISO function<span class="math notranslate nohighlight">\( f: \mathbb{R} \rightarrow \mathbb{R} \)</span> mapping the values at this unit to the values at the unit ahead <span class="math notranslate nohighlight">\({\color{darkred}v'}\)</span>. In the later case, we compute the Gradient at the units <span class="math notranslate nohighlight">\({\color{green}\mathbf{v}}\)</span> using the Jacobian <span class="math notranslate nohighlight">\( {\color{blue} \mathbf{J}^{T}} \)</span> of the MIMO function <span class="math notranslate nohighlight">\( f: \mathbb{R}^{n} \rightarrow \mathbb{R}^{m} \)</span> summarizing all computational paths of these units to the units ahead <span class="math notranslate nohighlight">\({\color{darkred} \mathbf{v}'}\)</span>.</p>
<figure class="align-left" id="vectorized-backprop-fig">
<a class="reference internal image-reference" href="_images/jacobian.svg"><img alt="_images/jacobian.svg" height="640px" src="_images/jacobian.svg" /></a>
<figcaption>
<p><span class="caption-number">Fig. 38 </span><span class="caption-text">Vectorized backpropagation of the Gradients.</span><a class="headerlink" href="#vectorized-backprop-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="vectorized-backprop-in-mlps">
<h2>Vectorized Backprop in MLPs<a class="headerlink" href="#vectorized-backprop-in-mlps" title="Permalink to this headline">#</a></h2>
<p>Note that MLPs have <strong>only two</strong> vectorized operations. Specifically, at a given layer <span class="math notranslate nohighlight">\(\ell\)</span>, we have</p>
<ul class="simple">
<li><p>A matrix multiplication <span class="math notranslate nohighlight">\({\bf a}^{\ell} = {{\bf W}^{\ell}}^{T} {\bf h}^{\ell-1}\)</span>; and</p></li>
<li><p>An element-wise nonlinearity <span class="math notranslate nohighlight">\({\bf h}^{\ell} = \phi_{\ell}({\bf a}^{\ell})\)</span>.</p></li>
</ul>
<p>Note though that the Jacobian of a <strong>matrix multiplication</strong> of the type <span class="math notranslate nohighlight">\( {\bf y} = f({\bf x}) = {\bf W} {\bf x}\)</span> is just the matrix <span class="math notranslate nohighlight">\( {\bf W} \)</span> defining the affine transform, whereas the Jacobian of an <strong>element-wise nonlinearity</strong> of the type <span class="math notranslate nohighlight">\({\bf y} = f({\bf x})\)</span> is a diagonal matrix <span class="math notranslate nohighlight">\( {\bf J} = \mathrm{diag}(f'({\bf x})) \)</span>.</p>
<p>More explicitly, let the matrix multiplication</p>
<div class="amsmath math notranslate nohighlight" id="equation-1ce198a6-abde-40e5-bcc3-26fa14eff8d8">
<span class="eqno">(99)<a class="headerlink" href="#equation-1ce198a6-abde-40e5-bcc3-26fa14eff8d8" title="Permalink to this equation">#</a></span>\[\begin{equation}
\underbrace{\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}}_{\bf y}
=
\underbrace{\begin{bmatrix}
w_{11} &amp; w_{12} &amp; \dots  &amp; w_{1m} \\
w_{21} &amp; w_{22} &amp; \dots  &amp; w_{2m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{n1} &amp; w_{n2} &amp; \dots  &amp; w_{nm}
\end{bmatrix}}_{\bf W}
\underbrace{\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_m
\end{bmatrix}}_{\bf x}
\end{equation}\]</div>
<p>define a linear function of the type <span class="math notranslate nohighlight">\({\bf y} = f({\bf x}) = \mathbf{W} {\bf x}\)</span>. The Jacobian of this matrix multiplication operation is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-11d9cfc1-6e40-4909-b90a-981664020b8a">
<span class="eqno">(100)<a class="headerlink" href="#equation-11d9cfc1-6e40-4909-b90a-981664020b8a" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\bf J}
&amp;=&amp;
\begin{bmatrix}
{\partial y_1 \over \partial x_1} &amp; {\partial y_1 \over \partial x_2} &amp; \dots  &amp; {\partial y_1 \over \partial x_m} \\
{\partial y_2 \over \partial x_1} &amp; {\partial y_2 \over \partial x_2} &amp; \dots  &amp; {\partial y_2 \over \partial x_m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
{\partial y_n \over \partial x_1} &amp; {\partial y_n \over \partial x_2} &amp; \dots  &amp; {\partial y_n \over \partial x_m} \\
\end{bmatrix}  \\
&amp;=&amp;
\underbrace{\begin{bmatrix}
w_{11} &amp; w_{12} &amp; \dots  &amp; w_{1m} \\
w_{21} &amp; w_{22} &amp; \dots  &amp; w_{2m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
w_{n1} &amp; w_{n2} &amp; \dots  &amp; w_{nm}
\end{bmatrix}}_{\bf W}. 
\end{eqnarray}\]</div>
<p>On the other hand, let the function <span class="math notranslate nohighlight">\({\bf y} = f({\bf x}) = \begin{bmatrix} f(x_1) &amp; f(x_2) &amp; \ldots &amp; f(x_n) \end{bmatrix}^{T} \)</span> represent an element-wise nonlinearity applied to the input vector <span class="math notranslate nohighlight">\( {\bf x} = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_n \end{bmatrix}^{T} \)</span>. The Jacobian of this element-wise nonlinearity is given</p>
<div class="math notranslate nohighlight" id="equation-jacobian-magic2">
<span class="eqno">(101)<a class="headerlink" href="#equation-jacobian-magic2" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
{\bf J}
&amp;=&amp;
\begin{bmatrix}
{\partial y_1 \over \partial x_1} &amp; {\partial y_1 \over \partial x_2} &amp; \dots  &amp; {\partial y_1 \over \partial x_n} \\
{\partial y_2 \over \partial x_1} &amp; {\partial y_2 \over \partial x_2} &amp; \dots  &amp; {\partial y_2 \over \partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
{\partial y_n \over \partial x_1} &amp; {\partial y_n \over \partial x_2} &amp; \dots  &amp; {\partial y_n \over \partial x_n} \\
\end{bmatrix}  \\
&amp;=&amp;
\underbrace{\begin{bmatrix}
f'(x_1) &amp; 0       &amp; \dots  &amp; 0 \\
0       &amp; f'(x_2) &amp; \dots  &amp; 0 \\
\vdots  &amp; \vdots  &amp; \ddots &amp; \vdots \\
0       &amp; 0       &amp; 0      &amp; f'(x_n)
\end{bmatrix}}_{\mathrm{diag}(f'({\bf x}))}. 
\end{eqnarray}\end{split}\]</div>
<div class="proof example admonition" id="example-4">
<p class="admonition-title"><span class="caption-number">Example 23 </span> (Vectorized backprop over a small MLP)</p>
<section class="example-content" id="proof-content">
<p><a class="reference internal" href="#vectorized-backprop2-fig"><span class="std std-numref">Fig. 39</span></a> shows a MLP with <span class="math notranslate nohighlight">\(3\)</span> non-linear hidden layers and one linear output layer.</p>
<figure class="align-left" id="vectorized-backprop2-fig">
<a class="reference internal image-reference" href="_images/multi_layer_MLP_07.png"><img alt="_images/multi_layer_MLP_07.png" src="_images/multi_layer_MLP_07.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 39 </span><span class="caption-text">Example of a small MLPs to illustrate the vectorized backpropagation procedure.</span><a class="headerlink" href="#vectorized-backprop2-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>In the sequel, we detail the feedforward evaluation and the backpropagation of error procedures for the small MLP shown in <a class="reference internal" href="#vectorized-backprop2-fig"><span class="std std-numref">Fig. 39</span></a>. First, remember that the network output <span class="math notranslate nohighlight">\( \hat{y} = f({\bf x}) \)</span> for a given input vector <span class="math notranslate nohighlight">\({\bf x} = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_D \end{bmatrix}^{T} \)</span> is recursively computed – from the inputs to the output – by applying a linear transformation <span class="math notranslate nohighlight">\( \mathbf{a}^\ell = {\mathbf{W}^{\ell}}^{T} \mathbf{h}^{\ell-1} \)</span> followed by an element-wise nonlinearity <span class="math notranslate nohighlight">\( \mathbf{h}^\ell = \phi_\ell(\mathbf{a}^\ell) \)</span> at each layer <span class="math notranslate nohighlight">\( \ell \)</span>. For the MLP shown in <a class="reference internal" href="#vectorized-backprop2-fig"><span class="std std-numref">Fig. 39</span></a>, we have the following feedforward computation steps</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\( \mathbf{a}^1 = {\mathbf{W}^{1}}^{T} \mathbf{h}^0 \)</span> with <span class="math notranslate nohighlight">\( \mathbf{h}^0 \triangleq \begin{bmatrix} 1 &amp; {\bf x}^{T} \end{bmatrix}^{T} \equiv \begin{bmatrix} 1 &amp; x_1 &amp; x_2 &amp; \ldots &amp; x_D \end{bmatrix}^{T} \)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{h}^1 = \phi_1(\mathbf{a}^1) \)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{a}^2 = {\mathbf{W}^2}^{T} \mathbf{h}^1 \)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{h}^2 = \phi_2(\mathbf{a}^2) \)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{a}^3 = {\mathbf{W}^3}^{T} \mathbf{h}^2 \)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\( \mathbf{h}^3 = \phi_3(\mathbf{a}^3) \)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\( \hat{y}={\mathbf{W}^4}^{T} \mathbf{h}^3 \)</span>.</p></li>
</ol>
<p>We can summarize the feedforward network evaluation in the following nested expression
<div class="math notranslate nohighlight">
\[
\hat{y} = \underbrace{{\mathbf{W}^{4}}^{T} 
\underbrace{\phi_3 \Big(
\underbrace{{\mathbf{W}^{3}}^{T}
\underbrace{\phi_2 \Big(
\underbrace{{\mathbf{W}^{2}}^{T} 
\underbrace{\phi_1 \Big(
\underbrace{{\mathbf{W}^{1}}^{T} {\bf h}^0}_{\mathbf{a}^1}
\Big)}_{\mathbf{h}^1=\phi_1(\mathbf{a}^1)}}_{\mathbf{a}^2= {\mathbf{W}^2}^{T} \mathbf{h}^1}
\Big)}_{\mathbf{h}^2=\phi_2(\mathbf{a}^2)}}_{\mathbf{a}^3={\mathbf{W}^3}^{T} \mathbf{h}^2}
\Big)}_{\mathbf{h}^3=\phi_3(\mathbf{a}^3)}}_{\hat{y}={\mathbf{W}^4}^{T} \mathbf{h}^3}
\]</div>

in which the layer outputs <span class="math notranslate nohighlight">\( {\bf h}^{1} \)</span>, <span class="math notranslate nohighlight">\( {\bf h}^{2} \)</span> and <span class="math notranslate nohighlight">\( {\bf h}^{3} \)</span> contain the intermediate results of the feedforward evaluation procedure. For convenience, the activations <span class="math notranslate nohighlight">\( {\bf a}^{1} \)</span>, <span class="math notranslate nohighlight">\( {\bf a}^{2} \)</span> and <span class="math notranslate nohighlight">\( {\bf a}^{3} \)</span> are also stored for the backpropagation procedure.</p>
<p>Now, let the operator <span class="math notranslate nohighlight">\(\odot\)</span> denote the element-wise multiplication such that</p>
<div class="math notranslate nohighlight" id="equation-odot">
<span class="eqno">(102)<a class="headerlink" href="#equation-odot" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{equation}
{\bf A} \odot {\bf B} \triangleq 
\begin{bmatrix} 
a_{1,1} b_{1,1} &amp; a_{1,2} b_{1,2} &amp; \ldots &amp; a_{1,n} b_{1,n} \\
a_{2,1} b_{2,1} &amp; a_{2,2} b_{2,2} &amp; \ldots &amp; a_{2,n} b_{2,n} \\ 
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m,1} b_{m,1} &amp; a_{m,2} b_{m,2} &amp; \ldots &amp; a_{m,n} b_{m,n} \\ 
\end{bmatrix}^{T} 
\end{equation}\end{split}\]</div>
<p>for any pair of <span class="math notranslate nohighlight">\(m \times n\)</span> matrices <span class="math notranslate nohighlight">\( {\bf A} \)</span> and <span class="math notranslate nohighlight">\( {\bf B} \)</span>. Furthermore, let us assume that the gradient <span class="math notranslate nohighlight">\(\nabla_{\hat{y}} L\)</span> of the loss function w.r.t. to the network output <span class="math notranslate nohighlight">\( \hat{y} \)</span> is given in closed form and can be easily computed. Then, we can recursively compute the gradients from the output to the inputs – backpropagation of error – using the following steps</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\nabla_{{\bf h}^3} L = \left( {\mathbf{W}^4}^{T} \right)^{^T} \nabla_{\hat{y}} L \therefore \nabla_{{\bf h}^3} L = \mathbf{W}^4 \nabla_{\hat{y}} L\)</span> in which <span class="math notranslate nohighlight">\( \mathbf{W}^4 \)</span> is the Jacobian of the affine transform (see <a class="reference internal" href="#equation-jacob-magic">(97)</a>);</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_{\mathbf{a}^3} L = \mathrm{diag}(\phi_{3}'(\mathbf{a}^3)) \nabla_{{\bf h}^3} L \therefore \nabla_{\mathbf{a}^3} L = \phi_{3}'(\mathbf{a}^3) \odot \nabla_{{\bf h}^3} L \)</span> by combining <a class="reference internal" href="#equation-jacobian-magic2">(101)</a> and <a class="reference internal" href="#equation-odot">(102)</a>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_{{\bf h}^2} L = \mathbf{W}^3 \nabla_{\mathbf{a}^3} L\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_{\mathbf{a}^2} L = \phi_{2}'(\mathbf{a}^2) \odot \nabla_{{\bf h}^2} L\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_{{\bf h}^1} L = \mathbf{W}^2 \nabla_{\mathbf{a}^2} L\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_{\mathbf{a}^1} L = \phi_{1}'(\mathbf{a}^1) \odot \nabla_{{\bf h}^1} L\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(\nabla_{{\bf h}^0} L = \mathbf{W}^1 \nabla_{\mathbf{a}^1} L\)</span>.</p></li>
</ol>
<p>Alternatively, we can write
<div class="math notranslate nohighlight">
\[
\nabla_{{\bf h}^0} L = \underbrace{\mathbf{W}^1 \Big( 
\underbrace{\phi_{1}'(\mathbf{a}^1) \odot \Big(
\underbrace{\mathbf{W}^2 \Big( 
\underbrace{\phi_{2}'(\mathbf{a}^2) \odot \Big(
\underbrace{\mathbf{W}^3 \Big(
\underbrace{\phi_{3}'(\mathbf{a}^3) \odot \Big(
\underbrace{\mathbf{W}^4 \nabla_{\hat{y}} L}_{\nabla_{{\bf h}^3} L}
\Big)}_{\nabla_{\mathbf{a}^3} L = \phi_{3}'(\mathbf{a}^3) \odot \nabla_{{\bf h}^3} L}
\Big)}_{\nabla_{{\bf h}^2} L = \mathbf{W}^3 \nabla_{\mathbf{a}^3} L}
\Big)}_{\nabla_{\mathbf{a}^2} L = \phi_{2}'(\mathbf{a}^2) \odot \nabla_{{\bf h}^2} L}
\Big)}_{\nabla_{{\bf h}^1} L = \mathbf{W}^2 \nabla_{\mathbf{a}^2} L}
\Big)}_{\nabla_{\mathbf{a}^1} L = \phi_{1}'(\mathbf{a}^1) \odot \nabla_{{\bf h}^1} L}
\Big)}_{\nabla_{{\bf h}^0} L = \mathbf{W}^1 \nabla_{\mathbf{a}^1} L}
\]</div>

to stress how gradients of the loss function <span class="math notranslate nohighlight">\(L\)</span> are backpropagated to compute the intermediate gradients <span class="math notranslate nohighlight">\( \nabla_{{\bf h}^3} L \)</span>, <span class="math notranslate nohighlight">\( \nabla_{{\bf h}^2} L\)</span>, <span class="math notranslate nohighlight">\( \nabla_{{\bf h}^1} L \)</span> and <span class="math notranslate nohighlight">\( \nabla_{{\bf h}^0} L \)</span> from the gradient <span class="math notranslate nohighlight">\(\nabla_{\hat{y}} L\)</span>. Note that the activation vectors <span class="math notranslate nohighlight">\( {\bf a}^{1} \)</span>, <span class="math notranslate nohighlight">\( {\bf a}^{2} \)</span> and <span class="math notranslate nohighlight">\( {\bf a}^{3} \)</span> obtained in the feedforward evaluation procedure are required to compute the gradients <span class="math notranslate nohighlight">\( \phi_{1}'({\bf a}^{1}) \)</span>, <span class="math notranslate nohighlight">\( \phi_{2}'({\bf a}^{2}) \)</span> and <span class="math notranslate nohighlight">\( \phi_{3}'({\bf a}^{3}) \)</span>.</p>
<p>In the sequel, we compute the gradient of the loss <span class="math notranslate nohighlight">\(L\)</span> w.r.t. the network parameters <span class="math notranslate nohighlight">\( {\bf W}^{1} \)</span>, <span class="math notranslate nohighlight">\( {\bf W}^{2} \)</span>, <span class="math notranslate nohighlight">\( {\bf W}^{3} \)</span> and <span class="math notranslate nohighlight">\( {\bf W}^{4} \)</span> are computed as</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( {\partial L \over \partial w_{i,1}^{4}} = {\partial L \over \partial \hat{y}} {\partial \hat{y} \over \partial w_{i,1}^{4}} = {\partial L \over \partial \hat{y}} h_{i}^{3} \,\,\,\,\,\therefore\,\, \nabla_{{\bf W}^{4}} L = {\bf h}^{3} \, \nabla_{\hat{y}} L\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\( {\partial L \over \partial w_{i,j}^{3}} = {\partial L \over \partial a_{j}^{3}} {\partial a_{j}^{3} \over \partial w_{i,j}^{3}} = {\partial L \over \partial a_{j}^{3}} h_{i}^{2} \,\,\therefore\,\, \nabla_{{\bf W}^{3}} L = {\bf h}^{2} \left( \nabla_{\mathbf{a}^{3}} L \right)^{T}\)</span>; (outer product, check this as an exercise)</p></li>
<li><p><span class="math notranslate nohighlight">\( {\partial L \over \partial w_{i,j}^{2}} = {\partial L \over \partial a_{j}^{2}} {\partial a_{j}^{2} \over \partial w_{i,j}^{2}} = {\partial L \over \partial a_{j}^{2}} h_{i}^{1} \,\,\therefore\,\, \nabla_{{\bf W}^{2}} L = {\bf h}^{1} \left( \nabla_{\mathbf{a}^{2}} L \right)^{T}\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\( {\partial L \over \partial w_{i,j}^{1}} = {\partial L \over \partial a_{j}^{1}} {\partial a_{j}^{1} \over \partial w_{i,j}^{1}} = {\partial L \over \partial a_{j}^{1}} h_{i}^{0} \,\,\therefore\,\, \nabla_{{\bf W}^{1}} L = {\bf h}^{0} \left( \nabla_{\mathbf{a}^{1}} L \right)^{T}\)</span>.
Note therefore that we also need to bookkeep the hidden layers’ outputs <span class="math notranslate nohighlight">\( {\bf h}^{1} \)</span>, <span class="math notranslate nohighlight">\( {\bf h}^{2} \)</span> and <span class="math notranslate nohighlight">\( {\bf h}^{3} \)</span> from the feedforward evaluation procedure as weel as the gradients <span class="math notranslate nohighlight">\( \nabla_{\mathbf{a}^{1}} L \)</span>, <span class="math notranslate nohighlight">\( \nabla_{\mathbf{a}^{2}} L \)</span>, <span class="math notranslate nohighlight">\( \nabla_{\mathbf{a}^{3}} L\)</span> and <span class="math notranslate nohighlight">\(\nabla_{\hat{y}} L\)</span> from the backpropagation procedure.</p></li>
</ul>
<p>The feedforward evaluation and backpropagation procedures are performed for each training example <span class="math notranslate nohighlight">\( \left( {\bf x}_{i}, y_{i} \right) \in {\cal D}_{train} \)</span>. Finally, we compute the gradient of the empirical training loss <span class="math notranslate nohighlight">\( L_{train} \)</span> w.r.t. parameters <span class="math notranslate nohighlight">\( {\bf W}^{\ell} \)</span> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-70ad2c14-3faa-48ca-b37e-c844c3b591aa">
<span class="eqno">(103)<a class="headerlink" href="#equation-70ad2c14-3faa-48ca-b37e-c844c3b591aa" title="Permalink to this equation">#</a></span>\[\begin{equation}
\nabla_{{\bf W}^{\ell}} L_{train} = \frac{1}{N} \sum_{i=1}^{N} \nabla_{{\bf W}^{\ell}} L (y_{i}, \hat{y}_{i})
\end{equation}\]</div>
<p>and update the parameters at the <span class="math notranslate nohighlight">\(\ell\)</span>-th layer as</p>
<div class="amsmath math notranslate nohighlight" id="equation-720bc8cf-b0eb-465d-92e0-5824709cd7f8">
<span class="eqno">(104)<a class="headerlink" href="#equation-720bc8cf-b0eb-465d-92e0-5824709cd7f8" title="Permalink to this equation">#</a></span>\[\begin{equation}
{\bf W}^{\ell} \gets {\bf W}^{\ell} - \eta_{k} \nabla_{{\bf W}^{\ell}} L_{train},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\( \eta_{k} \)</span> is the learning rate used in the <span class="math notranslate nohighlight">\( k \)</span>-th step of the standard gradient descent method, a.k.a. batch gradient descent as it employs the whole training dataset <span class="math notranslate nohighlight">\( {\cal D}_{train} \)</span> at each step <span class="math notranslate nohighlight">\( k \)</span> to update the network parameters.</p>
</section>
</div><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We can empirically check the correctness of our analytical derivations of the derivatives employed by the backprop procedure. More precisely, let <span class="math notranslate nohighlight">\(f({\bf x}) = f(x_1, x_2, \dots, x_D)\)</span> be a <span class="math notranslate nohighlight">\(D\)</span>-dimensional function. Its gradient</p>
<div class="amsmath math notranslate nohighlight" id="equation-96173ce7-8334-479e-8854-b1382c0605e9">
<span class="eqno">(105)<a class="headerlink" href="#equation-96173ce7-8334-479e-8854-b1382c0605e9" title="Permalink to this equation">#</a></span>\[\begin{equation}
\nabla_{{\bf x}} f = \begin{bmatrix} {\partial f \over \partial x_1} &amp; {\partial f \over \partial x_2} &amp; \ldots &amp; {\partial f \over \partial x_D} \end{bmatrix}^{T}
\end{equation}\]</div>
<p>w.r.t. <span class="math notranslate nohighlight">\( {\bf x} \)</span> can be approximated – at each dimension <span class="math notranslate nohighlight">\( i \)</span> – using finite differences as</p>
<div class="amsmath math notranslate nohighlight" id="equation-f90b4b64-3a6d-44a7-b76d-e59e9a6e0f9b">
<span class="eqno">(106)<a class="headerlink" href="#equation-f90b4b64-3a6d-44a7-b76d-e59e9a6e0f9b" title="Permalink to this equation">#</a></span>\[\begin{equation}
\left. {\partial f \over \partial x_i} \right|_{{\bf x}} \approx  {f(x_1, \dots, x_i + \delta, \dots, x_D) - f(x_1, \dots, x_i, \dots, x_D) \over \delta}
\end{equation}\]</div>
<p>and then compare this approximation with results obtained by the closed form expression for the partial derivatives <span class="math notranslate nohighlight">\( \lbrace  {\partial f \over \partial x_i} \rbrace \)</span>. Note though that the approximations are computed around a given <span class="math notranslate nohighlight">\( {\bf x} \)</span>, i.e. the aproximation to <span class="math notranslate nohighlight">\( \nabla_{{\bf x}} f \)</span> is valid for a particular input vector <span class="math notranslate nohighlight">\( {\bf x} \)</span>. For <span class="math notranslate nohighlight">\(\delta \rightarrow 0\)</span>, the approximation becomes (by definition) the partial derivative. Besides being usefull for double checking analytical derivations using some data samples, this approximation is too expensive to use it in the backprop procedure itself. Thus, activation functions with closed form expressions for their derivatives are still required by the network to efficiently learn.</p>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "None/None",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="neuralnets_mlps.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Multi-Layer Perceptrons</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="dim_reduction.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Dimensionality Reduction Techniques</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Sibylle Hess and Stiven Dias<br/>
  
      &copy; Copyright 2022. Eindhoven University of Technology.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>