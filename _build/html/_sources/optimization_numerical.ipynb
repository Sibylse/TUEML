{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f69b60a5",
   "metadata": {},
   "source": [
    "## Numerical Optimization\n",
    "We have seen some strategies to find a solution to optimization objectives by means of solving a system of equations. These strategies work usually fine for simple objectives, but if I have a lot of valleys and hills in my optimization objective, then solving this system of equations analytically will not always be possible. What can we do then?       \n",
    "\n",
    "If the minimizers can not be computed directly/analytically, then numerical optimization can come to the rescue. The idea is that I start somewhere in my hilly landscape and then try to walk to a valley with a specified strategy. For those types of methods it's good to know how good the strategy is. Important is for example to ask whether I will ever arrive at my minimum if I just walk long enough, or if I have to wonder endlessly around in a bad case. And what happens if I just walk a few steps, would I then have improved upon my starting position or might I not have descended at all? We state here two very popular numerical optimization methods: coordinate descent and gradient descent. Both are presented for the optimization of an unconstrained objective, but they do have extensions to incorporate constraints as well. This is beyond of the scope of this course, though.  \n",
    "The general scheme of numerical optimization methods is typically \n",
    "\n",
    "```{prf:algorithm} Numerical Optimization\n",
    "\n",
    "**Input**: the function $f$ to minimize, the maximum number of steps $t_{max}$ \n",
    "1. $\\vvec{x}_0\\gets$ `Initialize`($\\vvec{x}_0$)  \n",
    "2. **for** $t\\in\\{1,\\ldots,t_{max}-1\\}$\n",
    "    1. $\\vvec{x}_{t+1}\\gets $`Update`($\\vvec{x}_t,f$)\n",
    "3. **return** $\\vvec{x}_{t_{max}}$\n",
    "```\n",
    "\n",
    "### Coordinate descent\n",
    "The coordinate descent method is promising if we can not determine the minimum to the function analytically, but the minimum in a coordinate direction. The update is performed by cycling over all coordinates and walking to the minimum subject to that coordinate in each step.\n",
    "\n",
    "```{prf:algorithm} Coordinate Descent\n",
    "**Input**: the function $f$ to minimize \n",
    "1. $\\vvec{x}_0\\gets$ `Initialize`($\\vvec{x}_0$)  \n",
    "2. **for** $t\\in\\{1,\\ldots,t_{max}-1\\}$\n",
    "    1. **for** $i\\in\\{1,\\ldots d\\}$ do\n",
    "        1. $\\displaystyle {x_i}^{(t+1)}\\leftarrow \\argmin_{x_i} f({x_1}^{(t+1)},\\ldots ,{x_{i-1}}^{(t+1)}, x_i,{x_{i+1}}^{(t)},\\ldots ,{x_d}^{(t)})$\n",
    "3. **return** $\\vvec{x}_{t_{max}}$\n",
    "```\n",
    "The figure below shows the level set of a function, where every ring indicates the points in the domain of the function that have a specified function value $\\{\\vvec{x}\\mid f(\\vvec{x})=c\\}$. The plotted function has a minimum at the center of the ellipses. We start at $\\vvec{x}_0$ and then move to the minimum in direction of the vertical coordinate. That is, we move to the smallest ellipse (smallest diameter) we can touch in direction of the vertical coordinate. Then we move to the minimum in direction of the horizontal coordinate and we are already at our minimum where the method stops.\n",
    "```{tikz}\n",
    "\\begin{tikzpicture}[samples=200,smooth]\n",
    "        \\begin{scope}\n",
    "            \\clip(-4,-1) rectangle (4,4);\n",
    "            \\draw plot[domain=0:360] ({cos(\\x)*sqrt(20/(sin(2*\\x)+2))},{sin(\\x)*sqrt(20/(sin(2*\\x)+2))});\n",
    "            \\draw plot[domain=0:360] ({cos(\\x)*sqrt(16/(sin(2*\\x)+2))},{sin(\\x)*sqrt(16/(sin(2*\\x)+2))});\n",
    "            \\draw plot[domain=0:360] ({cos(\\x)*sqrt(12/(sin(2*\\x)+2))},{sin(\\x)*sqrt(12/(sin(2*\\x)+2))});\n",
    "            \\draw plot[domain=0:360] ({cos(\\x)*sqrt(8/(sin(2*\\x)+2))},{sin(\\x)*sqrt(8/(sin(2*\\x)+2))});\n",
    "            \\draw plot[domain=0:360] ({cos(\\x)*sqrt(4/(sin(2*\\x)+2))},{sin(\\x)*sqrt(4/(sin(2*\\x)+2))});\n",
    "            \\draw plot[domain=0:360] ({cos(\\x)*sqrt(1/(sin(2*\\x)+2))},{sin(\\x)*sqrt(1/(sin(2*\\x)+2))});\n",
    "            \\draw plot[domain=0:360] ({cos(\\x)*sqrt(0.0625/(sin(2*\\x)+2))},{sin(\\x)*sqrt(0.0625/(sin(2*\\x)+2))});\n",
    "\n",
    "            \\draw[->,blue,ultra thick] (-2,3.65) to (-2,0);\n",
    "            \\draw[->,blue,ultra thick] (-2,0) to (0,0);\n",
    "            \n",
    "            \\node at (-2.9,3.5){ $(x_1^{(0)},x_2^{(0)})$};\n",
    "            \\node at (-2.9,0.4){ $(x_1^{(1)},x_2^{(0)})$};\n",
    "            \\node at (0,0.5){$(x^{(1)}_1,x^{(1)}_1)$};\n",
    "        \\end{scope}\n",
    "    \\end{tikzpicture}\n",
    "```\n",
    "Coordinate descent minimizes the function value in every step:\n",
    "$$ f(\\vvec{x}^{(0)})\\geq f(\\vvec{x}^{(1)})\\geq f(\\vvec{x}^{(2)})\\geq\\ldots$$\n",
    "\n",
    "````{prf:example} Coordinate descent of the Rosenbrock function\n",
    "```{figure} /images/optimization/coordinateDescentRosenbrock.png\n",
    "---\n",
    "height: 200px\n",
    "name: coord_desc_rosen\n",
    "align: right\n",
    "---\n",
    "Coordinate descent updates\n",
    "``` \n",
    "Let's try to apply coordinate descent to find the minimum of the Rosenbrock function. From {prf:ref}`expl_fonc` we know the partial derivatives of the function \n",
    "\\begin{align*}\n",
    "    f(\\vvec{x})&= 100(x_2-x_1^2)^2 +(1-x_1)^2.\\\\\n",
    "    \\frac{\\partial}{\\partial x_1}f(\\vvec{x})&= 400x_1(x_1^2-x_2) +2(x_1-1)\\\\\n",
    "    \\frac{\\partial}{\\partial x_2}f(\\vvec{x})&= 200(x_2-x_1^2).\n",
    "\\end{align*}\n",
    "We compute the minima of the function in direction of the coordinates by means of FONC. The derivatives subject to each coordinate are exactly given by the partial derivatives, hence we set them equal to zero:\n",
    "\\begin{align*}\n",
    "      \\frac{\\partial}{\\partial x_1}f\\begin{pmatrix}x_1\\\\x_1^2\\end{pmatrix}&= 2(x_1-1) =0 \n",
    "      &\\Leftrightarrow x_1=1\\\\\n",
    "      \\frac{\\partial}{\\partial x_2}f(\\vvec{x})&=200(x_2-x_1^2)=0\n",
    "      &\\Leftrightarrow x_2 =x_1^2\n",
    "\\end{align*}\n",
    "We have here only one minimizer candidate for each coordinate, and since we know from {prf:ref}`expl_fonc` that the function has only one minimizer (and no other maximizer or such), we know that the coordinate-wise minimizer candidates actually minimize the function in each coordinate. From this, we derive our update rules: \n",
    "\\begin{align*}\n",
    "    \\argmin_{x_1\\in\\mathbb{R}} f(x_1,x_2) =1 \\\\\n",
    "    \\argmin_{x_2\\in\\mathbb{R}} f(x_1,x_2) =x_1^2\n",
    "\\end{align*}\n",
    "{numref}`coord_desc_rosen` shows the result of these update rules when starting at $(-2,2)$. We see that the minimum is quickly reached after one cycle of updating each coordinate. \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767bec40",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "If we can't solve the system of equations given by FONC, also no coordinate-wise, but the function is differentiable, then we can apply gradient descent. Gradient descent is a strategy according to which you take a step in the direction which goes down the most steeply from where you stand.\n",
    "\n",
    "```{prf:algorithm} Gradient Descent\n",
    "**Input**: the function $f$ to minimize, step-size $\\eta$ \n",
    "1. $\\vvec{x}_0\\gets$ `Initialize`($\\vvec{x}_0$)  \n",
    "2. **for** $t\\in\\{1,\\ldots,t_{max}-1\\}$\n",
    "    1. $\\vvec{x}_{t+1}\\leftarrow \\vvec{x}_t - \\eta \\nabla f(\\vvec{x}_t)$\n",
    "3. **return** $\\vvec{x}_{t_{max}}$\n",
    "```\n",
    "\n",
    "The parameter $\\eta$ doesn't have to be a constant, it might also be a function that returns the *step size* depending on the amount of steps that have already performed. Setting the step size well is often a difficult task. The figure below shows how gradient descent makes the updates based on the local information.\n",
    "\n",
    "```{tikz}\n",
    "\\begin{tikzpicture}[samples=200,smooth]\n",
    "\\begin{scope}\n",
    "    \\clip(-4,-1) rectangle (4,4);\n",
    "    \\draw plot[domain=0:360] ({cos(\\x)*sqrt(20/(sin(2*\\x)+2))},{sin(\\x)*sqrt(20/(sin(2*\\x)+2))});\n",
    "    \\draw plot[domain=0:360] ({cos(\\x)*sqrt(16/(sin(2*\\x)+2))},{sin(\\x)*sqrt(16/(sin(2*\\x)+2))});\n",
    "    \\draw plot[domain=0:360] ({cos(\\x)*sqrt(12/(sin(2*\\x)+2))},{sin(\\x)*sqrt(12/(sin(2*\\x)+2))});\n",
    "    \\draw plot[domain=0:360] ({cos(\\x)*sqrt(8/(sin(2*\\x)+2))},{sin(\\x)*sqrt(8/(sin(2*\\x)+2))});\n",
    "    \\draw plot[domain=0:360] ({cos(\\x)*sqrt(4/(sin(2*\\x)+2))},{sin(\\x)*sqrt(4/(sin(2*\\x)+2))});\n",
    "    \\draw plot[domain=0:360] ({cos(\\x)*sqrt(1/(sin(2*\\x)+2))},{sin(\\x)*sqrt(1/(sin(2*\\x)+2))});\n",
    "    \\draw plot[domain=0:360] ({cos(\\x)*sqrt(0.0625/(sin(2*\\x)+2))},{sin(\\x)*sqrt(0.0625/(sin(2*\\x)+2))});\n",
    "\n",
    "    \\draw[->,blue,ultra thick] (-2,3.65) to (-1.93,3);\n",
    "    \\draw[->,blue,ultra thick] (-1.93,3) to (-1.75,2.4);\n",
    "    \\draw[->,blue,ultra thick] (-1.75,2.4) to (-1.5,1.8);\n",
    "    \\draw[->,blue,ultra thick] (-1.5,1.8) to (-1.15,1.3);      \\node at (-1.4,3.8){ $\\mathbf{x}_0$};\n",
    "    \\node at (-1.2,3.2){$\\mathbf{x}_1$};\n",
    "    \\node at (-1.05,2.6){ $\\mathbf{x}_2$};\n",
    "    \\node at (-0.8,2){ $\\mathbf{x}_3$};\n",
    "    \\node at (-0.6,1.4){ $\\mathbf{x}_4$};\n",
    "\\end{scope}\n",
    "\\end{tikzpicture}\n",
    "```\n",
    "\n",
    "The negative gradient points into the direction of steepest descent. Hence, for a small enough step size we will go down each step:\n",
    "$$f(\\vvec{x}_0)\\geq f(\\vvec{x}_1)\\geq f(\\vvec{x}_2)\\geq\\ldots$$\n",
    "However, decreasing the function value in every step is in practice not neccessarily desirable. In particular in the beginning of the optimization, it's useful to take larger steps to survey the landscape before converging to a local minimum.\n",
    "````{prf:example} Gradient Descent on the Rosenbrock Function\n",
    "We illustrate the effect of the step-size by means of the Rosenbrock function. {numref}`grad_desc_smallstep` shows the trajectory when using a small step-size, {numref}`grad_desc_goodstep` shows a moderate step size and {numref}`grad_desc_bigstep` shows a larger step-size. We observe that all three step-sizes result in a sequence that converges to the minimim $(1,1)$. However, in particular the very small step-size results in a trajectory that requires many many steps. The fastest convergence has the larger step-size, but even this one needs approximately 800 iterations. In comparison to coordinate descent, that just required to update each coordinate once, gradient descent is much more inefficient. We also observe that the trajectory for the larger step-size is already zig-zagging, which often indicates that the step-size is actually too large, since it overshoots in each step. Hence, this function is not very efficiently optimizable with gradient descent. But feel free to try it yourself. \n",
    "```{figure} /images/optimization/gradientDescentRosenbrockSmall.png\n",
    "---\n",
    "height: 220px\n",
    "name: grad_desc_smallstep\n",
    "---\n",
    "Gradient Descent with $\\eta=0.0005$ on the Rosenbrock Function\n",
    "``` \n",
    "```{figure} /images/optimization/gradientDescentRosenbrock.png\n",
    "---\n",
    "height: 220px\n",
    "name: grad_desc_goodstep\n",
    "---\n",
    "Gradient Descent with $\\eta=0.00125$ on the Rosenbrock Function\n",
    "``` \n",
    "```{figure} /images/optimization/gradientDescentRosenbrockLarge.png\n",
    "---\n",
    "height: 220px\n",
    "name: grad_desc_bigstep\n",
    "---\n",
    "Gradient Descent with $\\eta=0.0016$ on the Rosenbrock Function\n",
    "``` \n",
    "\n",
    "\n",
    "````"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
