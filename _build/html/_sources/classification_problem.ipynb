{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fff969e6-e092-4549-a97c-e58f91aba87f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classification Problem\n",
    "This section describes the general classification problem and introduces then the Bayes optimal classifier.\n",
    "\n",
    "### Feature space\n",
    "\n",
    "Let $ {\\bf x} $ denote a feature vector residing in a multidimensional space $ {\\cal X} $ comprising several  features. We describe the data items -- i.e. the objects we are trying to classify -- by feature vectors in $ {\\cal X} $. Note that the vector $ {\\bf x} \\in {\\cal X} $ describing a given data item may comprise either continuous (e.g. length, height, weight) or discrete (e.g. color code) features.\n",
    "\n",
    "```{figure} /images/classification/feature_space.svg\n",
    "---\n",
    "height: 320px\n",
    "name: feature_space_fig\n",
    "align: left\n",
    "---\n",
    "A data item in the feature vector space. Feature values are represented by different colors.\n",
    "```\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "We denote vectors by boldfaced letters e.g. $ {\\bf a} $ and scalars by regular letters e.g. $ a $.\n",
    "```\n",
    "````\n",
    "\n",
    "### Class labels\n",
    "\n",
    "Moreover, let $ y $ be a discrete variable indicating the class label (target) among the set of possible labels $ {\\cal Y} $, where $ {\\cal Y} \\triangleq \\lbrace \\ell_{1}, \\ell_{2}, \\ldots, \\ell_{L} \\rbrace $ is a finite set -- a.k.a. alphabet -- of $ L $ arbitrary labels. The variable $ y \\in {\\cal Y} $ indicates thus a class value (label) assigned to a data item. A label $ \\ell $ can be either the class name or a unique identification number / code representing the class. However, for the sake of simplicity, the class labels are usually normalized as indexes spanning e.g. from $ 0 $ to $ L-1 $. \n",
    "\n",
    "```{figure} /images/classification/class_labels.svg\n",
    "---\n",
    "height: 320px\n",
    "name: class_labels_fig\n",
    "align: left\n",
    "---\n",
    "An alphabet of arbitrary labels. Class values (labels) are represented by different colors.\n",
    "```\n",
    "\n",
    "```{prf:remark}\n",
    "We should pick up the set of features according to the particular classification problem we are trying to solve and also make sure that it really encloses sufficient information to help distinguishing data items / objects from different classes. Note therefore that the selection of classes and the selection of features are often entangled.\n",
    "```\n",
    "\n",
    "### Training examples\n",
    "\n",
    "Now let\n",
    "\\begin{eqnarray}\n",
    "{\\cal D} &\\triangleq& \\bigcup_{i=1}^{N} \\lbrace \\left( {\\bf x}_{i}, y_{i} \\right) \\rbrace \\\\\n",
    "&\\equiv& \\lbrace \\left( {\\bf x}_{1}, y_{1} \\right), \\left( {\\bf x}_{2}, y_{2} \\right), \\ldots, \\left( {\\bf x}_{N}, y_{N} \\right) \\rbrace \\nonumber\n",
    "\\end{eqnarray}\n",
    "denote a dataset of $ N $ training examples, i.e. the data items for which we already know the true classification. Specifically, the $ i $-th _known_ training example is represented by a pair $ {\\cal D}_{i} \\triangleq \\left( {\\bf x}_{i}, y_{i} \\right) $ including both the feature vector $ {\\bf x}_{i} $ and the associated class label $ y_{i} $. Note that $ {\\cal D} \\subset {\\cal X} \\times {\\cal Y} $, where $ \\times $ denotes the Cartesian product. Note also that the dataset $ {\\cal D} $ is often much smaller than its complement $ \\bar{\\cal D} = ({\\cal X} \\times {\\cal Y}) \\setminus {\\cal D} $ w.r.t. $ {\\cal X} \\times {\\cal Y} $. The set difference operator is defined as $ {\\cal A} \\setminus {\\cal B} \\triangleq \\lbrace a \\mid a \\in {\\cal A} \\wedge a \\notin {\\cal B} \\rbrace $.\n",
    "\n",
    "```{figure} /images/classification/training_dataset.svg\n",
    "---\n",
    "height: 320px\n",
    "name: class_labels_fig\n",
    "align: left\n",
    "---\n",
    "The training dataset. The darkest shading area is often much smaller than the lightest area.\n",
    "```\n",
    "\n",
    "```{prf:remark}\n",
    "As we increase the number of features (more dimensions) or the granularity of the existing features (e.g. more values per discrete feature), the classifier might increase its ability to distinguish training examples from different classes. However, besides possibly increasing the computational burden to deal with all input features, the required number of training examples might significantly increase to prevent over fitting.\n",
    "```\n",
    "\n",
    "### Independent and identically distributed (i.i.d.)\n",
    "\n",
    "````{prf:property} i.i.d. assumption\n",
    "The overall assumption is that training examples $ \\cup_{i=1}^{N} \\lbrace {\\cal D}_{i} \\rbrace \\equiv \\cup_{i=1}^{N} \\lbrace \\left( {\\bf x}_{i}, y_{i} \\right) \\rbrace $ from training dataset $ {\\cal D} $ are _identically_ distributed and were _independently_ drawn from an _unknown_ probability distribution $ p^{\\ast}({\\bf x}, y) $, i.e. \n",
    "```{math}\n",
    ":label: iid_assumption\n",
    "{\\bf x}_{i}, y_{i} \\sim  p^{\\ast}({\\bf x}, y), \\forall i \\in \\lbrace 1, \\ldots, N \\rbrace.\n",
    "```\n",
    "````\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "We also assume that testing and validation samples follow the i.i.d. assumption. Nevertheless, this assumption is quite strong, i.e. it might be easily broken by real world data.\n",
    "```\n",
    "\n",
    "```{note}\n",
    "We denote random vectors by capitalized letters $ {\\bf X} $ and samples of random variables / vectors by non-capitalized letters $ {\\bf x} $. The distinction between $ {\\bf X} \\sim p({\\bf x}) \\equiv p_{\\bf X}({\\bf x}) $ and $ {\\bf x} \\sim p({\\bf x}) \\equiv p_{\\bf X}({\\bf x}) $ is implied by the context.\n",
    "```\n",
    "````\n",
    "\n",
    "`````{toggle}\n",
    "````{prf:proof}\n",
    "Let us assume that, $ \\forall i \\in \\lbrace 1, \\ldots, N \\rbrace $, $ {\\bf x}_{i} \\in {\\cal X} $ and $ y_{i} \\in {\\cal Y} $ are realizations of a random vector $ {\\bf X}_{i} $ and a random variable $ Y_{i} $, respectively. Let us further assume that $ {\\bf X}_{i} $ and $ Y_{i} $ are jointly distributed according to a _mixed_ probability distribution $ p_{{\\bf X}_{i}, Y_{i}}({\\bf x}_{i}, y_{i}) $ or, more concisely, $ p({\\bf x}_{i}, y_{i}) $. Conversely, we can write $ {\\bf x}_{i}, y_{i} \\sim p({\\bf x}_{i}, y_{i}) $ which means that the samples $ {\\bf x}_{i}, y_{i} $ are drawn from the joint distribution $ p({\\bf x}_{i}, y_{i}) $. Note that we can also write $ {\\bf X}_{i}, Y_{i} \\sim p({\\bf x}_{i}, y_{i}) $ meaning in this case that the random variables / vectors $ {\\bf X}_{i}, Y_{i} $ are jointly distributed according to $ p({\\bf x}_{i}, y_{i}) $.\n",
    "\n",
    "We assume that the training samples $ \\cup_{i=1}^{N} \\lbrace {\\cal D}_{i} \\rbrace $ are _mutually independent_ such that the following holds\n",
    "```{math}\n",
    ":label: mutually_independent_assumption\n",
    "p({\\bf x}_{1}, y_{1}, {\\bf x}_{2}, y_{2}, \\ldots, {\\bf x}_{N}, y_{N}) \\equiv p({\\bf x}_{1}, y_{1}) \\, p({\\bf x}_{2}, y_{2}) \\ldots p({\\bf x}_{N}, y_{N}).\n",
    "```\n",
    "\n",
    "Additionally, we assume that the training samples $ \\cup_{i=1}^{N} \\lbrace {\\cal D}_{i} \\rbrace $ are _identically_ distributed according to an _unknown_ probability distribution $ p^{\\ast}({\\bf x}, y) $ such that\n",
    "```{math}\n",
    ":label: identically_distributed_assumption\n",
    "p({\\bf x}_{i}, y_{i}) \\equiv p^{\\ast}({\\bf x}, y), \\forall i \\in \\lbrace 1, \\ldots, N \\rbrace.\n",
    "```\n",
    "\n",
    "In summary, the overall assumption in {eq}`iid_assumption` follows from {eq}`mutually_independent_assumption` and {eq}`identically_distributed_assumption`. $ \\blacksquare $\n",
    "````\n",
    "`````\n",
    "\n",
    "### Classifier definition\n",
    "\n",
    "```{prf:definition} Classifier\n",
    ":label: classifier\n",
    "A classifier is a function $ h: {\\cal X} \\rightarrow {\\cal Y} $ mapping feature vectors $ {\\bf x} $ to labels $ y $. In particular, for a given _observed_ sample $ \\check{\\bf x} $, the classifier returns a _predicted_ label $ \\hat{y} = h(\\check{\\bf x}) $. We use $ \\check{\\bf x} $ to denote an observed -- or, equivalently, measured -- realization of the random vector $ {\\bf X} $ which was possibly not seen before to distinguish it from the data items within the training dataset. On the other hand, $ \\hat{y} $ denotes an estimation of the random variable $ Y $, in this case, the label predicted by the classifier.\n",
    "```\n",
    "\n",
    "```{figure} /images/classification/classification_task.svg\n",
    "---\n",
    "height: 320px\n",
    "name: classification_task_fig\n",
    "align: left\n",
    "---\n",
    "A classifier struggling to generalize. Typically the classifier maps multiple data items to the same class value (label), i.e. $ \\exists {\\bf x}, {\\bf x}' \\in {\\cal X} \\mid {\\bf x} \\neq {\\bf x}' \\wedge h({\\bf x}) = h({\\bf x}') $.\n",
    "```\n",
    "\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "Note that in general $ \\check{\\bf x} $ has not been seen before, i.e. it is not in the training dataset -- $ \\left( \\check{\\bf x}, \\hat{y} \\right) \\notin {\\cal D} $. The classifier's ability to properly classify unseen samples is called _generalization_.\n",
    "```\n",
    "````\n",
    "\n",
    "### True classification error\n",
    "\n",
    "````{prf:definition} True classification error\n",
    ":label: true_classifier_error\n",
    "Let the function $ h(\\cdot) $ be a classifier. Furthermore, let $ Pr \\lbrace A \\rbrace $ and $ {E}_{{\\bf Z} \\sim p} \\lbrace g({\\bf Z}) \\rbrace $ denote respectively the probability of an event $ A $ occurring and the expected value of a function $ g(\\cdot) $ of a random vector -- or a variable, i.e. one-dimensional vector -- $ {\\bf Z} $ distributed according to a probability distribution $ p({\\bf z}) \\equiv p_{\\bf Z}({\\bf z}) $, i.e. $ {\\bf Z} \\sim p({\\bf z}) $. We can then write the true classification error of $ h $ as\n",
    "```{math}\n",
    ":label: true_classification_error\n",
    "Pr \\lbrace Y \\neq h({\\bf X}) \\rbrace \\triangleq {E}_{{\\bf X},Y \\sim p^{\\ast}} \\lbrace \\left[ Y \\neq h(\\bf X) \\right] \\rbrace,\n",
    "```\n",
    "where the Iverson bracket -- which takes as an argument any logical proposition $ \\Psi $ -- is defined as\n",
    "```{math}\n",
    "\\left[ \\Psi \\right] = \\left\\lbrace\n",
    "\\begin{matrix}\n",
    "1, & \\mbox{if } \\Psi \\mbox{ is true} \\\\\n",
    "0, & \\mbox{otherwise.}\n",
    "\\end{matrix}\n",
    "\\right. \\nonumber\n",
    "```\n",
    "The difference to regular brackets is implied by the enclosed expression. For the Iverson bracket, must be a logical proposition that evaluates to true or false.\n",
    "````\n",
    "\n",
    "%Without loss of generality, let us assume that the multidimensional feature space $ {\\cal X} $ includes only continuous-valued features. Therefore, the expected value of the function $ \\left[ Y \\neq h(\\bf X) \\right] $ of $ {\\bf X},Y $ w.r.t. the _mixed_ probability distribution $ p^{\\ast} $ can be written as\n",
    "%\\begin{equation} \\label{eq:expected_value}\n",
    "%{E}_{{\\bf X},Y \\sim p^{\\ast}} \\lbrace \\left[ Y \\neq h(\\bf X) \\right] \\rbrace = \\sum_{y \\in {\\cal Y}} \\int_{{\\bf x} \\in {\\cal X}} \\left[ y \\neq h({\\bf x}) \\right] p^{\\ast} ({\\bf x}, y) d {\\bf x}.\n",
    "%\\end{equation}\n",
    "%Note, however, that the underlying distribution $ p^{\\ast} $ is _unknown_. Thus, in general, the true classification error can not be computed exactly as in~\\eqref{eq:true_classification_error}.\n",
    "%\n",
    "%In practice, we approximate the true classification error by computing a Monte Carlo approximation of the integrals/summations on the right-hand side of~\\eqref{eq:expected_value}. Specifically, let\n",
    "%\\begin{eqnarray}\n",
    "%{\\cal D}' &\\triangleq& \\bigcup_{i=1}^{M} \\lbrace \\left( \\bar{\\bf x}_{i}, \\bar{y}_{i} \\right) \\rbrace \\nonumber \\\\\n",
    "%&\\equiv& \\lbrace \\left( \\bar{\\bf x}_{1}, \\bar{y}_{1} \\right), \\left( \\bar{\\bf x}_{2}, \\bar{y}_{2} \\right), \\ldots, \\left( \\bar{\\bf x}_{M}, \\bar{y}_{M} \\right) \\rbrace \\nonumber\n",
    "%\\end{eqnarray}\n",
    "%be a dataset containing $ M $ _testing_ or _validation_ examples also drawn from the underlying distribution $ p^{\\ast} $. Note that we know the true label $ \\bar{y}_{i} $ of each data item $ \\bar{\\bf x}_{i} $. From the law of large numbers, the integrals/summations on the right-hand side of~\\eqref{eq:expected_value} can be computed as\n",
    "%\\begin{equation} \\label{eq:expected_value_approximation}\n",
    "%\\sum_{y \\in {\\cal Y}} \\int_{{\\bf x} \\in {\\cal X}} \\left[ y \\neq h(\\bf x) \\right] p^{\\ast} ({\\bf x}, y) d {\\bf x} = \\lim_{M \\rightarrow \\infty} \\frac{1}{M} \\sum_{i=1}^{M} \\left[ \\bar{y}_{i} \\neq h(\\bar{\\bf x}_{i}) \\right]\n",
    "%\\end{equation}\n",
    "%with $ \\bar{\\bf x}_{i}, \\bar{y}_{i} \\sim p^{\\ast} ({\\bf x}, y) $, $ \\forall M \\in \\lbrace 1, 2, 3, \\ldots \\rbrace $, since by definition $$ \\sum_{y \\in {\\cal Y}} \\int_{{\\bf x} \\in {\\cal X}} p^{\\ast} ({\\bf x}, y) d {\\bf x} = 1. $$\n",
    "%\n",
    "%Thus, for a finite number of samples $ M $, the true classification error is typically approximated in a Monte Carlo sense using a very large _testing_ or _validation_ dataset $ {\\cal D}' $ as\n",
    "%\\begin{equation}\n",
    "%Pr \\lbrace Y \\neq h({\\bf X}) \\rbrace \\approx \\frac{1}{M} \\sum_{i=1}^{M} \\left[ \\bar{y}_{i} \\neq \\hat{y}_{i} \\right]\n",
    "%\\end{equation}\n",
    "%with $ \\hat{y}_{i} = h(\\bar{\\bf x}_{i}) $.\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "The probability of miss-classification $ Pr \\lbrace Y \\neq h({\\bf X}) \\rbrace $ is also known as the risk $ R(h) $ of the classifier $ h $.\n",
    "```\n",
    "````\n",
    "\n",
    "### Bayes optimal classifier\n",
    "\n",
    "````{prf:definition} Bayes optimal classifier\n",
    ":label: bayes_optimal_classifier\n",
    "The classifier\n",
    "```{math}\n",
    ":label: final_estimator\n",
    "h^{\\ast}({\\bf x}) = \\argmax_{y \\in {\\cal Y}} p^{\\ast} ({\\bf x}, y)\n",
    "```\n",
    "is called Bayes optimal as it minimizes the classification risk, i.e. the true classification error. More precisely,\n",
    "\\begin{eqnarray}\n",
    "h^{\\ast}({\\bf x}) &=& \\argmin_{h} R(h) \\nonumber \\\\\n",
    " &=& \\argmin_{h} Pr \\lbrace Y \\neq h({\\bf X}) \\rbrace. \\nonumber\n",
    "\\end{eqnarray}\n",
    "However, Eq. {eq}`final_estimator` can not be used for obtaining the Bayes optimal classifier $ h^{\\ast} $ since, again, the joint distribution $ p^{\\ast} $ is unknown. \n",
    "````\n",
    "\n",
    "`````{toggle}\n",
    "````{prf:proof}\n",
    "From {eq}`true_classification_error`, we can write\n",
    "```{math}\n",
    ":label: bayes_optimal_classifier\n",
    "h^{\\ast}({\\bf x}) = \\argmin_{h} {E}_{{\\bf X},Y \\sim p^{\\ast}} \\lbrace \\left[ Y \\neq h(\\bf X) \\right] \\rbrace\n",
    "```\n",
    "where the argument $ h $ resides on the space of arbitrary functions of the type $ {\\cal X} \\rightarrow {\\cal Y} $.\n",
    "\n",
    "We offer without proof that the solution to {eq}`bayes_optimal_classifier` is given by the maximum likelihood (ML) estimator \n",
    "```{math}\n",
    ":label: ml_estimator\n",
    "h^{\\ast}({\\bf x}) = \\argmax_{y \\in {\\cal Y}} p^{\\ast} ( y \\mid {\\bf x}),\n",
    "```\n",
    "where $ p^{\\ast} ( y \\mid {\\bf x}) $ is the true likelihood function. In this case, it indicates how likely a particular label $ y $ represents the true class value given that a feature vector $ {\\bf x} $ was observed.\n",
    "\n",
    "Note that we can multiply the right-hand side of {eq}`ml_estimator` by the true marginal distribution $ p^{\\ast}({\\bf x}) $ without affecting the result\n",
    "\\begin{eqnarray}\n",
    "h^{\\ast}({\\bf x}) &=& \\argmax_{y \\in {\\cal Y}} p^{\\ast} ( y \\mid {\\bf x}) \\, p^{\\ast}({\\bf x}) \\nonumber \\\\\n",
    "&=& \\argmax_{y \\in {\\cal Y}} p^{\\ast} ({\\bf x}, y) \\nonumber\n",
    "\\end{eqnarray}\n",
    "and finally write Eq. {eq}`final_estimator`. $ \\blacksquare $\n",
    "````\n",
    "`````\n",
    "\n",
    "````{margin}\n",
    "```{note}\n",
    "As $ p^{\\ast} $ is typically unknown in must practical applications, suitable learning algorithms must rely on different approaches aiming to minimize computable approximations to the true classification error and are therefore sub-optimal in a Bayesian sense.\n",
    "```\n",
    "````\n",
    "\n",
    "```{prf:definition} Excess risk\n",
    "The excess risk of a given classifier $ h $ is given by $ R(h) - R(h^{\\ast}) $, where $ R(h^{\\ast}) $ is the risk of the Bayes optimal classifier $ h^{\\ast} $. The excess risk of a consistent classifier $ h $ converges to zero as the number of training examples $ N $ grows unbounded, i.e. $ N \\rightarrow \\infty $.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e21e6a1-98f0-457c-877f-e39d1ebecc55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
