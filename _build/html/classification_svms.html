
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Support Vector Machines &#8212; Data Mining and Machine Learning Jupyter Book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"diag": "\\mathrm{diag}", "tr": "\\mathrm{tr}", "argmin": "\\mathrm{arg\\,min}", "argmax": "\\mathrm{arg\\,max}", "minimize": "\\mathrm{minimize}", "maximize": "\\mathrm{maximize}", "sgn": "\\mathrm{sgn}", "softmax": "\\mathrm{softmax}", "vvec": ["\\mathbf{#1}", 1], "bm": ["{\\boldsymbol #1}", 1], "concat": "\\mathbin{{+}\\mspace{-8mu}{+}}"}, "preamble": "\\usepackage{arydshln}"}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Neural Networks" href="neuralnets.html" />
    <link rel="prev" title="Random Forests" href="classification_random_forests.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Mining and Machine Learning Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to the Data Mining and Machine Learning Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="notation.html">
   Notation
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="linalg.html">
   Linear Algebra
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_spaces.html">
     Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_normed_vs.html">
     Normed Vector Spaces
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="linalg_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="optimization.html">
   Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_problems.html">
     Optimization Problems
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_numerical.html">
     Numerical Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_convex.html">
     Convex Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_gradients.html">
     Matrix Derivatives
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="optimization_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="regression.html">
   Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_objective.html">
     Regression Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_functions.html">
     Regression Functions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_optimization.html">
     Minimizing the RSS
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_bias_var.html">
     The Bias-Variance Tradeoff
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_sparse.html">
     The Sparse Regression Task
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_ridge.html">
     Ridge Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_lasso.html">
     Lasso
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_L1vsL2.html">
     L1 vs L2 Regularization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="regression_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="classification.html">
   Classification
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="classification_problem.html">
     Classification Objective
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_evaluation.html">
     Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_knn.html">
     K-Nearest Neighbor
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_naive_bayes.html">
     Naive Bayes
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_decision_trees.html">
     Decision Trees
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="classification_random_forests.html">
     Random Forests
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Support Vector Machines
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="neuralnets.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_intro.html">
     Neural Networks Intro
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_comp_graphs.html">
     Computational graphs
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_func_approx.html">
     Function approximator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_mlps.html">
     Multi-Layer Perceptrons
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="neuralnets_backprop.html">
     Backpropagation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="dim_reduction.html">
   Dimensionality Reduction Techniques
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_mf.html">
     Low Rank Matrix Factorization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_matrix_completion.html">
     Matrix Completion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_pca.html">
     Principal Component Analysis
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="dim_reduction_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="clustering.html">
   Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means.html">
     k-Means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_k_means_mf.html">
     k-Means is MF
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_kernel_kmeans.html">
     Kernel k-means
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_spectral.html">
     Spectral Clustering
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="clustering_exercises.html">
     Exercises
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://gitlab.tue.nl/20214358/dmml"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://gitlab.tue.nl/20214358/dmml/issues/new?title=Issue%20on%20page%20%2Fclassification_svms.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/20214358/dmml/edit/master/classification_svms.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/classification_svms.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-word-on-hyperplanes">
   A word on hyperplanes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-principle">
   Basic principle
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-with-hard-constraints">
   Learning with hard-constraints
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-with-soft-constraints">
   Learning with soft-constraints
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-word-on-duality">
   A word on duality
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dual-svm-formulation">
   Dual SVM formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-word-on-kernels">
   A word on kernels
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-trick">
   Kernel trick
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Support Vector Machines</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-word-on-hyperplanes">
   A word on hyperplanes
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#basic-principle">
   Basic principle
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-with-hard-constraints">
   Learning with hard-constraints
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#learning-with-soft-constraints">
   Learning with soft-constraints
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-word-on-duality">
   A word on duality
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dual-svm-formulation">
   Dual SVM formulation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-word-on-kernels">
   A word on kernels
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernel-trick">
   Kernel trick
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="support-vector-machines">
<h1>Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">#</a></h1>
<section id="a-word-on-hyperplanes">
<h2>A word on hyperplanes<a class="headerlink" href="#a-word-on-hyperplanes" title="Permalink to this headline">#</a></h2>
<p>Let the expression <div class="math notranslate nohighlight">
\[ {\bf w}^{T} {\bf x} - b = 0 \]</div>
 – parameterized by a coefficient vector <span class="math notranslate nohighlight">\( {\bf w} = \begin{bmatrix} w_{1} &amp; w_{2} &amp; \ldots &amp; w_{D} \end{bmatrix}^{T} \)</span> and an offset term <span class="math notranslate nohighlight">\( b \)</span> – denote a hyperplane in the space <span class="math notranslate nohighlight">\( {\cal X} \)</span>. Note that <div class="math notranslate nohighlight">
\[ {\bf w}^{T} {\bf x} = w_{1}x_{1} + w_{2}x_{2} + \ldots + w_{D}x_{D} \]</div>
 is the inner product of vectors <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( {\bf x} \)</span> such that <div class="math notranslate nohighlight">
\[ {\bf w}^{T} {\bf x} = 0 \Leftrightarrow {\bf w} \perp {\bf x}. \]</div>
 Hence, vectors <span class="math notranslate nohighlight">\( {\bf x} = \begin{bmatrix} x_{1} &amp; x_{2} &amp; \ldots &amp; x_{D} \end{bmatrix}^{T} \)</span> satisfying <span class="math notranslate nohighlight">\( {\bf w}^{T} {\bf x} - b = 0 \)</span> determine a  hyperplane orthogonal to <span class="math notranslate nohighlight">\( {\bf w} \)</span>. Reciprocally, the coefficient vector <span class="math notranslate nohighlight">\( {\bf w} \)</span> determines the orientation of the hyperplane <span class="math notranslate nohighlight">\( {\bf w}^{T} {\bf x} - b = 0 \)</span>.</p>
<p>Moreover, we assume that <span class="math notranslate nohighlight">\( {\bf w} \)</span> is oriented to the positive side of the hyperplane. In this sense, the hyperplane segments the space into a <em>negative</em> side and a <em>positive</em> side. Specifically, a vector <span class="math notranslate nohighlight">\( {\bf x} \)</span> resides in the hyperplane when it satisfies the equality <span class="math notranslate nohighlight">\( {\bf w}^{T} {\bf x} - b = 0 \)</span>. On the other hand, it falls in the <em>negative</em> or <em>positive</em> – actually, non-negative – sides when one of the following inequalities holds</p>
<div class="amsmath math notranslate nohighlight" id="equation-139c4e43-e238-4fdc-9228-825e436d68d8">
<span class="eqno">(67)<a class="headerlink" href="#equation-139c4e43-e238-4fdc-9228-825e436d68d8" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\bf w}^{T} {\bf x} - b &amp;&lt;&amp; 0\,\, \mbox{(negative side)} \nonumber \\
{\bf w}^{T} {\bf x} - b &amp;\geq&amp; 0\,\, \mbox{(positive side)}. \nonumber
\end{eqnarray}\]</div>
<figure class="align-left" id="distance-hyperplane-02-fig">
<a class="reference internal image-reference" href="_images/distance_hyperplane_02.png"><img alt="_images/distance_hyperplane_02.png" src="_images/distance_hyperplane_02.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">Hyperplane splitting a bidimensional space with <span class="math notranslate nohighlight">\( {\bf x} = \begin{bmatrix} x_{1} &amp; x_{2} \end{bmatrix}^{T} \)</span>. The <em>positive</em> and <em>negative</em> sides of the hyperplane are highlighted in light blue and light red colors, respectively. The coefficients vector <span class="math notranslate nohighlight">\( {\bf w} \)</span> is normal to the hyperplane by construction. We also assume that it is oriented towards the <em>positive</em> side of the hyperplane.</span><a class="headerlink" href="#distance-hyperplane-02-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Now, let the function</p>
<div class="amsmath math notranslate nohighlight" id="equation-dc85476d-596a-4cc8-9c7e-aa05bcf07f80">
<span class="eqno">(68)<a class="headerlink" href="#equation-dc85476d-596a-4cc8-9c7e-aa05bcf07f80" title="Permalink to this equation">#</a></span>\[\begin{equation}
d({\bf x}; {\bf w}, b) = \frac{ {\bf w}^{T} {\bf x} - b }{|| {\bf w} ||}
\end{equation}\]</div>
<p>define the <em>signed</em> distance between an input vector <span class="math notranslate nohighlight">\( {\bf x} \)</span> and the hyperplane defined by <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span>, where the operator <span class="math notranslate nohighlight">\( || \cdot || \)</span> denotes the <span class="math notranslate nohighlight">\( L_{2} \)</span> (Euclidean) norm. Note that the signal of this distance measurement indicates whether the input vector <span class="math notranslate nohighlight">\( {\bf x} \)</span> resides on the negative – <span class="math notranslate nohighlight">\( d({\bf x}; {\bf w}, b) &lt; 0 \)</span> – or positive – <span class="math notranslate nohighlight">\( d({\bf x}; {\bf w}, b) &gt; 0 \)</span> – side of the hyperplane. Lastly, the offset term <span class="math notranslate nohighlight">\( b \)</span> determines the signed distance of the hyperplane from the origin of the feature space, since <div class="math notranslate nohighlight">
\[ d({\bf 0}; {\bf w}, b) = \frac{b}{|| {\bf w} ||}. \]</div>
</p>
<figure class="align-left" id="distance-hyperplane-06-fig">
<a class="reference internal image-reference" href="_images/distance_hyperplane_06.png"><img alt="_images/distance_hyperplane_06.png" src="_images/distance_hyperplane_06.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">Hyperplane splitting a bidimensional feature space with <span class="math notranslate nohighlight">\( {\bf x} = \begin{bmatrix} x_{1} &amp; x_{2} \end{bmatrix}^{T} \)</span>. Note that the offset term <span class="math notranslate nohighlight">\( b \)</span> determines the distance of the hyperplane from the origin <span class="math notranslate nohighlight">\( {\bf 0} = \begin{bmatrix} 0 &amp; 0 \end{bmatrix}^{T} \)</span>. In this case, <span class="math notranslate nohighlight">\( b &lt; 0 \)</span> as the origin falls on the <em>negative</em> side of the hyperplane.</span><a class="headerlink" href="#distance-hyperplane-06-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
<section id="basic-principle">
<h2>Basic principle<a class="headerlink" href="#basic-principle" title="Permalink to this headline">#</a></h2>
<p>Support Vector Machines (SVMs) use hyperplanes to separate data items from multiple classes. For the sake of simplicity, let us restrict the discussion first to the binary classification problem, i.e <span class="math notranslate nohighlight">\( {\cal Y} = \lbrace \ell_{1}, \ell_{2} \rbrace \)</span>. Now, let us further assume that the training dataset <div class="math notranslate nohighlight">
\[ {\cal D} = \bigcup_{i=1}^{N} \lbrace \left( {\bf x}_{i}, y_{i} \right) \rbrace, \]</div>
 in which <span class="math notranslate nohighlight">\( {\bf x}_{i} \in {\cal X} \)</span> and <span class="math notranslate nohighlight">\( y_{i} \in {\cal Y} \)</span>, <span class="math notranslate nohighlight">\( \forall i \in \lbrace 1, 2, \ldots N \rbrace \)</span>, is linearly separable in the sense that there is at least one hyperplane <div class="math notranslate nohighlight">
\[ {\bf w}^{T} {\bf x} - b = 0 \]</div>
 in the feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span> that is able to separate the training examples such that data items in <span class="math notranslate nohighlight">\( {\cal D} \)</span> assigned to different labels <span class="math notranslate nohighlight">\( \ell_{1} \)</span> and <span class="math notranslate nohighlight">\( \ell_{2} \)</span> are in different sides of the hyperplane. In precise mathematical terms, <span class="math notranslate nohighlight">\( \exists {\bf w} \in \mathbb{R}^{D} \wedge b \in \mathbb{R} \)</span> such that the following holds <span class="math notranslate nohighlight">\( \forall i \in \lbrace 1, 2, \ldots, N \rbrace \)</span></p>
<div class="math notranslate nohighlight" id="equation-cond1">
<span class="eqno">(69)<a class="headerlink" href="#equation-cond1" title="Permalink to this equation">#</a></span>\[{\bf w}^{T} {\bf x}_{i} - b &lt; 0\,\, \mbox{ if } y_{i} = \ell_{1}\]</div>
<div class="math notranslate nohighlight" id="equation-cond2">
<span class="eqno">(70)<a class="headerlink" href="#equation-cond2" title="Permalink to this equation">#</a></span>\[{\bf w}^{T} {\bf x}_{i} - b \geq 0\,\, \mbox{ if } y_{i} = \ell_{2}.\]</div>
<p>Alternatively, let the function <div class="math notranslate nohighlight">
\[\begin{split} \sgn(a) = \left\lbrace \begin{matrix} +1 &amp; \mbox{if } a \geq 0 \\ -1 &amp; \mbox{otherwise} \end{matrix} \right. \end{split}\]</div>
 indicate the sign of a real number <span class="math notranslate nohighlight">\( a \in \mathbb{R} \)</span>. For a linearly separable binary classification problem, we can find a hyperplane defined by parameters <span class="math notranslate nohighlight">\( \tilde{\bf w} \)</span> and <span class="math notranslate nohighlight">\( \tilde{b} \)</span> that satisfies the conditions <a class="reference internal" href="#equation-cond1">(69)</a> and <a class="reference internal" href="#equation-cond2">(70)</a> and use then this hyperplane to build a classifier by plugging <span class="math notranslate nohighlight">\( \tilde{\bf w} \)</span> and <span class="math notranslate nohighlight">\( \tilde{b} \)</span> into</p>
<div class="math notranslate nohighlight" id="equation-hyperplane-classifier">
<span class="eqno">(71)<a class="headerlink" href="#equation-hyperplane-classifier" title="Permalink to this equation">#</a></span>\[h({\bf x}; {\bf w}, b) = \sgn({\bf w}^{T} {\bf x}_{i} - b)\]</div>
<p>such that labels <span class="math notranslate nohighlight">\( \ell_{1} = -1 \)</span> and <span class="math notranslate nohighlight">\( \ell_{2} = +1 \)</span>.</p>
<div class="proof remark admonition" id="remark-0">
<p class="admonition-title"><span class="caption-number">Remark 7 </span></p>
<section class="remark-content" id="proof-content">
<p>In general, there are multiple hyperplanes satisfying conditions <a class="reference internal" href="#equation-cond1">(69)</a> and <a class="reference internal" href="#equation-cond2">(70)</a>. Thus, we need to setup some optimization criteria to select the best hyperplane.</p>
<figure class="align-left" id="svm-hyperplane-1-fig">
<a class="reference internal image-reference" href="images/classification/svm_hyperplane_1.svg"><img alt="images/classification/svm_hyperplane_1.svg" height="320px" src="images/classification/svm_hyperplane_1.svg" /></a>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">A hyperplane parameterized by <span class="math notranslate nohighlight">\( \tilde{\bf w}\)</span> and <span class="math notranslate nohighlight">\( \tilde{b} \)</span> separating the training samples in <span class="math notranslate nohighlight">\( {\cal D} \)</span>. Training data items colored in <span style="color: red;">red</span> and <span style="color: blue;">blue</span> are assigned to labels <span class="math notranslate nohighlight">\( \ell_{1} = -1 \)</span> and <span class="math notranslate nohighlight">\( \ell_{2} = +1 \)</span>, respectively. The observed data item <span class="math notranslate nohighlight">\( \check{\bf x} \)</span> is assigned to a label <span class="math notranslate nohighlight">\( \hat{y} = h(\check{\bf x}; \tilde{\bf w}, \tilde{b}) \)</span> according to the side of the hyperplane – illustrated by the solid line – it resides. The proposed classifier works fine for the particular samples in <span class="math notranslate nohighlight">\( {\cal D} \)</span>, but its hyperplane seems too close to some <span style="color: red;">red</span> samples.</span><a class="headerlink" href="#svm-hyperplane-1-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</div><p>Now, let the dataset partitions <div class="math notranslate nohighlight">
\[ {\cal D}_{-1} = \lbrace \left( {\bf x}', y' \right) \in {\cal D} \mid y' = -1 \rbrace \]</div>
 and <div class="math notranslate nohighlight">
\[ {\cal D}_{+1} = \lbrace \left( {\bf x}', y' \right) \in {\cal D} \mid y' = +1 \rbrace \]</div>
 collect the training samples assigned respectively to the class labels <span class="math notranslate nohighlight">\( \ell_{1} = -1 \)</span> and <span class="math notranslate nohighlight">\( \ell_{2} = +1 \)</span>.</p>
<p>We define the margin as the distance between two parallel hyperplanes touching the dataset partitions <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span> such that none of the training samples in <span class="math notranslate nohighlight">\( {\cal D} \)</span> fall within the region between the two hyperplanes. The decision boundary correspond to a hyperplane with parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span> equidistant to these parallel hyperplanes.</p>
<p>We seek to select the decision boundary that maximizes the margin between <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span>. Equivalently, we seek to select parameters <span class="math notranslate nohighlight">\( {\bf w}^{\ast} \)</span> and <span class="math notranslate nohighlight">\( b^{\ast} \)</span> defining a linear decision boundary such that the equidistant hyperplanes touching the dataset partitions <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span> have maximum distance.</p>
<p>More precisely, let <div class="math notranslate nohighlight">
\[ m_{-1}({\bf w}, b) = \min_{\left( {\bf x}', y' \right) \in {\cal D}_{-1}} - d({\bf x}'; {\bf w}, b) \]</div>
 and <div class="math notranslate nohighlight">
\[ m_{+1}({\bf w}, b) = \min_{\left( {\bf x}', y' \right) \in {\cal D}_{+1}} d({\bf x}'; {\bf w}, b) \]</div>
 denote the <em>non-signed</em> distances between the hyperplane defined by <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span> and the closest point(s) in the dataset partitions <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span>, respectively. The parameters of the hyperplane that provides the maximum margin between the dataset partitions <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span> are given by</p>
<div class="math notranslate nohighlight" id="equation-svm-form-1">
<span class="eqno">(72)<a class="headerlink" href="#equation-svm-form-1" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\left( {\bf w}^{\ast}, b^{\ast} \right) &amp;=&amp; \argmax_{\left( {\bf w}, b \right) \in \mathbb{R}^{D+1}} \left\lbrace m_{-1}({\bf w}, b) + m_{+1}({\bf w}, b) \right\rbrace \\
s.t. &amp;&amp; m_{-1}({\bf w}, b) = m_{+1}({\bf w}, b) \\
&amp;&amp; h({\bf x}_{i}; {\bf w}, b) = y_{i}, \,\,\, \forall \left( {\bf x}_{i}, y_{i} \right) \in {\cal D}.
\end{eqnarray}\end{split}\]</div>
<p>For a linearly separable training dataset <span class="math notranslate nohighlight">\( {\cal D} \)</span>, the maximum margin <span class="math notranslate nohighlight">\( m_{-1}({\bf w}^{\ast}, b^{\ast}) + m_{+1}({\bf w}^{\ast}, b^{\ast}) \)</span> is achieved for a unique pair of parameters <span class="math notranslate nohighlight">\( {\bf w}^{\ast} \)</span> and <span class="math notranslate nohighlight">\( b^{\ast} \)</span>. Moreover, the solution is fully determined by a subset of the feature vectors in the dataset partitions <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span> which <strong>support</strong> the hyperplane with maximum margin defined by <span class="math notranslate nohighlight">\( {\bf w}^{\ast} \)</span> and <span class="math notranslate nohighlight">\( b^{\ast} \)</span>. These feature vectors are called <strong>support vectors</strong> and, therefore, the classifiers built using this principle are called Support Vector Machines (SVMs). For an arbitrary feature vector <span class="math notranslate nohighlight">\( {\bf x} \)</span>, the SVM classifier is obtained by plugging the maximum margin hyperplane parameters <span class="math notranslate nohighlight">\( {\bf w}^{\ast} \)</span> and <span class="math notranslate nohighlight">\( b^{\ast} \)</span> into <a class="reference internal" href="#equation-hyperplane-classifier">(71)</a>
<div class="math notranslate nohighlight">
\[
h_{SVM}({\bf x}) \triangleq h({\bf x}; {\bf w}^{\ast}, b^{\ast}).
\]</div>
</p>
<figure class="align-left" id="max-margin-decision-boundary-fig">
<a class="reference internal image-reference" href="images/classification/max_margin_decision_boundary.svg"><img alt="images/classification/max_margin_decision_boundary.svg" height="320px" src="images/classification/max_margin_decision_boundary.svg" /></a>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">The maximum margin linear decision boundary separating the <span style="color: red;">red</span> and <span style="color: blue;">blue</span> training samples in <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span>, respectively. The data items highlighted with <span style="color: green;">green</span> borders correspond to the support vectors. The dashed lines illustrate the two parallel hyperplanes with maximum margin between them which are in turn uniquely defined by the support vectors. Note that the dashed lines can touch multiple support vectors of the training dataset splits <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span>. Lastly, the solid line indicates the decision boundary, i.e. the single hyperplane equidistant to the parallel hyperplanes that maximizes the margin between <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span>.</span><a class="headerlink" href="#max-margin-decision-boundary-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<div class="proof remark admonition" id="remark-1">
<p class="admonition-title"><span class="caption-number">Remark 8 </span></p>
<section class="remark-content" id="proof-content">
<p>Real-world training datasets are often not linearly separable. Specifically, for a non-linearly separable training dataset <span class="math notranslate nohighlight">\( {\cal D} \)</span>, there is no hyperplane with parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span> such that, <span class="math notranslate nohighlight">\( \forall \left( {\bf x}_{i}, y_{i} \right) \in {\bf D} \)</span>,</p>
<div class="amsmath math notranslate nohighlight" id="equation-e8a51bd1-6a2c-4a73-9c7e-1242aa4dc858">
<span class="eqno">(73)<a class="headerlink" href="#equation-e8a51bd1-6a2c-4a73-9c7e-1242aa4dc858" title="Permalink to this equation">#</a></span>\[\begin{equation}
h({\bf x}_{i}; {\bf w}, b) = y_{i}.
\end{equation}\]</div>
<p>Multiple causes may lead to non-linearly separable training datasets</p>
<ul class="simple">
<li><p>The training dataset <span class="math notranslate nohighlight">\( {\cal D} \)</span> contains noise due to miss classifications performed by a human supervisor – responsible for assigning <em>true</em> labels <span class="math notranslate nohighlight">\( y_{i} \)</span> to the data items <span class="math notranslate nohighlight">\( {\bf x}_{i} \)</span> within the training dataset;</p></li>
<li><p>Too much noise in the observed data itself – each feature vector <span class="math notranslate nohighlight">\( {\bf x}_{i} \)</span> in <span class="math notranslate nohighlight">\( {\cal D} \)</span> contains noisy observations of the features describing a real-world object to be classified – is preventing the training dataset <span class="math notranslate nohighlight">\( {\cal D} \)</span> to be linearly separable in the feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span>; and</p></li>
<li><p>The data items from different classes are not linearly separable at all in the current feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span> – perhaps we need more features – i.e. a higher dimensional feature space <span class="math notranslate nohighlight">\( {\cal X}' \)</span> – or to apply some transformation to the feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span> to turn it into a linearly separable classification problem.</p></li>
</ul>
<figure class="align-left" id="noisy-training-dataset-fig">
<a class="reference internal image-reference" href="images/classification/noisy_training_dataset.svg"><img alt="images/classification/noisy_training_dataset.svg" height="320px" src="images/classification/noisy_training_dataset.svg" /></a>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Noisy training dataset <span class="math notranslate nohighlight">\( {\cal D} \)</span>. Circles filled in <span style="color: red;">red</span> and <span style="color: blue;">blue</span> represent labeled data items from the set partitions  <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span>, respectively. In particular, the <span style="color: red;">red</span> circle with <span style="color: blue;">blue</span> border corresponds to a data item assigned to the wrong label. On the other hand, the <span style="color: blue;">blue</span> circle with <span style="color: yellow;">yellow</span> border corresponds to a high-noise observed data item in <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span> which fell too far in the region of the feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span> containing data items from <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span>. Note that there is no way to change the dashed line representing a possible linear decision boundary so that it separates <span class="math notranslate nohighlight">\( {\cal D}_{-1} \)</span> from <span class="math notranslate nohighlight">\( {\cal D}_{+1} \)</span>.</span><a class="headerlink" href="#noisy-training-dataset-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</div></section>
<section id="learning-with-hard-constraints">
<h2>Learning with hard-constraints<a class="headerlink" href="#learning-with-hard-constraints" title="Permalink to this headline">#</a></h2>
<p>The problem formulation in <a class="reference internal" href="#equation-svm-form-1">(72)</a> seeks to find the parameters <span class="math notranslate nohighlight">\( {\bf w}^{\ast} \)</span> and <span class="math notranslate nohighlight">\( b^{\ast} \)</span> that maximize the distances between the training samples <span class="math notranslate nohighlight">\( \left( {\bf x}_{i}, y_{i} \right) \in {\cal D} \)</span> to the hyperplane <span class="math notranslate nohighlight">\( {\bf w}^{T} {\bf x} - b = 0 \)</span> subjected to the correct classification of all training samples, i.e. <span class="math notranslate nohighlight">\( h({\bf x}_{i}; {\bf w}, b) = y_{i} \)</span>, <span class="math notranslate nohighlight">\( \forall i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>. Unfortunately, this optimization problem is still too abstract for a practical optimizer to solve it. Thus, we need to reformulate it using geometric principles to turn it into a solvable optimization problem.</p>
<p>Let us consider first the following two parallel hyperplanes</p>
<div class="amsmath math notranslate nohighlight" id="equation-932d5b54-a782-4ba3-b0f8-9d6b8ccd365f">
<span class="eqno">(74)<a class="headerlink" href="#equation-932d5b54-a782-4ba3-b0f8-9d6b8ccd365f" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\bf w}^{T} {\bf x} - b &amp;=&amp; -1\,\, \mbox{(negative boundary)} \nonumber \\
{\bf w}^{T} {\bf x} - b &amp;=&amp; +1\,\, \mbox{(positive boundary)} \nonumber
\end{eqnarray}\]</div>
<p>defining <em>negative</em> and <em>positive</em> boundaries such that the maximum-margin hyperplane <span class="math notranslate nohighlight">\( {\bf w}^{T} {\bf x} - b = 0 \)</span> lies halfway between them. We call the region between these <em>negative</em> and <em>positive</em> boundaries as the <em>margin</em>. Lastly, the distance between the <em>negative</em> and <em>positive</em> boundaries – or equivalently, the margin thickness – is given by <span class="math notranslate nohighlight">\( \frac{2}{||{\bf w}||} \)</span>.</p>
<figure class="align-left" id="margin-planes-03-fig">
<a class="reference internal image-reference" href="_images/margin_planes_03.png"><img alt="_images/margin_planes_03.png" src="_images/margin_planes_03.png" style="height: 480px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">The <em>margin</em> between the <em>negative</em> and <em>positive</em> boundaries with the decision boundary lies half way between them. The <em>margin</em> thickness <span class="math notranslate nohighlight">\( \frac{2}{||{\bf w}||} \)</span> is in turn regulated by the reciprocal of <span class="math notranslate nohighlight">\( || {\bf w} ||\)</span>.</span><a class="headerlink" href="#margin-planes-03-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>As the training samples are not allowed to fall in the margin region, we must select <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span> such that</p>
<div class="amsmath math notranslate nohighlight" id="equation-aab0a13a-333f-4b88-a1f7-9453207bc50f">
<span class="eqno">(75)<a class="headerlink" href="#equation-aab0a13a-333f-4b88-a1f7-9453207bc50f" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\bf w}^{T} {\bf x}_{i} - b &amp;\leq&amp; -1\,\, \mbox{if } y_{i} = -1 \nonumber \\
{\bf w}^{T} {\bf x}_{i} - b &amp;\geq&amp; +1\,\, \mbox{if } y_{i} = +1 \nonumber
\end{eqnarray}\]</div>
<p>for all training samples <span class="math notranslate nohighlight">\( \left( {\bf x}_{i}, y_{i} \right) \in {\cal D} \)</span>. Alternatively, we can write</p>
<div class="amsmath math notranslate nohighlight" id="equation-1155526c-2075-42f3-a503-bda2836cd4fb">
<span class="eqno">(76)<a class="headerlink" href="#equation-1155526c-2075-42f3-a503-bda2836cd4fb" title="Permalink to this equation">#</a></span>\[\begin{equation}
y_{i} \left( {\bf w}^{T} {\bf x} - b \right) \geq +1,  \nonumber
\end{equation}\]</div>
<p>since the class label <span class="math notranslate nohighlight">\( y_{i} \)</span> and the function <span class="math notranslate nohighlight">\( h({\bf x}_{i}; {\bf w}, b) = {\bf w}^{T} {\bf x}_{i} - b \)</span> have always<a class="footnote-reference brackets" href="#footnote2" id="id1">1</a> the same sign <span class="math notranslate nohighlight">\( \forall i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>.</p>
<p>Thus, we can rewrite the optimization problem in <a class="reference internal" href="#equation-svm-form-1">(72)</a> as</p>
<div class="math notranslate nohighlight" id="equation-svm-form-2">
<span class="eqno">(77)<a class="headerlink" href="#equation-svm-form-2" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\left( {\bf w}^{\ast}, b^{\ast} \right) &amp;=&amp; \argmax_{\left( {\bf w}, b \right) \in \mathbb{R}^{D+1}} \frac{2}{||{\bf w}||} \\
&amp;\equiv&amp; \argmax_{\left( {\bf w}, b \right) \in \mathbb{R}^{D+1}} \frac{1}{||{\bf w}||} \\
s.t. &amp;&amp;  y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1, \,\,\, \forall \left( {\bf x}_{i}, y_{i} \right) \in {\cal D},
\end{eqnarray}\end{split}\]</div>
<p>i.e. we seek to find the parameters <span class="math notranslate nohighlight">\( {\bf w}^{\ast} \)</span> and <span class="math notranslate nohighlight">\( b^{\ast} \)</span> that maximize the margin thickness <span class="math notranslate nohighlight">\( \frac{2}{||{\bf w}||} \)</span> subjected to the linear constrains <span class="math notranslate nohighlight">\( y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1 \)</span>, <span class="math notranslate nohighlight">\( \forall i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>. However, this is a non-convex optimization problem as the denominator <span class="math notranslate nohighlight">\( ||{\bf w}|| \)</span> introduces a singularity in the objective function <span class="math notranslate nohighlight">\( \frac{1}{||{\bf w}||} \)</span> in <a class="reference internal" href="#equation-svm-form-2">(77)</a>. Fortunately, we can reformulate the problem of maximizing the reciprocal of the norm <span class="math notranslate nohighlight">\( ||{\bf w}|| \)</span> into the problem o minimizing the norm <span class="math notranslate nohighlight">\( ||{\bf w}|| \)</span> itself</p>
<div class="math notranslate nohighlight" id="equation-svm-form-3">
<span class="eqno">(78)<a class="headerlink" href="#equation-svm-form-3" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\left( {\bf w}^{\ast}, b^{\ast} \right) &amp;=&amp; \argmin_{\left( {\bf w}, b \right) \in \mathbb{R}^{D+1}} ||{\bf w}|| \\
s.t. &amp;&amp;  y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1, \,\,\, \forall \left( {\bf x}_{i}, y_{i} \right) \in {\cal D},
\end{eqnarray}\end{split}\]</div>
<p>which is in turn a convex optimization problem. For convenience, we replace the objective function <span class="math notranslate nohighlight">\( ||{\bf w}|| \)</span> in <a class="reference internal" href="#equation-svm-form-3">(78)</a> by <span class="math notranslate nohighlight">\( \frac{1}{2}||{\bf w}||^{2} \)</span> – which is a quadratic function differentiable everywhere – and write</p>
<div class="math notranslate nohighlight" id="equation-svm-form-4">
<span class="eqno">(79)<a class="headerlink" href="#equation-svm-form-4" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\left( {\bf w}^{\ast}, b^{\ast} \right) &amp;=&amp; \argmin_{\left( {\bf w}, b \right) \in \mathbb{R}^{D+1}} \frac{1}{2}||{\bf w}||^{2} \\
s.t. &amp;&amp;  y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1, \,\,\, \forall \left( {\bf x}_{i}, y_{i} \right) \in {\cal D},
\end{eqnarray}\end{split}\]</div>
<p>so that we can use e.g. gradient descent solvers to optimize the parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span>.</p>
<p>In summary, we transformed the non-convex optimization problem of maximizing the margin in <a class="reference internal" href="#equation-svm-form-1">(72)</a> into a convex optimization problem of minimizing the squared norm <span class="math notranslate nohighlight">\( \frac{1}{2}||{\bf w}||^{2} \)</span> in <a class="reference internal" href="#equation-svm-form-4">(79)</a> which is subjected in turn to several margin constraints <div class="math notranslate nohighlight">
\[ y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1, \]</div>
 for <span class="math notranslate nohighlight">\( i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>.</p>
</section>
<section id="learning-with-soft-constraints">
<h2>Learning with soft-constraints<a class="headerlink" href="#learning-with-soft-constraints" title="Permalink to this headline">#</a></h2>
<p>Unfortunately, the hard-margin formulation still requires a linearly separable training dataset <span class="math notranslate nohighlight">\( {\cal D} \)</span> so that the linear constraints <div class="math notranslate nohighlight">
\[ y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1, \]</div>
 <span class="math notranslate nohighlight">\( i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>, can be satisfied. To overcome this limitation, we can relax the linear constraints by discounting a fixed amount <span class="math notranslate nohighlight">\( \xi_{i} \geq 0 \)</span> from each constraint as <div class="math notranslate nohighlight">
\[ y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1 - \xi_{i} \]</div>
 such that the original constraint is fully imposed for <span class="math notranslate nohighlight">\( \xi_{i} = 0 \)</span> and it is incrementally relaxed as <span class="math notranslate nohighlight">\( \xi_{i} \)</span> grows for <span class="math notranslate nohighlight">\( \xi_{i} &gt; 0 \)</span>. Let the vector <span class="math notranslate nohighlight">\( \boldsymbol{\xi} = \begin{bmatrix} \xi_{1} &amp; \xi_{2} &amp; \ldots &amp; \xi_{N} \end{bmatrix}^{T} \)</span> collect all <span class="math notranslate nohighlight">\( N \)</span> <em>slack</em> variables. Thus, we can rewrite the optimization problem in <a class="reference internal" href="#equation-svm-form-4">(79)</a> assuming relaxed linear constraints as</p>
<div class="math notranslate nohighlight" id="equation-svm-form-5">
<span class="eqno">(80)<a class="headerlink" href="#equation-svm-form-5" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\left( {\bf w}^{\ast}, b^{\ast}, \boldsymbol{\xi}^{\ast} \right) &amp;=&amp; \argmin_{\left( {\bf w}, b, \boldsymbol{\xi} \right) \in \mathbb{R}^{D+N+1}} \left\lbrace \frac{1}{2}||{\bf w}||^{2} + C \sum_{i=1}^{N} \xi_{i} \right\rbrace \\
s.t. &amp;&amp;  y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq +1 - \xi_{i} \\
&amp;&amp; \xi_{i} \geq 0, \,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace,
\end{eqnarray}\end{split}\]</div>
<p>where the hyperparameter <span class="math notranslate nohighlight">\( C &gt; 0 \)</span> regulates the trade off between the original objective function <span class="math notranslate nohighlight">\( \frac{1}{2}||{\bf w}||^{2} \)</span> and the slacks sum <span class="math notranslate nohighlight">\( \sum_{i=1}^{N} \xi_{i} \)</span> such that <span class="math notranslate nohighlight">\( C \rightarrow \infty \)</span> leads to the original optimization problem with hard margin. Conversely, as <span class="math notranslate nohighlight">\( C \rightarrow 0^{+} \)</span>, the constraints are increasingly relaxed. Note that this formulation allows the <span class="math notranslate nohighlight">\(i\)</span>-th training example to violate the original constraint by a fixed amount <span class="math notranslate nohighlight">\( \xi_{i} \)</span> (slack). However, the optimization problem in <a class="reference internal" href="#equation-svm-form-5">(80)</a> is set up such that the sum of these violations <span class="math notranslate nohighlight">\( \sum_{i=1}^{N} \xi_{i} \)</span> is also minimized.</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The slack variables <span class="math notranslate nohighlight">\( \lbrace \xi_{1}, \ldots, \xi_{N} \rbrace \)</span> allow therefore to train a SVM classifier using linearly non-separable datasets.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We refer to <a class="reference internal" href="#equation-svm-form-5">(80)</a> as the <strong>primal</strong> SVM formulation which allows us to build a SVM classifier using either linearly separable or linearly non-separable training datasets.</p>
</div>
</aside>
<div class="proof remark admonition" id="remark-2">
<p class="admonition-title"><span class="caption-number">Remark 9 </span></p>
<section class="remark-content" id="proof-content">
<p>Equivalently, the soft-margin problem in <a class="reference internal" href="#equation-svm-form-5">(80)</a> can be reformulated as a convex optimization problem without constraints. In particular, we can write</p>
<div class="math notranslate nohighlight" id="equation-svm-form-6">
<span class="eqno">(81)<a class="headerlink" href="#equation-svm-form-6" title="Permalink to this equation">#</a></span>\[\begin{equation}
\left( {\bf w}^{\ast}, b^{\ast} \right) = \argmin_{\left( {\bf w}, b, \boldsymbol{\xi} \right) \in \mathbb{R}^{D+1}} \left\lbrace \frac{1}{2}||{\bf w}||^{2} + C \sum_{i=1}^{N} \underbrace{ \max \left( 1 - y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right), 0 \right)}_{\mbox{hinge loss}} \right\rbrace,
\end{equation}\]</div>
<p>in which the hinge loss <div class="math notranslate nohighlight">
\[ \epsilon_{i} \triangleq \max \left( 1 - y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right), 0 \right) \]</div>
 associated with the <span class="math notranslate nohighlight">\( i \)</span>-th training data item is such that</p>
<div class="amsmath math notranslate nohighlight" id="equation-7ef6bab3-3aa9-442b-828c-a18ff0658e07">
<span class="eqno">(82)<a class="headerlink" href="#equation-7ef6bab3-3aa9-442b-828c-a18ff0658e07" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq 1 &amp;\rightarrow&amp; \epsilon_{i} = 0\,\, \mbox{(no penalty)} \nonumber \\
0 \leq y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) &lt; 1 &amp;\rightarrow&amp; 0 &lt; \epsilon_{i} \leq 1\,\, \mbox{(small penalty)} \nonumber \\
y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) &lt; 0 &amp;\rightarrow&amp; \epsilon_{i} &gt; 1\,\, \mbox{(unbounded penalty)}. \nonumber
\end{eqnarray}\]</div>
<p>Note that there is no penalty in the first case, since the training sample <span class="math notranslate nohighlight">\( \left( {\bf x}_{i}, y_{i} \right) \)</span> is not violating the constraint. On the other hand, there is a small penalty in the second case, as the data item <span class="math notranslate nohighlight">\( {\bf x}_{i} \)</span> falls within the margin, but still in the right side of the hyperplane defined by <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span>. The hinge loss grows unbounded in the last case since the data item <span class="math notranslate nohighlight">\( {\bf x}_{i} \)</span> falls on the wrong side of the hyperplane in this case. The objective function in <a class="reference internal" href="#equation-svm-form-6">(81)</a> penalizes thus the sum of the hinge losses committed by all data items in <span class="math notranslate nohighlight">\( {\cal D} \)</span>. Lastly, as the hinge loss function has a non-linearity around the origin – it is clamped to zero when the constraint <span class="math notranslate nohighlight">\( y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \geq 1 \)</span> is satisfied –, the objective function is neither differentiable everywhere nor quadratic with respect to the parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span> anymore.</p>
</section>
</div><div class="proof example admonition" id="example-3">
<p class="admonition-title"><span class="caption-number">Example 23 </span></p>
<section class="example-content" id="proof-content">
<p>The effect of the hyperparameter <span class="math notranslate nohighlight">\( C \)</span> on the decision boundary. Note that, as <span class="math notranslate nohighlight">\( C \)</span> grows, the maximum-margin hyperplane becomes more diplomatic in the sense of keeping as much as possible distance to the training examples from different classes. Lastly, note that the final solution becomes influence by more and more data items in <span class="math notranslate nohighlight">\( {\cal D} \)</span> as <span class="math notranslate nohighlight">\( C \)</span> decreases, i.e. more and more support vectors – corresponding to training samples <span class="math notranslate nohighlight">\( \lbrace \left( {\bf x}', y'\right) \in {\cal D} \mid y' \left( {\bf w}^{T} {\bf x}' - b \right) \leq 1 \rbrace \)</span> – will contribute to determine the linear decision boundary <span class="math notranslate nohighlight">\( {\bf w}^{T} {\bf x}' - b = 0 \)</span>. On the other hand, the training samples <span class="math notranslate nohighlight">\( \lbrace \left( {\bf x}', y'\right) \in {\cal D} \mid y' \left( {\bf w}^{T} {\bf x}' - b \right) &gt; 1 \rbrace \)</span> do not contribute directly to the values of the parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span> in the sense that any changes of their positions in the feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span> without violating the constraints lead to the same solution.</p>
<figure class="align-left" id="effect-softmargin-sep-fig">
<a class="reference internal image-reference" href="_images/effect_softmargin_sep.png"><img alt="_images/effect_softmargin_sep.png" src="_images/effect_softmargin_sep.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">Linearly separable dataset.</span><a class="headerlink" href="#effect-softmargin-sep-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-left" id="effect-softmargin-nonsep-fig">
<a class="reference internal image-reference" href="_images/effect_softmargin_nonsep.png"><img alt="_images/effect_softmargin_nonsep.png" src="_images/effect_softmargin_nonsep.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">Linearly non-separable dataset.</span><a class="headerlink" href="#effect-softmargin-nonsep-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</div></section>
<section id="a-word-on-duality">
<h2>A word on duality<a class="headerlink" href="#a-word-on-duality" title="Permalink to this headline">#</a></h2>
<p>Consider first the <strong>primal</strong> optimization problem of minimizing an objective function <span class="math notranslate nohighlight">\( f:{\cal Z} \rightarrow \mathbb{R} \)</span> across some space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> subjected to several constraints. We can write down this problem using the standard format as</p>
<div class="math notranslate nohighlight" id="equation-primal-prob1">
<span class="eqno">(83)<a class="headerlink" href="#equation-primal-prob1" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\minimize_{{\bf z} \in {\cal Z}} &amp;&amp;  f({\bf z}) \\
s.t. \,\,\, &amp;&amp;  g_{i} ({\bf z}) \leq 0,\,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace,
\end{eqnarray}\end{split}\]</div>
<p>in which we omitted equality constraints of the type <span class="math notranslate nohighlight">\( h_{j}({\bf z}) = 0 \)</span>, <span class="math notranslate nohighlight">\( j \in \lbrace 1, 2, \ldots M \rbrace \)</span>, for convenience. Now, let</p>
<div class="math notranslate nohighlight" id="equation-lagrandian">
<span class="eqno">(84)<a class="headerlink" href="#equation-lagrandian" title="Permalink to this equation">#</a></span>\[{\cal L}({\bf z}, \boldsymbol{\lambda}) = f({\bf z}) + \sum_{i=1}^{N} \lambda_{i} g_{i}({\bf z})\]</div>
<p>be the Lagrangian of the primal problem as stated in <a class="reference internal" href="#equation-primal-prob1">(83)</a> in which the vector <span class="math notranslate nohighlight">\( \boldsymbol{\lambda} \triangleq \begin{bmatrix} \lambda_{1} &amp; \lambda_{2} &amp; \ldots &amp; \lambda_{N} \end{bmatrix}^{T} \)</span> collect the so-called Lagrange multipliers <span class="math notranslate nohighlight">\( \lbrace \lambda_{i} \rbrace \)</span> corresponding to the constraints <span class="math notranslate nohighlight">\( \lbrace  g_{i} ({\bf z}) \leq 0 \rbrace \)</span>, <span class="math notranslate nohighlight">\( i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>. The <span class="math notranslate nohighlight">\( i \)</span>-th Lagrangian multiplier determines how much penalty is assigned to the violation of the constraint <span class="math notranslate nohighlight">\( g_{i} ({\bf z}) \leq 0 \)</span> such that any violation is allowed for <span class="math notranslate nohighlight">\( \lambda_{i} = 0 \)</span> and no violation is allowed at all for <span class="math notranslate nohighlight">\( \lambda_{i} \rightarrow \infty \)</span>. Thus, the hard constraints in <a class="reference internal" href="#equation-primal-prob1">(83)</a> can be enforced in <a class="reference internal" href="#equation-lagrandian">(84)</a> by chosen sufficiently high values for the Lagrange multipliers.</p>
<p>Let us define</p>
<div class="math notranslate nohighlight" id="equation-lagrandian-primal">
<span class="eqno">(85)<a class="headerlink" href="#equation-lagrandian-primal" title="Permalink to this equation">#</a></span>\[{\cal L}_{primal}({\bf z}) = \max_{\boldsymbol{\lambda} \geq {\bf 0}} {\cal L}({\bf z}, \boldsymbol{\lambda})\]</div>
<p>as the maximum of the Lagrangian in <a class="reference internal" href="#equation-lagrandian">(84)</a> for some value of <span class="math notranslate nohighlight">\( {\bf z} \in {\cal Z} \)</span>.</p>
<p>Assume that all constraints <span class="math notranslate nohighlight">\( \lbrace g_{i} ({\bf z}') \leq 0 \rbrace \)</span>, <span class="math notranslate nohighlight">\( \forall i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>, are satisfied for a feasible <span class="math notranslate nohighlight">\( {\bf z}' \)</span>, then all <span class="math notranslate nohighlight">\( \lbrace g_{i} ({\bf z}') \rbrace \)</span> in the right-hand side of <a class="reference internal" href="#equation-lagrandian">(84)</a> will be negative and the best thing we can do to maximize the Lagrangian <span class="math notranslate nohighlight">\( {\cal L}({\bf z}', \boldsymbol{\lambda}) \)</span> in <a class="reference internal" href="#equation-lagrandian-primal">(85)</a> is to choose <span class="math notranslate nohighlight">\( \boldsymbol{\lambda} = {\bf 0} \)</span>. On the other hand, let us assume that <span class="math notranslate nohighlight">\( {\bf z}' \)</span> is an unfeasible point that violates at least one of the constraints, let us say the <span class="math notranslate nohighlight">\(i\)</span>-th constraint. In this case, the best thing we can do to maximize the Lagrangian <span class="math notranslate nohighlight">\( {\cal L}({\bf z}', \boldsymbol{\lambda}) \)</span> in <a class="reference internal" href="#equation-lagrandian-primal">(85)</a> is to allow the <span class="math notranslate nohighlight">\( i \)</span>-th Lagrange multiplier to grow unbounded, i.e. to make <span class="math notranslate nohighlight">\( \lambda_{i} \rightarrow \infty \)</span>.</p>
<p>As the maximization in <a class="reference internal" href="#equation-lagrandian-primal">(85)</a> strongly penalizes unfeasible points, we can restate then the <strong>primal</strong> problem <a class="reference internal" href="#equation-primal-prob1">(83)</a> using the Lagrangian <a class="reference internal" href="#equation-lagrandian">(84)</a> as</p>
<div class="math notranslate nohighlight" id="equation-primal-prob2">
<span class="eqno">(86)<a class="headerlink" href="#equation-primal-prob2" title="Permalink to this equation">#</a></span>\[\min_{{\bf z} \in {\cal Z}} {\cal L}_{primal}({\bf z}).\]</div>
<p>In the sequel, let</p>
<div class="math notranslate nohighlight" id="equation-lagrandian-dual">
<span class="eqno">(87)<a class="headerlink" href="#equation-lagrandian-dual" title="Permalink to this equation">#</a></span>\[{\cal L}_{dual}(\boldsymbol{\lambda}) = \min_{{\bf z} \in {\cal Z}} {\cal L}({\bf z}, \boldsymbol{\lambda})\]</div>
<p>be the minimum of Lagrangian in <a class="reference internal" href="#equation-lagrandian">(84)</a> for some <span class="math notranslate nohighlight">\( \boldsymbol{\lambda} \geq {\bf 0} \)</span>. In this case, for a fixed vector <span class="math notranslate nohighlight">\( \boldsymbol{\lambda}' \)</span> modulating how strongly violations to the constraints <span class="math notranslate nohighlight">\( \lbrace g_{i} ({\bf z}') \leq 0 \rbrace \)</span>, <span class="math notranslate nohighlight">\( \forall i \in \lbrace 1, 2, \ldots, N \rbrace \)</span> shall be penalized, we find some point <span class="math notranslate nohighlight">\( {\bf z} \in {\cal Z} \)</span> in <a class="reference internal" href="#equation-lagrandian-dual">(87)</a> that minimizes the Lagrangian <span class="math notranslate nohighlight">\( {\cal L}({\bf z}, \boldsymbol{\lambda}') \)</span>.</p>
<p>Additionally, one can show that <span class="math notranslate nohighlight">\( \forall \boldsymbol{\lambda} \geq {\bf 0} \)</span></p>
<div class="math notranslate nohighlight" id="equation-dual-prob2">
<span class="eqno">(88)<a class="headerlink" href="#equation-dual-prob2" title="Permalink to this equation">#</a></span>\[{\cal L}_{dual}(\boldsymbol{\lambda}) \leq \min_{{\bf z} \in {\cal Z}} {\cal L}_{primal}({\bf z}).\]</div>
<p>That is, the <strong>primal</strong> problem in <a class="reference internal" href="#equation-primal-prob2">(86)</a> is lower bounded by <span class="math notranslate nohighlight">\( {\cal L}_{dual}(\boldsymbol{\lambda}) \)</span>. Hence, one can find a tighter lower bound to the solution of the original problem by finding the Lagrangian multipliers <span class="math notranslate nohighlight">\( \boldsymbol{\lambda} \)</span> that maximizes <a class="reference internal" href="#equation-lagrandian-dual">(87)</a>. Specifically, the following tighter lower-bound holds</p>
<div class="math notranslate nohighlight" id="equation-lower-bound">
<span class="eqno">(89)<a class="headerlink" href="#equation-lower-bound" title="Permalink to this equation">#</a></span>\[\max_{\boldsymbol{\lambda} \geq {\bf 0}} {\cal L}_{dual}(\boldsymbol{\lambda}) \leq \min_{{\bf z} \in {\cal Z}} {\cal L}_{primal}({\bf z}).\]</div>
<p>The alternative formulation</p>
<div class="math notranslate nohighlight" id="equation-dual-prob1">
<span class="eqno">(90)<a class="headerlink" href="#equation-dual-prob1" title="Permalink to this equation">#</a></span>\[\max_{\boldsymbol{\lambda} \geq {\bf 0}} {\cal L}_{dual}(\boldsymbol{\lambda})\]</div>
<p>from the left-hand side of <a class="reference internal" href="#equation-dual-prob1">(90)</a> is called the <strong>dual</strong> problem and has some amenable properties. In particular, it is a lower bound to the <strong>primal</strong> problem. Finally, it is worth noting that solving the <strong>dual</strong> problem in <a class="reference internal" href="#equation-dual-prob1">(90)</a> is equivalent to finding the Lagrange multipliers in <span class="math notranslate nohighlight">\( \boldsymbol{\lambda} \geq {\bf 0} \)</span> that lead to the tightest (best) lower bound in <a class="reference internal" href="#equation-dual-prob2">(88)</a>.</p>
<div class="proof remark admonition" id="remark-4">
<p class="admonition-title"><span class="caption-number">Remark 10 </span></p>
<section class="remark-content" id="proof-content">
<p>From <a class="reference internal" href="#equation-lower-bound">(89)</a>, we conclude that, for any <span class="math notranslate nohighlight">\( {\bf z} \in {\cal Z} \)</span> and <span class="math notranslate nohighlight">\( \boldsymbol{\lambda} \geq {\bf 0} \)</span>, <div class="math notranslate nohighlight">
\[ {\cal L}_{dual}(\boldsymbol{\lambda}) \leq {\cal L}_{primal}({\bf z}). \]</div>
 Thus, in general, the <strong>dual</strong> problem <div class="math notranslate nohighlight">
\[ \max_{\boldsymbol{\lambda} \geq {\bf 0}} {\cal L}_{dual}(\boldsymbol{\lambda}) \]</div>
 is a lower bound to the <strong>primal</strong> problem. Alternatively, we can write <div class="math notranslate nohighlight">
\[ \max_{\boldsymbol{\lambda} \geq {\bf 0}} {\cal L}_{dual}(\boldsymbol{\lambda}) \leq \min_{{\bf z} \in {\cal Z}} {\cal L}_{primal}({\bf z}). \]</div>
 However, for convex optimization problems, the <strong>primal</strong> and <strong>dual</strong> problems are tight, i.e. their optimal values are the same <div class="math notranslate nohighlight">
\[ \max_{\boldsymbol{\lambda} \geq {\bf 0}} {\cal L}_{dual}(\boldsymbol{\lambda}) = \min_{{\bf z} \in {\cal Z}} {\cal L}_{primal}({\bf z}). \]</div>
 Thus, in some cases, the <strong>dual</strong> problem also provides a solution to the <strong>primal</strong> one.</p>
</section>
</div><aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <strong>dual</strong> formulation in <a class="reference internal" href="#equation-dual-prob1">(90)</a> is also useful even when the <strong>primal</strong> and <strong>dual</strong> problems are not tight. Sometimes the solution to the <strong>primal</strong> problem is hard to achieve while the <strong>dual</strong> problem has a computationally efficient solution. Thus, we can solve the <strong>dual</strong> problem to evaluate how close to the lower bound an iterative solution to the <strong>primal</strong> problem was able to reach so far and use it as a stop criteria.</p>
</div>
</aside>
</section>
<section id="dual-svm-formulation">
<h2>Dual SVM formulation<a class="headerlink" href="#dual-svm-formulation" title="Permalink to this headline">#</a></h2>
<p>Let us rewrite the hard-margin SVM problem in <a class="reference internal" href="#equation-svm-form-4">(79)</a> using the standard format as</p>
<div class="math notranslate nohighlight" id="equation-svm-form-7">
<span class="eqno">(91)<a class="headerlink" href="#equation-svm-form-7" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\minimize_{{\bf w} \in \mathbb{R}^{D}, b \in \mathbb{R}} &amp;&amp;  \frac{1}{2}||{\bf w}||^{2} \\
s.t. \,\,\, &amp;&amp;  1 - y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right) \leq 0, \,\,\, \forall \left( {\bf x}_{i}, y_{i} \right) \in {\cal D}.
\end{eqnarray}\end{split}\]</div>
<p>We can write the Lagrangian of <a class="reference internal" href="#equation-svm-form-7">(91)</a> as</p>
<div class="math notranslate nohighlight" id="equation-lagrandian2">
<span class="eqno">(92)<a class="headerlink" href="#equation-lagrandian2" title="Permalink to this equation">#</a></span>\[{\cal L}({\bf z}, \boldsymbol{\lambda}) = f({\bf z}) + \sum_{i=1}^{N} \lambda_{i} \left( 1 - y_{i} \left( {\bf w}^{T} {\bf x}_{i} - b \right)  \right),\]</div>
<p>which is convex with respect to the parameters <span class="math notranslate nohighlight">\( {\bf w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span>. Thus, we can find the solution to <div class="math notranslate nohighlight">
\[ {\cal L}_{dual}(\boldsymbol{\lambda}) = \min_{{\bf z} \in {\cal Z}} {\cal L}({\bf z}, \boldsymbol{\lambda}) \]</div>
 by setting both the gradient <div class="math notranslate nohighlight">
\[ \nabla_{\bf w} {\cal L}({\bf z}, \boldsymbol{\lambda}) = {\bf w} - \sum_{i=1}^{N} \lambda_{i} y_{i} {\bf x}_{i} \]</div>
 and the partial derivative <div class="math notranslate nohighlight">
\[ \frac{\partial {\cal L}({\bf z}, \boldsymbol{\lambda})}{\partial b} = \sum_{i=1}^{N} \lambda_{i} y_{i} \]</div>
 to zero. Hence, we write</p>
<div class="math notranslate nohighlight" id="equation-w-grad-zero">
<span class="eqno">(93)<a class="headerlink" href="#equation-w-grad-zero" title="Permalink to this equation">#</a></span>\[\nabla_{\bf w} {\cal L}({\bf z}, \boldsymbol{\lambda}) = 0 \Leftrightarrow {\bf w} = \sum_{i=1}^{N} \lambda_{i} y_{i} {\bf x}_{i}\]</div>
<div class="math notranslate nohighlight" id="equation-cond-partial-zero">
<span class="eqno">(94)<a class="headerlink" href="#equation-cond-partial-zero" title="Permalink to this equation">#</a></span>\[\frac{\partial {\cal L}({\bf z}, \boldsymbol{\lambda})}{\partial b} = 0 \Leftrightarrow \sum_{i=1}^{N} \lambda_{i} y_{i} = 0.\]</div>
<p>Now, by plugging <a class="reference internal" href="#equation-w-grad-zero">(93)</a> and <a class="reference internal" href="#equation-cond-partial-zero">(94)</a> back into the Lagrangian definition <a class="reference internal" href="#equation-lagrandian2">(92)</a>, we obtain</p>
<div class="amsmath math notranslate nohighlight" id="equation-7affdf02-4320-4d09-897c-da79dbff7c78">
<span class="eqno">(95)<a class="headerlink" href="#equation-7affdf02-4320-4d09-897c-da79dbff7c78" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
{\cal L}_{dual}(\boldsymbol{\lambda}) &amp;=&amp; \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j} + \sum_{i=1}^{N} \lambda_{i} - \underbrace{\sum_{i=1}^{N} \lambda_{i}y_{i} {\bf x}_{i} \sum_{j=1}^{N} \lambda_{j} y_{j} {\bf x}_{j}}_{= \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j}} + \underbrace{\sum_{i=1}^{N} \lambda_{i} y_{i}}_{= 0} b \nonumber \\
&amp;=&amp; - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j} + \sum_{i=1}^{N} \lambda_{i}.
\end{eqnarray}\]</div>
<p>Thus, we can write the dual problem in <a class="reference internal" href="#equation-dual-prob1">(90)</a> as</p>
<div class="math notranslate nohighlight" id="equation-svm-form8">
<span class="eqno">(96)<a class="headerlink" href="#equation-svm-form8" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\maximize_{\boldsymbol{\lambda} \geq {\bf 0}} &amp;&amp;  - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j} + \sum_{i=1}^{N} \lambda_{i} \\
s.t. \,\,\, &amp;&amp;  \sum_{i=1}^{N} \lambda_{i} y_{i} = 0 \\
&amp;&amp;  0 \leq \lambda_{i}, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace,
\end{eqnarray}\end{split}\]</div>
<p>which is clearly a quadratic program leading therefore to a convex optimization problem. Hence, the optimal <span class="math notranslate nohighlight">\( \boldsymbol{\lambda}^{\ast} \)</span> in <a class="reference internal" href="#equation-svm-form8">(96)</a> also delivers the optimal solution for the primal problem in <a class="reference internal" href="#equation-svm-form-7">(91)</a>. Specifically,</p>
<div class="math notranslate nohighlight" id="equation-w-optimal">
<span class="eqno">(97)<a class="headerlink" href="#equation-w-optimal" title="Permalink to this equation">#</a></span>\[{\bf w}^{\ast} = \sum_{i=1}^{N} \lambda_{i}^{\ast} y_{i} {\bf x}_{i}\]</div>
<div class="math notranslate nohighlight" id="equation-b-optimal">
<span class="eqno">(98)<a class="headerlink" href="#equation-b-optimal" title="Permalink to this equation">#</a></span>\[b^{\ast} = ({\bf w}^{\ast})^{T} {\bf x}_{j} - y_{j},\]</div>
<p>in which we plug any training example <span class="math notranslate nohighlight">\( \left( {\bf x}_{j}, y_{j} \right) \)</span> with index <span class="math notranslate nohighlight">\( j \in \lbrace 1, 2, \ldots, N \rbrace \)</span> such that <span class="math notranslate nohighlight">\( \lambda_{j} &gt; 0 \)</span>.</p>
<p>Finally, we offer without proof that <a class="reference internal" href="#equation-svm-form8">(96)</a> can be rewritten to consider soft-margin constraints simply by plugging in the hyperparameter <span class="math notranslate nohighlight">\( C \)</span> into the linear constraints as follows</p>
<div class="math notranslate nohighlight" id="equation-svm-form9">
<span class="eqno">(99)<a class="headerlink" href="#equation-svm-form9" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\maximize_{\boldsymbol{\lambda} \geq {\bf 0}}  &amp;&amp;  - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j} + \sum_{i=1}^{N} \lambda_{i} \\
s.t. \,\,\, &amp;&amp;  \sum_{i=1}^{N} \lambda_{i} y_{i} = 0 \\
&amp;&amp;  0 \leq \lambda_{i} \leq C, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace.
\end{eqnarray}\end{split}\]</div>
<p>Alternatively, we can rewrite the dual problem in <a class="reference internal" href="#equation-svm-form9">(99)</a> more concisely in terms of the so-called Gram matrix, a.k.a. pairwise influence matrix, <span class="math notranslate nohighlight">\( {\bf G} \triangleq \left[ G_{i,j} \right] \)</span> with <span class="math notranslate nohighlight">\( G_{i,j} = y_{i} y_{j} {\bf x}_{i}^{T} {\bf x}_{j} \)</span> for <span class="math notranslate nohighlight">\( i,j \in \lbrace 1, 2, \ldots N \rbrace \)</span> as</p>
<div class="math notranslate nohighlight" id="equation-svm-form10">
<span class="eqno">(100)<a class="headerlink" href="#equation-svm-form10" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\maximize_{\boldsymbol{\lambda} \geq {\bf 0}}  &amp;&amp;  - \frac{1}{2} \boldsymbol{\lambda}^{T} {\bf G} \boldsymbol{\lambda} + {\bf 1}^{T} \boldsymbol{\lambda} \\
s.t. \,\,\, &amp;&amp;   {\bf y}^{T} \boldsymbol{\lambda} = 0 \\
&amp;&amp;  0 \leq \lambda_{i} \leq C, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace,
\end{eqnarray}\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\( {\bf y} \triangleq \begin{bmatrix} y_{1} &amp; y_{2} &amp; \ldots &amp; y_{N} \end{bmatrix}^{T} \)</span> and <span class="math notranslate nohighlight">\( \boldsymbol{\lambda} = \begin{bmatrix} \lambda_{1} &amp; \lambda_{2} &amp; \ldots &amp; \lambda_{N} \end{bmatrix}^{T} \)</span> collecting the labels and the slack variables associated with each training example, respectively, and <span class="math notranslate nohighlight">\( {\bf 1} \triangleq \begin{bmatrix} 1 &amp; 1 &amp; \ldots &amp; 1 \end{bmatrix}^{T} \)</span> representing a vector of <span class="math notranslate nohighlight">\(1\)</span>’s with appropriate number of dimensions – in this case <span class="math notranslate nohighlight">\( N \)</span>. Note also that <a class="reference internal" href="#equation-svm-form10">(100)</a> is equivalent to writing</p>
<div class="math notranslate nohighlight" id="equation-svm-form11">
<span class="eqno">(101)<a class="headerlink" href="#equation-svm-form11" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\minimize_{\boldsymbol{\lambda} \geq {\bf 0}}  &amp;&amp;  \frac{1}{2} \boldsymbol{\lambda}^{T} {\bf G} \boldsymbol{\lambda} - {\bf 1}^{T} \boldsymbol{\lambda} \\
s.t. \,\,\, &amp;&amp;   {\bf y}^{T} \boldsymbol{\lambda} = 0 \\
&amp;&amp;  0 \leq \lambda_{i} \leq C, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace.
\end{eqnarray}\end{split}\]</div>
<div class="proof remark admonition" id="remark-5">
<p class="admonition-title"><span class="caption-number">Remark 11 </span></p>
<section class="remark-content" id="proof-content">
<p>As the objective function <span class="math notranslate nohighlight">\( \frac{1}{2} || {\bf w} ||^{2} \)</span> in <a class="reference internal" href="#equation-svm-form9">(99)</a> depends on the coefficient vector <div class="math notranslate nohighlight">
\[ {\bf w} = \begin{bmatrix} w_{1} &amp; w_{2} &amp; \ldots &amp; w_{D} \end{bmatrix}^{T}, \]</div>
 the primal SVM problem has computational complexity <span class="math notranslate nohighlight">\( {\cal O}(D) \)</span> with <span class="math notranslate nohighlight">\( D \)</span> denoting the number of features in the feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span>. On the other hand, the dual SVM problem in <a class="reference internal" href="#equation-svm-form9">(99)</a> is clearly quadratic on the number of samples <span class="math notranslate nohighlight">\( N \)</span>, i.e it has computational complexity <span class="math notranslate nohighlight">\( {\cal O} (N^{2}) \)</span>. Thus, for a large number of features <span class="math notranslate nohighlight">\( D \gg N \)</span>, the dual SVM problem can be cheaper, while the solution to the primal SVM problem has a smaller computational burden for large datasets with <span class="math notranslate nohighlight">\( N \gg D \)</span>.</p>
</section>
</div><aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The objective function of the dual SVM problem formulations in <a class="reference internal" href="#equation-svm-form8">(96)</a>–<a class="reference internal" href="#equation-svm-form11">(101)</a> depends only on the inner products <span class="math notranslate nohighlight">\( \lbrace {\bf x}_{i}^{T} {\bf x}_{j} \rbrace \)</span> with <span class="math notranslate nohighlight">\( i,j \in \lbrace 1, 2, \ldots, N \rbrace \)</span>. This dependence unlocks the kernel trick that will be discussed in the sequel.</p>
</div>
</aside>
</section>
<section id="a-word-on-kernels">
<h2>A word on kernels<a class="headerlink" href="#a-word-on-kernels" title="Permalink to this headline">#</a></h2>
<p>A kernel function <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> maps pairs of vectors <span class="math notranslate nohighlight">\( {\bf x}_{i}, {\bf x}_{j} \)</span> residing in a <span class="math notranslate nohighlight">\(D\)</span>-dimensional Euclidean space <span class="math notranslate nohighlight">\( \mathbb{R}^{D} \)</span> into real numbers in <span class="math notranslate nohighlight">\( \mathbb{R} \)</span>, i.e. <div class="math notranslate nohighlight">
\[ k: \mathbb{R}^{D} \times \mathbb{R}^{D} \rightarrow \mathbb{R}. \]</div>
 Furthermore, a kernel is positive definite if for any <em>finite</em> collection of vectors <span class="math notranslate nohighlight">\( {\bf x}_{1}, \ldots, {\bf x}_{N} \)</span> and any collection of real numbers <span class="math notranslate nohighlight">\( a_{1}, \ldots, a_{N} \)</span>, the following holds <div class="math notranslate nohighlight">
\[ \sum_{i=1}^{N} \sum_{j=1}^{N} a_{i} a_{j} {\bf x}_{i}^{T} {\bf x}_{j} \geq 0. \]</div>
 Alternatively, we can write in vector notation as</p>
<div class="math notranslate nohighlight" id="equation-positive-definite">
<span class="eqno">(102)<a class="headerlink" href="#equation-positive-definite" title="Permalink to this equation">#</a></span>\[{\bf a}^{T} {\bf K} {\bf a} \geq 0,\]</div>
<p>where <span class="math notranslate nohighlight">\( {\bf K} = \left[ K_{i,j} \right] \)</span> is a <span class="math notranslate nohighlight">\( N \times N \)</span> matrix with <span class="math notranslate nohighlight">\( K_{i,j} = k({\bf x}_{i}, {\bf x}_{j}) \)</span> for all <span class="math notranslate nohighlight">\( i,j \in \lbrace 1, 2, \ldots, N \rbrace \)</span> and <span class="math notranslate nohighlight">\( {\bf a} = \begin{bmatrix} a_{1} &amp; a_{2} &amp; \ldots &amp; a_{N} \end{bmatrix}^{T} \)</span> is an arbitrary vector in <span class="math notranslate nohighlight">\( \mathbb{R}^{N} \)</span>.</p>
</section>
<section id="kernel-trick">
<h2>Kernel trick<a class="headerlink" href="#kernel-trick" title="Permalink to this headline">#</a></h2>
<p>Applying non-linear transformations of the type <div class="math notranslate nohighlight">
\[ \phi : {\cal X} \rightarrow {\cal Z} \]</div>
 mapping feature vectors <span class="math notranslate nohighlight">\( {\bf x} \in {\cal X} \)</span> into a higher-dimensional space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> can significantly boost several Machine Learning (ML) algorithms, for instance SVMs, Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA).</p>
<p>Mapping feature vectors into such high-dimensional space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> can be effective but it is often expensive and selecting the proper non-linear transformation <span class="math notranslate nohighlight">\( \phi(\cdot) \)</span> is also a hard task. Fortunately, provided that the ML algorithm works with inner products of feature vectors as in <a class="reference internal" href="#equation-svm-form8">(96)</a>–<a class="reference internal" href="#equation-svm-form11">(101)</a>, the kernel trick allows one to work with high-dimensional spaces efficiently since it avoids computing <span class="math notranslate nohighlight">\( \phi(\cdot) \)</span> explicitly.</p>
<p>In most classification problems, decision boundaries are far from linear. In particular, for real-world binary classification problems, chances are that no hyperplane can separate training examples from both classes. On the other hand, high-dimensional data tends to be linearly separable. Intuitively, as the number of features increases, chances are that the data become linearly separable with respect of some of these features.</p>
<p>One approach to increase the number of dimensions is to apply <strong>non-linear feature transforms</strong> to the original features in <span class="math notranslate nohighlight">\( {\cal X} \)</span> transforming thus the learning problem into a higher dimensional feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span>. Note though that the feature transformations must be non-linear, for instance polynomials, exponential (e.g. <span class="math notranslate nohighlight">\(\exp\)</span>), logarithm (e.g. <span class="math notranslate nohighlight">\(\log\)</span>) and trigonometric functions (e.g. <span class="math notranslate nohighlight">\(\cos\)</span>, <span class="math notranslate nohighlight">\(\sin\)</span>, <span class="math notranslate nohighlight">\(\tanh\)</span>). This is a necessary (non-sufficient) condition. Note though that if we apply linear (affine) transformations, the data in <span class="math notranslate nohighlight">\({\cal Z} \)</span> will still linearly non-separable as in the original feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span>.</p>
<p><a class="reference internal" href="#non-linear-transformation-fig"><span class="std std-numref">Fig. 19</span></a> illustrates a particular non-linear transformations of features. Note that, despite being linearly non-separable in the original space <span class="math notranslate nohighlight">\( {\cal X} \)</span>, the non-linear features in <span class="math notranslate nohighlight">\( {\cal Z} \)</span> become linearly separable, i.e. one can design a linear classifier of the type
<div class="math notranslate nohighlight">
\[
h({\bf z}; {\bf w}, b) = \sgn({\bf w}^{T} {\bf z} - b)
\]</div>

residing in the non-linear feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> that is able to separate the data from both classes. Equivalently, we can also write
<div class="math notranslate nohighlight">
\[
h({\bf x}; {\bf w}, b) = \sgn({\bf w}^{T} \phi({\bf x}) - b).
\]</div>
</p>
<figure class="align-left" id="non-linear-transformation-fig">
<a class="reference internal image-reference" href="_images/degree2_monomials.png"><img alt="_images/degree2_monomials.png" src="_images/degree2_monomials.png" style="height: 320px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Non-linear transformation <span class="math notranslate nohighlight">\(\phi: \mathbb{R}^2 \rightarrow \mathbb{R}^3 \)</span> mapping feature vectors <span class="math notranslate nohighlight">\( {\bf x} = \begin{bmatrix} x_1 &amp; x_2 \end{bmatrix}^{T} \)</span> into non-linear feature vectors <span class="math notranslate nohighlight">\( {\bf z} = \begin{bmatrix} z_1 &amp; z_2 &amp; z_3 \end{bmatrix}^{T} \)</span> such that <span class="math notranslate nohighlight">\( {\bf z} = \phi({\bf x}) := \begin{bmatrix} x_1^2 &amp; \sqrt{2} x_1 x_2 &amp; x_2^2 \end{bmatrix}^{T} \)</span>. The original bi-dimensional feature space in the left is unsuitable for a linear classifier. However, the non-linear transformation of features yields to a three-dimensional feature space in the right in which the data is linearly separable for some classifier of the type <span class="math notranslate nohighlight">\( h({\bf z}; {\bf w}, b) = \sgn({\bf w}^{T} {\bf z} - b) \)</span> (borrowed from <span id="id2">[<a class="reference internal" href="bibliography.html#id10">13</a>]</span>).</span><a class="headerlink" href="#non-linear-transformation-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Therefore, a linear classifier operating on a non-linear feature space leads to a non-linear classifier. Schematically, <div class="math notranslate nohighlight">
\[ \mbox{non-linear features} + \mbox{linear classifier} = \mbox{non-linear classifier}. \]</div>
</p>
</div>
</aside>
<p>Unfortunately, there are too many non-linear transforms to the features. In addition to this, selecting a non-linear transform <span class="math notranslate nohighlight">\(\phi: {\cal X} \rightarrow {\cal Z} \)</span> that leads to a non-linear feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> in which the transformed data from <span class="math notranslate nohighlight">\( {\cal X} \)</span> is linearly separable is not a straightforward task. Furthermore, for a given class of non-linear transformations, the number of possible features might quickly explode. For example, let us consider all polynomials of degree <span class="math notranslate nohighlight">\(K\)</span>. Thus, there are <span class="math notranslate nohighlight">\({D + K - 1}\choose {K}\)</span> possible features to select – with <span class="math notranslate nohighlight">\(D\)</span> denoting the number of features in the original feature space <span class="math notranslate nohighlight">\( {\cal X}\)</span> – when designing the non-linear feature space <span class="math notranslate nohighlight">\({\cal Z}\)</span>. In particular, <span class="math notranslate nohighlight">\(D=100\)</span> and <span class="math notranslate nohighlight">\(K=5\)</span> yields <span class="math notranslate nohighlight">\(75 \times 10^6\)</span> possible features. Therefore, checking all possible combinations of non-linear features to select a suitable non-linear transform <span class="math notranslate nohighlight">\(\phi\)</span> is prohibitive.</p>
<p>Fortunately, many learning algorithms can be re-formulated such that they work only with labels <span class="math notranslate nohighlight">\( y_{1}, y_{2}, \ldots, y_{N} \)</span> and <strong>inner products</strong> <span class="math notranslate nohighlight">\( {\bf x}_{i}^{T} {\bf x}_{j} \)</span>. For those algorithms, we can employ the <strong>Kernel Trick</strong> to efficiently work with high-dimensional features spaces without explicitly transforming the original features.</p>
<p>More precisely, let us define a <span class="math notranslate nohighlight">\( N \times N \)</span> pairwise similarity matrix <span class="math notranslate nohighlight">\( {\bf K} \triangleq \left[ K_{i,j} \right] \)</span> – a.k.a. Gram matrix – such that</p>
<div class="math notranslate nohighlight" id="equation-pairwise-similarity1">
<span class="eqno">(103)<a class="headerlink" href="#equation-pairwise-similarity1" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
K_{i,j} &amp;=&amp; k({\bf x}_{i}, {\bf x}_{j}) \\
&amp;=&amp; \phi({\bf x}_{i})^{T} \phi({\bf x}_{j})
\end{eqnarray}\end{split}\]</div>
<p><span class="math notranslate nohighlight">\( \forall i,j \lbrace 1, 2, \ldots, N \rbrace \)</span>. As <div class="math notranslate nohighlight">
\[ \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) = 0 \Leftrightarrow \phi({\bf x}_{i}) \perp \phi({\bf x}_{j}), \]</div>
 the function <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> is a similarity measure – a scalar – of the transformed, non-linear feature vectors <span class="math notranslate nohighlight">\( \phi({\bf x}_{i}) \)</span> and <span class="math notranslate nohighlight">\( \phi({\bf x}_{j}) \)</span>. We can also define the similarity measure between the training samples <span class="math notranslate nohighlight">\( {\cal D}_{i} = \left( {\bf x}_{i}, y_{i} \right) \)</span> and <span class="math notranslate nohighlight">\( {\cal D}_{j} = \left( {\bf x}_{j}, y_{j} \right) \)</span> as</p>
<div class="math notranslate nohighlight" id="equation-pairwise-similarity2">
<span class="eqno">(104)<a class="headerlink" href="#equation-pairwise-similarity2" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
g({\cal D}_{i}, {\cal D}_{j}) &amp;=&amp; y_{i} y_{j} \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) \\
&amp;=&amp;  y_{i} y_{j} k({\bf x}_{i}, {\bf x}_{j}).
\end{eqnarray}\end{split}\]</div>
<p>Note that for normalized feature vectors <span class="math notranslate nohighlight">\( \phi({\bf x}_{i}) \)</span> and <span class="math notranslate nohighlight">\( \phi({\bf x}_{j}) \)</span>, <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \geq 0 \)</span>. Thus, the sign of the product <span class="math notranslate nohighlight">\( y_{i} y_{j} \)</span> indicates either a label matching (<span class="math notranslate nohighlight">\( +1 \)</span>) or mismatching (<span class="math notranslate nohighlight">\( -1 \)</span>) in binary classification problems with labels in <span class="math notranslate nohighlight">\( {\cal Y} = \lbrace -1, +1 \rbrace \)</span>. In this case, we can redefine the Gram matrix as a <span class="math notranslate nohighlight">\( N \times N \)</span> influence matrix <span class="math notranslate nohighlight">\( {\bf G} = \left[ G_{i,j} \right] \)</span> such that <div class="math notranslate nohighlight">
\[ G_{i,j} = g({\cal D}_{i}, {\cal D}_{j}) \]</div>
 for all <span class="math notranslate nohighlight">\( i,j \in \lbrace 1, 2, \ldots, N \rbrace \)</span>.</p>
<p>Now, let us rewrite the dual SVM problem in the non-linear feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> as</p>
<div class="math notranslate nohighlight" id="equation-svm-form12">
<span class="eqno">(105)<a class="headerlink" href="#equation-svm-form12" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\maximize_{\boldsymbol{\lambda} \geq {\bf 0}}  &amp;&amp;  - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) + \sum_{i=1}^{N} \lambda_{i} \\
s.t. \,\,\, &amp;&amp;  \sum_{i=1}^{N} \lambda_{i} y_{i} = 0 \\
&amp;&amp;  0 \leq \lambda_{i} \leq C, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace.
\end{eqnarray}\end{split}\]</div>
<p>Thus, as far as we are able to compute the similarity metric <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> implicitly, we can efficiently solve the optimization problem in <a class="reference internal" href="#equation-svm-form12">(105)</a> without computing the non-linear features <span class="math notranslate nohighlight">\( \lbrace \phi({\bf x}_{i}) \rbrace \)</span>, <span class="math notranslate nohighlight">\( i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>. Fortunately, this possible as far as the the kernel <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> satisfies some mild conditions.</p>
<p>Now, we offer without proof a key theorem for the Kernel Trick.</p>
<div class="proof theorem admonition" id="theorem-6">
<p class="admonition-title"><span class="caption-number">Theorem 16 </span> (Representer theorem)</p>
<section class="theorem-content" id="proof-content">
<p>A kernel function <span class="math notranslate nohighlight">\( k: {\cal X} \times {\cal X} \rightarrow \mathbb{R} \)</span> is positive definite i.f.f. it corresponds to the inner product in some feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> defined by the transformation <span class="math notranslate nohighlight">\( \phi: {\cal X} \rightarrow {\cal Z} \)</span>, i.e. <div class="math notranslate nohighlight">
\[ \exists \phi({\bf x}) \mid k({\bf x}_{i}, {\bf x}_{j}) = \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) \Leftrightarrow \forall {\bf a} \in \mathbb{R}^{N}  \mid {\bf a}^{T} {\bf K} {\bf a} \geq 0 \]</div>
 with <span class="math notranslate nohighlight">\( {\bf K} = \left[ K_{i,j} \right] \)</span> denoting a <span class="math notranslate nohighlight">\( N \times N \)</span> matrix such that <span class="math notranslate nohighlight">\( K_{i,j} = k({\bf x}_{i}, {\bf x}_{j}) \)</span>, <span class="math notranslate nohighlight">\( \forall i,j \in \lbrace 1, 2, \ldots, N \rbrace \)</span>.</p>
</section>
</div><p>As a corollary, if we choose a particular function <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> such that <span class="math notranslate nohighlight">\( \sum_{i=1}^{N} \sum_{j=1}^{N} a_{i} a_{j} {\bf x}_{i}^{T} {\bf x}_{j} \geq 0 \)</span> for all data items <span class="math notranslate nohighlight">\( i,j \in \lbrace 1, 2, \ldots, N \rbrace \)</span> in your training dataset <span class="math notranslate nohighlight">\( {\cal D} \)</span>, we can compute the inner product <span class="math notranslate nohighlight">\( \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) \)</span> implicitly without even knowing the non-linear transformation <span class="math notranslate nohighlight">\( \phi({\bf x}) \)</span>. Putting in other words, if the matrix <span class="math notranslate nohighlight">\( {\bf K} \)</span> is positive definite such that <a class="reference internal" href="#equation-positive-definite">(102)</a> holds, we can replace the the product <span class="math notranslate nohighlight">\( \phi({\bf x}_{i})^{T} \phi({\bf x}_{j}) \)</span> in <a class="reference internal" href="#equation-svm-form12">(105)</a> by the kernel <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> to obtain the kernelized SVM formulation</p>
<div class="math notranslate nohighlight" id="equation-svm-form13">
<span class="eqno">(106)<a class="headerlink" href="#equation-svm-form13" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\maximize_{\boldsymbol{\lambda} \geq {\bf 0}}  &amp;&amp;  - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \lambda_{i} \lambda_{j} y_{i} y_{j} k({\bf x}_{i}, {\bf x}_{j}) + \sum_{i=1}^{N} \lambda_{i} \\
s.t. \,\,\, &amp;&amp;  \sum_{i=1}^{N} \lambda_{i} y_{i} = 0 \\
&amp;&amp;  0 \leq \lambda_{i} \leq C, \,\,\, \forall i \in \lbrace 1, 2, \ldots, N \rbrace
\end{eqnarray}\end{split}\]</div>
<p>which is still a quadratic program (convex) and therefore the global optimal solution <span class="math notranslate nohighlight">\( \boldsymbol{\lambda}^{\ast} \)</span> to this dual problem also leads to an optimal solution to the primal problem in <a class="reference internal" href="#equation-svm-form-7">(91)</a>. Note that the Kernel Trick allows one to generalize several learning algorithms which rely on inner products (e.g. SVMs, PCAs and LDAs). Moreover, it also allows one to work with <em>non-vector</em> data (e.g. strings and graphs) by means of the selection of a proper kernel / similarity measure <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) \)</span> between the data items / objects within the original object space <span class="math notranslate nohighlight">\( {\cal X} \)</span>.</p>
<p>Typical kernel functions employed</p>
<ul class="simple">
<li><p><strong>Linear</strong> <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) = {\bf x}_{i}^{T} {\bf x}_{j} \)</span>, which corresponds to working in the original feature space, i.e. <span class="math notranslate nohighlight">\( \phi({\bf x}) = {\bf x} \)</span>;</p></li>
<li><p><strong>Polynomial</strong> <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) = (1 + {\bf x}_{i}^{T} {\bf x}_{j})^{p}\)</span>, which corresponds to <span class="math notranslate nohighlight">\(\phi\)</span> mapping to all polynomials – non-linear features – up to degree <span class="math notranslate nohighlight">\(p\)</span>;</p></li>
<li><p><strong>Gaussian</strong> <span class="math notranslate nohighlight">\( k({\bf x}_{i}, {\bf x}_{j}) = \exp(-\gamma || {\bf x}_{i} - {\bf x}_{j} ||^{2}) \)</span>, which corresponds in turn to an infinite feature space or functional space with infinite number of dimensions.</p></li>
</ul>
<p>Recall from <a class="reference internal" href="#equation-w-optimal">(97)</a> and <a class="reference internal" href="#equation-b-optimal">(98)</a> that</p>
<div class="math notranslate nohighlight" id="equation-w-optimal2">
<span class="eqno">(107)<a class="headerlink" href="#equation-w-optimal2" title="Permalink to this equation">#</a></span>\[{\bf w}^{\ast} = \sum_{i=1}^{N} \lambda^{\ast}_{i} y_{i} {\bf x}_{i}\]</div>
<div class="math notranslate nohighlight" id="equation-b-optimal2">
<span class="eqno">(108)<a class="headerlink" href="#equation-b-optimal2" title="Permalink to this equation">#</a></span>\[b^{\ast} = {{\bf w}^{\ast}}^{T} {\bf x}_{i} - y_{i}.\]</div>
<p>Now, by substituting <span class="math notranslate nohighlight">\( {\bf x}_{i} \)</span> by <span class="math notranslate nohighlight">\( \phi({\bf x}_{i} ) \)</span> in <a class="reference internal" href="#equation-w-optimal2">(107)</a> and in <a class="reference internal" href="#equation-b-optimal2">(108)</a>, we have</p>
<div class="math notranslate nohighlight" id="equation-w-optimal3">
<span class="eqno">(109)<a class="headerlink" href="#equation-w-optimal3" title="Permalink to this equation">#</a></span>\[{\bf w}^{\ast} = \sum_{i=1}^{N} \lambda^{\ast}_{i} y_{i} \phi({\bf x}_{i})\]</div>
<div class="math notranslate nohighlight" id="equation-b-optimal3">
<span class="eqno">(110)<a class="headerlink" href="#equation-b-optimal3" title="Permalink to this equation">#</a></span>\[b^{\ast} = {{\bf w}^{\ast}}^{T} \phi({\bf x}_{i}) - y_{i}.\]</div>
<p>Lastly, by plugging <a class="reference internal" href="#equation-w-optimal3">(109)</a> into <a class="reference internal" href="#equation-b-optimal3">(110)</a>, we can compute the optimal offset term</p>
<div class="amsmath math notranslate nohighlight" id="equation-7b0fd838-8979-4669-b57a-50c760090bce">
<span class="eqno">(111)<a class="headerlink" href="#equation-7b0fd838-8979-4669-b57a-50c760090bce" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
b^{\ast} &amp;=&amp; \left( \sum_{j=1}^{N} \lambda^{\ast}_{j} y_{j} \phi({\bf x}_{j}) \right)^{T} \phi({\bf x}_{i}) - y_{i} \nonumber \\
&amp;=&amp; \sum_{j=1}^{N} \lambda^{\ast}_{j} y_{j} \underbrace{\phi({\bf x}_{j})^{T} \phi({\bf x}_{i})}_{k({\bf x}_{j}, {\bf x}_{i})} - y_{i} \nonumber \\
&amp;=&amp; \sum_{j=1}^{N} \lambda^{\ast}_{j} y_{j} k({\bf x}_{j}, {\bf x}_{i}) - y_{i}
\end{eqnarray}\]</div>
<p>using any training example <span class="math notranslate nohighlight">\( {\cal D}_{i} = \left( {\bf x}_{i}, y_{i} \right) \)</span>, <span class="math notranslate nohighlight">\( i \in \lbrace 1, 2, \ldots, N \rbrace \)</span>, whose corresponding Lagrange multiplier <span class="math notranslate nohighlight">\( \lambda^{\ast}_{i} &gt; 0 \)</span>.</p>
<p>Note however that there is no need to compute the optimal coefficient vector <span class="math notranslate nohighlight">\( {\bf w}^{\ast} \)</span> explicitly. Specifically, for an arbitrary feature vector <span class="math notranslate nohighlight">\( {\bf x} \)</span> in the original feature space <span class="math notranslate nohighlight">\( {\cal X} \)</span>, the Kernelized SVM classifier in the non-linear feature space <span class="math notranslate nohighlight">\( {\cal Z} \)</span> – determined by some mapping function <span class="math notranslate nohighlight">\( \phi:{\cal X} \rightarrow {\cal Z} \)</span> – can be written as</p>
<div class="amsmath math notranslate nohighlight" id="equation-dd58d6d1-4364-4930-b57d-21f9aa64e796">
<span class="eqno">(112)<a class="headerlink" href="#equation-dd58d6d1-4364-4930-b57d-21f9aa64e796" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
h_{kSVM}({\bf x}) &amp;=&amp; \sgn \left( {{\bf w}^{\ast}}^{T} \phi({\bf x}) - b^{\ast} \right) \nonumber \\
&amp;=&amp; \sgn \left( \left( \sum_{i=1}^{N} \lambda^{\ast}_{i} y_{i} \phi({\bf x}_{i}) \right)^{T} \phi({\bf x}) - b^{\ast} \right) \nonumber \\
&amp;=&amp; \sgn \left( \sum_{i=1}^{N} \lambda^{\ast}_{i} y_{i} \underbrace{\phi({\bf x}_{i})^{T} \phi({\bf x})}_{k({\bf x}_{i}, {\bf x})} - b^{\ast} \right). \nonumber
\end{eqnarray}\]</div>
<p>Finally leading to</p>
<div class="math notranslate nohighlight" id="equation-kerneliezed-svm">
<span class="eqno">(113)<a class="headerlink" href="#equation-kerneliezed-svm" title="Permalink to this equation">#</a></span>\[h_{kSVM}({\bf x}) = \sgn \left( \sum_{i=1}^{N} \lambda^{\ast}_{i} y_{i} k({\bf x}_{i}, {\bf x}) - b^{\ast} \right).\]</div>
<div class="proof example admonition" id="example-7">
<p class="admonition-title"><span class="caption-number">Example 24 </span> (Kernelized SVM classifier in action using a toy example with <em>sklearn</em>)</p>
<section class="example-content" id="proof-content">
<p>Figures below illustrate the effect of different Kernels with <em>sklearn</em> using a toy binary classification example. <a class="reference internal" href="#toyexp-kernel-svm-01-fig"><span class="std std-numref">Fig. 20</span></a> shows a linear kernel failing miserably to separate training data items assigned to <span style="color: blue;">blue</span> and <span style="color: red;">red</span> class labels; <a class="reference internal" href="#toyexp-kernel-svm-02-fig"><span class="std std-numref">Fig. 21</span></a> a polynomial kernel with degree <span class="math notranslate nohighlight">\(p=4\)</span> is able to circumscribe the data items assigned to the <span style="color: blue;">blue</span> labels; and <a class="reference internal" href="#toyexp-kernel-svm-03-fig"><span class="std std-numref">Fig. 22</span></a> a Gaussian kernel – a.k.a. radial basis function – is also able to separate both classes, but using kind of more rounded decision boundary between classes.</p>
<figure class="align-left" id="toyexp-kernel-svm-01-fig">
<a class="reference internal image-reference" href="_images/toyexp_kernel_svm_01.png"><img alt="_images/toyexp_kernel_svm_01.png" src="_images/toyexp_kernel_svm_01.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">classifier = svm.SVC(kernel=’linear’, C=C)</span><a class="headerlink" href="#toyexp-kernel-svm-01-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-left" id="toyexp-kernel-svm-02-fig">
<a class="reference internal image-reference" href="_images/toyexp_kernel_svm_02.png"><img alt="_images/toyexp_kernel_svm_02.png" src="_images/toyexp_kernel_svm_02.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">classifier = svm.SVC(kernel=’poly’, degree=4, C=C)</span><a class="headerlink" href="#toyexp-kernel-svm-02-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-left" id="toyexp-kernel-svm-03-fig">
<a class="reference internal image-reference" href="_images/toyexp_kernel_svm_03.png"><img alt="_images/toyexp_kernel_svm_03.png" src="_images/toyexp_kernel_svm_03.png" style="height: 200px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">classifier = svm.SVC(kernel=’rbf’, C=C)</span><a class="headerlink" href="#toyexp-kernel-svm-03-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
</section>
</div><aside class="margin sidebar">
<p class="sidebar-title"></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>SVMs and Kernels were hot research topics in the 90’s and early 2000’s. Nevertheless, Kernelized SVMs still one of the strongest classifiers today.</p>
</div>
</aside>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="footnote2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Assuming that the hyperplane <span class="math notranslate nohighlight">\( {\bf w}^{T} {\bf x} - b = 0\)</span> is a valid decision boundary for the linearly separable training dataset <span class="math notranslate nohighlight">\({\cal D}\)</span>.</p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "None/None",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="classification_random_forests.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Random Forests</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="neuralnets.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Neural Networks</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Sibylle Hess and Stiven Dias<br/>
  
      &copy; Copyright 2022. Eindhoven University of Technology.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>